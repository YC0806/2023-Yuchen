{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:14:26.369872Z","iopub.execute_input":"2023-07-12T23:14:26.370767Z","iopub.status.idle":"2023-07-12T23:14:29.831285Z","shell.execute_reply.started":"2023-07-12T23:14:26.370730Z","shell.execute_reply":"2023-07-12T23:14:29.829605Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Initialize for CPU","metadata":{}},{"cell_type":"code","source":"!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n!pip install torch_geometric_temporal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:14:29.837950Z","iopub.execute_input":"2023-07-12T23:14:29.838984Z","iopub.status.idle":"2023-07-12T23:16:03.096644Z","shell.execute_reply.started":"2023-07-12T23:14:29.838940Z","shell.execute_reply":"2023-07-12T23:16:03.095640Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (504 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.1/504.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.1+pt20cpu\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.17%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-sparse) (1.23.5)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.17+pt20cpu\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (732 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.3/732.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-cluster) (1.10.1)\nRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-cluster) (1.23.5)\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.1+pt20cpu\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/pyg-team/pytorch_geometric.git\n  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-z7av_88m\n  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-z7av_88m\n  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 9bc70173159b44502d6908c1563cf8a72cac5cdb\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.10.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric==2.4.0) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric==2.4.0) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric==2.4.0) (3.1.0)\nBuilding wheels for collected packages: torch_geometric\n  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=971047 sha256=1b175a63c2744c30e3bdcfd284a8bc61fe9bbf4b3801e9a46201c453aa05bdc9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ws2hmaqv/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\nSuccessfully built torch_geometric\nInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torch_geometric_temporal\n  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting decorator==4.4.2 (from torch_geometric_temporal)\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (2.0.0+cpu)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (0.29.34)\nCollecting pandas<=1.3.5 (from torch_geometric_temporal)\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch_sparse in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (0.6.17+pt20cpu)\nRequirement already satisfied: torch_scatter in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (2.1.1+pt20cpu)\nRequirement already satisfied: torch_geometric in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (1.23.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (1.16.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch_geometric_temporal) (3.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas<=1.3.5->torch_geometric_temporal) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas<=1.3.5->torch_geometric_temporal) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torch_geometric_temporal) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torch_geometric_temporal) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torch_geometric_temporal) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torch_geometric_temporal) (3.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (4.64.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (1.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch_geometric_temporal) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torch_geometric_temporal) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric->torch_geometric_temporal) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric->torch_geometric_temporal) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric->torch_geometric_temporal) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric->torch_geometric_temporal) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric->torch_geometric_temporal) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric->torch_geometric_temporal) (3.1.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torch_geometric_temporal) (1.3.0)\nBuilding wheels for collected packages: torch_geometric_temporal\n  Building wheel for torch_geometric_temporal (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch_geometric_temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86744 sha256=818739f6ed3e38a4a018782557f988d102629db509b74558f787ec287dca372d\n  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\nSuccessfully built torch_geometric_temporal\nInstalling collected packages: decorator, pandas, torch_geometric_temporal\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Uninstalling decorator-5.1.1:\n      Successfully uninstalled decorator-5.1.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.58.190319 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\ngcsfs 2023.5.0 requires fsspec==2023.5.0, but you have fsspec 2023.6.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\nwoodwork 0.24.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.5.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nydata-profiling 4.1.2 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed decorator-4.4.2 pandas-1.3.5 torch_geometric_temporal-0.54.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize for GPU","metadata":{}},{"cell_type":"code","source":"# !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n# !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n# !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n# !pip install torch_geometric_temporal","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:16:03.099002Z","iopub.execute_input":"2023-07-12T23:16:03.099447Z","iopub.status.idle":"2023-07-12T23:16:03.105699Z","shell.execute_reply.started":"2023-07-12T23:16:03.099401Z","shell.execute_reply":"2023-07-12T23:16:03.104560Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom scipy.special import perm\nfrom itertools import combinations,chain\nfrom typing import List, Union\nfrom torch_geometric.data import Data","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:16:03.108837Z","iopub.execute_input":"2023-07-12T23:16:03.109521Z","iopub.status.idle":"2023-07-12T23:16:03.885416Z","shell.execute_reply.started":"2023-07-12T23:16:03.109478Z","shell.execute_reply":"2023-07-12T23:16:03.884407Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"Edge_Flag = List[Union[np.ndarray, None]]\nEdge_Indices = List[Union[np.ndarray, None]]\nEdge_Attr = List[Union[np.ndarray, None]]\n\nNode_Flag = List[Union[np.ndarray, None]]\nNode_Indices = List[Union[np.ndarray, None]]\nNode_Attr = List[Union[np.ndarray, None]]\nAdditional_Attrs = List[np.ndarray]\n\n\n\nclass GraphSignal(object):\n    # dynamic node static node attr dynamic edge and edge attr\n    def __init__(\n        self,\n        edge_flag: Edge_Flag,\n        edge_indices: Edge_Indices,\n        edge_attr: Edge_Attr,\n        node_flag: Node_Flag,\n        node_indices: Node_Indices,\n        node_attr: Node_Attr,\n    ):\n        \n        self.edge_flag = edge_flag \n        self.edge_indices = edge_indices\n        self.edge_attr = edge_attr\n        self.node_flag = node_flag\n        self.node_indices = node_indices\n        self.node_attr = node_attr\n        self._set_snapshot_count()\n\n    def _set_snapshot_count(self):\n        self.snapshot_count = len(self.edge_flag)\n\n    def _get_edge_index(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_index = self.edge_indices[_start:_end]\n        return torch.LongTensor(np.array(_edge_index).T)\n\n    # def _get_edge_weight(self, time_index: int):\n    #     if self.edge_weights[time_index] is None:\n    #         return self.edge_weights[time_index]\n    #     else:\n    #         return torch.FloatTensor(self.edge_weights[time_index])\n\n    def _get_edge_attr(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_attr = self.edge_attr[_start:_end]\n        return torch.FloatTensor(np.array(_edge_attr))\n    \n    def _get_node_index_attr(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.node_flag[time_index-1]\n        _end = self.node_flag[time_index]\n        _node_index = self.node_indices[_start:_end]\n        _node_attr = self.node_attr[_node_index]\n        return torch.LongTensor(np.array(_node_index)),torch.FloatTensor(np.array(_node_attr))\n\n\n    def __getitem__(self, time_index: int):\n        edge_index = self._get_edge_index(time_index)\n        edge_attr = self._get_edge_attr(time_index)\n        node_index,node_attr = self._get_node_index_attr(time_index)\n\n        snapshot = Data(\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            node_index=node_index,\n            node_attr=node_attr,\n        )\n        return snapshot\n\n    def __next__(self):\n        if self.t < self.snapshot_count:\n            snapshot = self[self.t]\n            self.t = self.t + 1\n            return snapshot\n        else:\n            self.t = 0\n            raise StopIteration\n\n    def __iter__(self):\n        self.t = 0\n        return self\n    \n    def __len__(self):\n        return self.snapshot_count\n\n\nclass GraphDatasetLoader(object):\n    def __init__(self,input_path=\"\"):\n        self.input_path = input_path\n        self._read_data()\n    \n    def _read_data(self):\n        self._dataset = np.load(self.input_path)\n\n    def get_dataset(self): # -> DynamicGraphTemporalSignal:\n        dataset = GraphSignal(\n            edge_flag = self._dataset['edge_flag_array'],\n            edge_indices = self._dataset['edge_index_array'],\n            edge_attr = self._dataset['edge_attr_array'],\n            node_flag = self._dataset['node_flag_array'],\n            node_indices = self._dataset['node_index_array'],\n            node_attr = self._dataset['node_attr_array'],\n        )\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:03.886972Z","iopub.execute_input":"2023-07-12T23:16:03.887436Z","iopub.status.idle":"2023-07-12T23:16:03.904345Z","shell.execute_reply.started":"2023-07-12T23:16:03.887407Z","shell.execute_reply":"2023-07-12T23:16:03.903194Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_gpu = torch.cuda.is_available()\ndataloader = GraphDatasetLoader(\"/kaggle/input/dissertation-data/test_60_30.npz\")\ntrain_set = dataloader.get_dataset()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:03.905486Z","iopub.execute_input":"2023-07-12T23:16:03.906361Z","iopub.status.idle":"2023-07-12T23:16:04.459231Z","shell.execute_reply.started":"2023-07-12T23:16:03.906324Z","shell.execute_reply":"2023-07-12T23:16:04.458141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pickle\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Linear as Lin\nfrom torch.nn import ReLU\nfrom torch.nn import Sequential as Seq\nimport torch.nn.functional as F\nfrom torch_geometric.nn import NNConv\nfrom torch_geometric_temporal import GConvGRU\nfrom torch import autograd","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:04.460883Z","iopub.execute_input":"2023-07-12T23:16:04.461201Z","iopub.status.idle":"2023-07-12T23:16:04.567932Z","shell.execute_reply.started":"2023-07-12T23:16:04.461174Z","shell.execute_reply":"2023-07-12T23:16:04.567076Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class MultiNNConv(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, edge_channels, gcn_hidden_nums, edge_hidden_nums, lin_hidden_nums):\n        super().__init__()\n        \n        def _create_edge_nn(edge_out_channels):\n            edge_nn = Seq()\n            pre_edge_h_num = edge_channels\n            for edge_h_num in edge_hidden_nums:\n                edge_nn.append(Lin(pre_edge_h_num,edge_h_num))\n                edge_nn.append(ReLU())\n                pre_edge_h_num = edge_h_num\n            edge_nn.append(Lin(pre_edge_h_num,edge_out_channels))\n            return edge_nn\n        \n        self.gcn_layers = nn.ModuleList()\n        pre_h_num = in_channels\n        for h_num in gcn_hidden_nums:\n            edge_nn = _create_edge_nn(pre_h_num*h_num)\n            self.gcn_layers.append(NNConv(pre_h_num, h_num, edge_nn, aggr='mean'))\n            pre_h_num = h_num\n\n        self.lin_net = Seq()\n        for h_num in lin_hidden_nums[:-1]:\n            self.lin_net.append(Lin(pre_h_num,h_num))\n            pre_h_num = h_num\n        self.lin_net.append(ReLU())\n        self.lin_net.append(Lin(pre_h_num,out_channels))\n\n    def forward(self, x, edge_index, edge_attr):\n        out = x\n        for conv in self.gcn_layers:\n            out = conv(\n                x=out,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n            )\n        out = self.lin_net(out)\n        return out\n\n\n\nclass NNConvGRU(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        edge_channels: int,\n        gcn_hidden_nums: List,\n        edge_hidden_nums: List,\n        lin_hidden_nums: List,\n        normalization: str = \"sym\",\n        bias: bool = True,\n    ):\n        super(NNConvGRU, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.edge_channels = edge_channels\n        self.gcn_hidden_nums = gcn_hidden_nums\n        self.edge_hidden_nums = edge_hidden_nums\n        self.lin_hidden_nums = lin_hidden_nums\n            \n        self.normalization = normalization\n        self.bias = bias\n        self._create_parameters_and_layers()\n\n    def _create_update_gate_parameters_and_layers(self):\n\n        self.conv_x_z = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n\n        self.conv_h_z = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n        \n    def _create_reset_gate_parameters_and_layers(self):\n\n        self.conv_x_r = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n\n        self.conv_h_r = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n\n    def _create_candidate_state_parameters_and_layers(self):\n\n        self.conv_x_h = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n\n        self.conv_h_h = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_hidden_nums = self.gcn_hidden_nums,\n            edge_hidden_nums = self.edge_hidden_nums,\n            lin_hidden_nums = self.lin_hidden_nums,\n        )\n\n    def _create_parameters_and_layers(self):\n        self._create_update_gate_parameters_and_layers()\n        self._create_reset_gate_parameters_and_layers()\n        self._create_candidate_state_parameters_and_layers()\n\n    def _set_hidden_state(self, X, H):\n        if H is None:\n            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n        return H\n\n    def _calculate_update_gate(self, X, edge_index, edge_attr, H):\n        Z = self.conv_x_z(X, edge_index, edge_attr)\n        Z = Z + self.conv_h_z(H, edge_index, edge_attr)\n        Z = torch.sigmoid(Z)\n        return Z\n\n    def _calculate_reset_gate(self, X, edge_index, edge_attr, H):\n        R = self.conv_x_r(X, edge_index, edge_attr)\n        R = R + self.conv_h_r(H, edge_index, edge_attr)\n        R = torch.sigmoid(R)\n        return R\n\n    def _calculate_candidate_state(self, X, edge_index, edge_attr, H, R):\n        H_tilde = self.conv_x_h(X, edge_index, edge_attr)\n        H_tilde = H_tilde + self.conv_h_h(H * R, edge_index, edge_attr)\n        H_tilde = torch.tanh(H_tilde)\n        return H_tilde\n\n    def _calculate_hidden_state(self, Z, H, H_tilde):\n        H = Z * H + (1 - Z) * H_tilde\n        return H\n\n    def forward(\n            self,\n            X: torch.FloatTensor,\n            edge_index: torch.LongTensor,\n            edge_attr: torch.FloatTensor,\n            H: torch.FloatTensor = None,\n        ) -> torch.FloatTensor:\n            H = self._set_hidden_state(X, H)\n            Z = self._calculate_update_gate(X, edge_index, edge_attr, H)\n            R = self._calculate_reset_gate(X, edge_index, edge_attr, H)\n            H_tilde = self._calculate_candidate_state(X, edge_index, edge_attr, H, R)\n            H = self._calculate_hidden_state(Z, H, H_tilde)\n            return H","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:04.569372Z","iopub.execute_input":"2023-07-12T23:16:04.569697Z","iopub.status.idle":"2023-07-12T23:16:04.592745Z","shell.execute_reply.started":"2023-07-12T23:16:04.569658Z","shell.execute_reply":"2023-07-12T23:16:04.591866Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class TGAE(torch.nn.Module): # Not Heterogeneous\n    def __init__(\n        self, in_channels, out_channels, edge_channels, \n        embedding_hidden_nums, gnn_out_channels, deciding_hidden_nums,\n        gru_gcn_hidden_nums, gru_edge_hidden_nums, gru_lin_hidden_nums):\n        super(TGAE, self).__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.edge_channels = edge_channels\n        self.embedding_hidden_nums = embedding_hidden_nums\n        self.gnn_out_channels = gnn_out_channels\n        self.deciding_hidden_nums = deciding_hidden_nums\n        self.gru_gcn_hidden_nums = gru_gcn_hidden_nums\n        self.gru_edge_hidden_nums = gru_edge_hidden_nums\n        self.gru_lin_hidden_nums = gru_lin_hidden_nums\n        \n        # Encoder\n        layers = []\n        pre_h_num = in_channels\n        for h_num in embedding_hidden_nums[:-1]:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,embedding_hidden_nums[-1]))\n        self.encoder_embedding_net = Seq(*layers)\n        \n        self.encoder_gru = NNConvGRU(\n            in_channels=embedding_hidden_nums[-1],\n            out_channels=gnn_out_channels,\n            edge_channels=edge_channels,\n            gcn_hidden_nums=gru_gcn_hidden_nums,\n            edge_hidden_nums=gru_edge_hidden_nums,\n            lin_hidden_nums=gru_lin_hidden_nums,\n        )\n\n        layers = []\n        pre_h_num = gnn_out_channels\n        for h_num in deciding_hidden_nums:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,out_channels))\n        self.encoder_deciding_net = Seq(*layers)\n        \n        # Decoder\n        layers = []\n        pre_h_num = out_channels\n        for h_num in deciding_hidden_nums[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,gnn_out_channels))\n        self.decoder_deciding_net = Seq(*layers)\n        \n        self.decoder_gru = NNConvGRU(\n            in_channels=gnn_out_channels,\n            out_channels=embedding_hidden_nums[-1],\n            edge_channels=edge_channels,\n            gcn_hidden_nums=gru_gcn_hidden_nums,\n            edge_hidden_nums=gru_edge_hidden_nums,\n            lin_hidden_nums=gru_lin_hidden_nums,\n        )\n        \n        layers = []\n        pre_h_num = embedding_hidden_nums[-1]\n        for h_num in embedding_hidden_nums[:-1][::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,in_channels))\n        self.decoder_embedding_net = Seq(*layers)\n        \n\n    def forward(self, x, edge_index, edge_attr, h_encoder=None, h_decoder=None):\n        \"\"\"\n        x = Node features for T time steps\n        edge_index = Graph edge indices\n        \"\"\"\n        # Encoder\n        out = self.encoder_embedding_net(x)\n        \n        # GNN layer\n        h_encoder = self.encoder_gru(out, edge_index, edge_attr, h_encoder) \n        \n        out = self.encoder_deciding_net(h_encoder)\n        \n        out = self.decoder_deciding_net(out)\n        \n        h_decoder = self.decoder_gru(out, edge_index, edge_attr, h_decoder)\n        \n        out = self.decoder_embedding_net(h_decoder)\n\n        return out, h_encoder, h_decoder","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:04.594057Z","iopub.execute_input":"2023-07-12T23:16:04.594542Z","iopub.status.idle":"2023-07-12T23:16:04.844646Z","shell.execute_reply.started":"2023-07-12T23:16:04.594514Z","shell.execute_reply":"2023-07-12T23:16:04.843384Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = TGAE(\n    in_channels=5, \n    out_channels=5, \n    edge_channels=57, \n    embedding_hidden_nums=[4,4],\n    gnn_out_channels=8,\n    deciding_hidden_nums=[4,4],\n    gru_gcn_hidden_nums=[16,16],\n    gru_edge_hidden_nums=[32],\n    gru_lin_hidden_nums=[64,64],\n)\n\nloss_f = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:04.848270Z","iopub.execute_input":"2023-07-12T23:16:04.848736Z","iopub.status.idle":"2023-07-12T23:16:04.927618Z","shell.execute_reply.started":"2023-07-12T23:16:04.848698Z","shell.execute_reply":"2023-07-12T23:16:04.926527Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"TGAE(\n  (encoder_embedding_net): Sequential(\n    (0): Linear(in_features=5, out_features=4, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=4, out_features=4, bias=True)\n  )\n  (encoder_gru): NNConvGRU(\n    (conv_x_z): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n    (conv_h_z): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n    (conv_x_r): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n    (conv_h_r): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n    (conv_x_h): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n    (conv_h_h): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=8, bias=True)\n      )\n    )\n  )\n  (encoder_deciding_net): Sequential(\n    (0): Linear(in_features=8, out_features=4, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=4, out_features=4, bias=True)\n    (3): LeakyReLU(negative_slope=0.01)\n    (4): Linear(in_features=4, out_features=5, bias=True)\n  )\n  (decoder_deciding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=5, out_features=4, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=4, out_features=4, bias=True)\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Linear(in_features=4, out_features=8, bias=True)\n  )\n  (decoder_gru): NNConvGRU(\n    (conv_x_z): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n    (conv_h_z): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n    (conv_x_r): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n    (conv_h_r): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n    (conv_x_h): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=128, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n    (conv_h_h): MultiNNConv(\n      (gcn_layers): ModuleList(\n        (0): NNConv(4, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=64, bias=True)\n        ))\n        (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n          (0): Linear(in_features=57, out_features=32, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=32, out_features=256, bias=True)\n        ))\n      )\n      (lin_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=4, bias=True)\n      )\n    )\n  )\n  (decoder_embedding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=4, out_features=4, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Linear(in_features=4, out_features=5, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def create_hidden_global(num_nodes, out_channels):\n    hidden_global = np.zeros([num_nodes,out_channels])\n    return hidden_global\n\ndef create_cell_global(num_nodes, out_channels):\n    cell_global = torch.zeros(num_nodes,out_channels)\n    return cell_global\n\ndef select_hidden_local(hidden_global, index):\n    h = hidden_global[index]\n    return h\n\ndef select_cell_local(cell_global, index):\n    c = cell_global[index]\n    return c\n\n# TODO: Aggregation of hidden and cell\ndef update_hidden_gobal(hidden_global, h, index):\n    hidden_global[index] = h.detach().cpu().numpy()\n    # for key,value in mapping.items():\n    #     hidden_global[value] = h[key] \n\ndef update_cell_gobal(cell_global, c, index):\n    cell_global[index] = c.detach().cpu().numpy()\n    # for key,value in mapping.items():\n    #     cell_global[value] = c[key] ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:16:04.929015Z","iopub.execute_input":"2023-07-12T23:16:04.929420Z","iopub.status.idle":"2023-07-12T23:16:04.936503Z","shell.execute_reply.started":"2023-07-12T23:16:04.929383Z","shell.execute_reply":"2023-07-12T23:16:04.935736Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(train_set.node_attr.shape)\nfor i,snapshot in enumerate(train_set,start=1):\n    print(snapshot.node_attr.shape[0])\n    print(snapshot.edge_index)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:04.937705Z","iopub.execute_input":"2023-07-12T23:16:04.938471Z","iopub.status.idle":"2023-07-12T23:16:05.055453Z","shell.execute_reply.started":"2023-07-12T23:16:04.938442Z","shell.execute_reply":"2023-07-12T23:16:05.054730Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(4839, 5)\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n163\ntensor([[ 1,  2,  3,  ...,  1,  2,  3],\n        [ 0,  0,  0,  ..., 12, 12, 12]])\n70\ntensor([[ 1,  2,  3,  ...,  2, 11, 10],\n        [ 0,  0,  0,  ..., 10, 10, 51]])\n68\ntensor([[ 1,  2,  3,  ...,  5,  2, 12],\n        [ 0,  0,  0,  ..., 11, 11, 11]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n187\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n65\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n75\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n66\ntensor([[ 1,  2,  3,  ..., 65, 65, 65],\n        [ 0,  0,  0,  ..., 19, 20, 21]])\n79\ntensor([[ 1,  2,  3,  ..., 28,  2, 48],\n        [ 0,  0,  0,  ..., 49, 49, 49]])\n85\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n74\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n75\ntensor([[ 1,  2,  3,  ..., 10,  2, 13],\n        [ 0,  0,  0,  ...,  1,  1,  1]])\n79\ntensor([[ 1,  2,  3,  ...,  2,  6,  4],\n        [ 0,  0,  0,  ...,  4,  4, 10]])\n67\ntensor([[ 1,  2,  3,  ...,  5,  2, 41],\n        [ 0,  0,  0,  ..., 40, 40, 40]])\n69\ntensor([[1, 2, 3,  ..., 2, 8, 6],\n        [0, 0, 0,  ..., 6, 6, 9]])\n82\ntensor([[ 1,  2,  3,  ...,  5,  2, 48],\n        [ 0,  0,  0,  ..., 47, 47, 47]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n101\ntensor([[  1,   2,   3,  ..., 100, 100, 100],\n        [  0,   0,   0,  ...,  19,  20,  21]])\n69\ntensor([[ 1,  2,  3,  ..., 35,  2, 44],\n        [ 0,  0,  0,  ..., 50, 50, 50]])\n85\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n71\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n80\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n79\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n100\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n83\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n90\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n936\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n72\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n82\ntensor([[1, 2, 3,  ..., 4, 2, 5],\n        [0, 0, 0,  ..., 1, 1, 1]])\n72\ntensor([[1, 2, 3,  ..., 0, 2, 5],\n        [0, 0, 0,  ..., 4, 4, 4]])\n310\ntensor([[1, 2, 3,  ..., 4, 2, 5],\n        [0, 0, 0,  ..., 1, 1, 1]])\n742\ntensor([[  1,   2,   3,  ...,  23, 740, 740],\n        [  0,   0,   0,  ..., 740,  24,  26]])\n65\ntensor([[ 1,  2,  3,  ...,  2,  3, 63],\n        [ 0,  0,  0,  ..., 63, 63,  4]])\n24\ntensor([[ 1,  2,  3,  0,  0,  1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0,  0,\n          0,  1,  2,  3,  0,  0,  1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0,\n          0,  0,  1,  2,  3,  0,  0, 10, 11, 12,  9,  9,  9, 10, 11, 12,  1,  2,\n          3,  0,  0,  0,  0, 10, 11, 12, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11,\n         12,  9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12,  9,\n          1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0,  1,  2,  3,  0,  0,  0,\n          0,  1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0, 18, 11, 12, 17, 17,\n         17,  1,  2,  3,  0,  0,  0,  0, 18, 11, 12, 18, 11, 12, 17, 18, 11, 12,\n         18, 11, 12, 17, 18, 11, 12, 17, 18, 11, 12, 17, 18, 11, 12, 17, 18, 11,\n         12, 17, 18, 11, 12, 17,  1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0,\n          1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,  0,  0,  0,  1,  2,  3,  0,\n          0, 20, 11, 12, 19, 19, 19,  1,  2,  3,  0,  0,  0,  0, 20, 11, 12, 22,\n         11, 23],\n        [ 0,  0,  0,  4,  5,  0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  6,  7,\n          8,  0,  0,  0,  4,  5,  0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  6,\n          7,  8,  0,  0,  0,  4,  5,  9,  9,  9, 13, 14, 15,  9,  9,  9,  0,  0,\n          0,  4,  6,  7,  8,  9,  9,  9,  9,  9,  9, 16,  9,  9,  9, 16,  9,  9,\n          9, 16,  9,  9,  9, 16,  9,  9,  9, 16,  9,  9,  9, 16,  9,  9,  9, 16,\n          0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  5,  0,  0,  0,  4,  6,  7,\n          8,  0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  5, 17, 17, 17, 13, 14,\n         15,  0,  0,  0,  4,  6,  7,  8, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17,\n         17, 17, 17, 16, 17, 17, 17, 16, 17, 17, 17, 16, 17, 17, 17, 16, 17, 17,\n         17, 16, 17, 17, 17, 16,  0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  5,\n          0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,  6,  7,  8,  0,  0,  0,  4,\n          5, 19, 19, 19, 13, 14, 15,  0,  0,  0,  4,  6,  7,  8, 19, 19, 19, 21,\n         21, 21]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n698\ntensor([[  1,   2,   3,  ...,   1,   2, 300],\n        [  0,   0,   0,  ..., 299, 299, 299]])\n132\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n79\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n82\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n78\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n69\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n167\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n64\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n78\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n69\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n66\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n75\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n77\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n56\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n80\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n103\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n69\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n64\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n80\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n98\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n83\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n63\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n76\ntensor([[1, 2, 3,  ..., 0, 2, 9],\n        [0, 0, 0,  ..., 8, 8, 8]])\n70\ntensor([[1, 2, 3,  ..., 4, 2, 5],\n        [0, 0, 0,  ..., 1, 1, 1]])\n75\ntensor([[1, 2, 3,  ..., 1, 2, 3],\n        [0, 0, 0,  ..., 0, 0, 0]])\n90\ntensor([[ 1,  2,  3,  ...,  1,  2, 65],\n        [ 0,  0,  0,  ..., 64, 64, 64]])\n4\ntensor([[1, 2, 3],\n        [0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_loop\ndef train_loop(train_set, num_nodes, state_channels, model, loss_fn, optimizer, device):\n\n    hidden_encoder_global = create_hidden_global(num_nodes=num_nodes,out_channels=state_channels)\n    hidden_decoder_global = create_hidden_global(num_nodes=num_nodes,out_channels=model.embedding_hidden_nums[-1])\n    # cell_global = create_cell_global(num_nodes=len(global_nodes),out_channels=256)\n    \n    train_losses = []\n    model.train()\n    for i,snapshot in enumerate(train_set,start=1):\n        if snapshot.node_attr.shape[0] == 0:\n            # print(\"snapshot_{} has no data...\".format(i))\n            continue\n        \n        node_attr = snapshot.node_attr\n        node_index = snapshot.node_index\n        edge_attr = snapshot.edge_attr\n        edge_index = snapshot.edge_index\n\n        \n        _node_attr = node_attr.to(device)\n        _edge_attr = edge_attr.to(device)\n        _edge_index = edge_index.to(device)\n        \n        pre_h_encoder = torch.tensor(select_hidden_local(hidden_encoder_global, node_index),dtype=torch.float32).to(device)\n        pre_h_decoder = torch.tensor(select_hidden_local(hidden_decoder_global, node_index),dtype=torch.float32).to(device)\n        \n        print(_node_attr.type())\n        # Compute prediction and loss\n        outs, h_encoder, h_decoder = model(_node_attr,_edge_index,_edge_attr)\n\n        update_hidden_gobal(hidden_encoder_global, h_encoder, node_index)\n        update_hidden_gobal(hidden_decoder_global, h_decoder, node_index)\n\n        train_loss = 0\n        for i in range(node_attr.shape[0]):\n#             train_loss += torch.sqrt(loss_f(torch.log(_node_attr[i]+1), torch.log(outs[i]+1)))\n            train_loss += torch.sqrt(loss_f(_node_attr[i], outs[i]))\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        if(use_gpu):\n            train_losses.append(train_loss.cpu().detach().numpy())\n        else:\n            train_losses.append(train_loss.detach().numpy())\n    return train_losses","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:05.056551Z","iopub.execute_input":"2023-07-12T23:16:05.057043Z","iopub.status.idle":"2023-07-12T23:16:05.067659Z","shell.execute_reply.started":"2023-07-12T23:16:05.057014Z","shell.execute_reply":"2023-07-12T23:16:05.066619Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_loop(train_set, train_set.node_attr.shape[0], model.gnn_out_channels, model, loss_f, optimizer, device)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T23:16:05.069330Z","iopub.execute_input":"2023-07-12T23:16:05.069655Z","iopub.status.idle":"2023-07-12T23:16:15.022062Z","shell.execute_reply.started":"2023-07-12T23:16:05.069627Z","shell.execute_reply":"2023-07-12T23:16:15.020986Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"torch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\ntorch.FloatTensor\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[array(2.3493378, dtype=float32),\n array(2.3336272, dtype=float32),\n array(108.26035, dtype=float32),\n array(45.413208, dtype=float32),\n array(44.236977, dtype=float32),\n array(46.69764, dtype=float32),\n array(127.73181, dtype=float32),\n array(42.241356, dtype=float32),\n array(48.34228, dtype=float32),\n array(46.534958, dtype=float32),\n array(42.703968, dtype=float32),\n array(50.730217, dtype=float32),\n array(54.502678, dtype=float32),\n array(46.273712, dtype=float32),\n array(47.68134, dtype=float32),\n array(48.71389, dtype=float32),\n array(47.98499, dtype=float32),\n array(50.367443, dtype=float32),\n array(43.04029, dtype=float32),\n array(44.147854, dtype=float32),\n array(52.13334, dtype=float32),\n array(45.880642, dtype=float32),\n array(62.836544, dtype=float32),\n array(44.025375, dtype=float32),\n array(54.171135, dtype=float32),\n array(45.621925, dtype=float32),\n array(44.78473, dtype=float32),\n array(50.396633, dtype=float32),\n array(49.668083, dtype=float32),\n array(64.06241, dtype=float32),\n array(52.777714, dtype=float32),\n array(58.45445, dtype=float32),\n array(570.72955, dtype=float32),\n array(45.235695, dtype=float32),\n array(51.152245, dtype=float32),\n array(45.132153, dtype=float32),\n array(220.4339, dtype=float32),\n array(513.3406, dtype=float32),\n array(42.78698, dtype=float32),\n array(15.414999, dtype=float32),\n array(2.2363892, dtype=float32),\n array(2.2348962, dtype=float32),\n array(2.2532163, dtype=float32),\n array(2.2251341, dtype=float32),\n array(2.230268, dtype=float32),\n array(522.3502, dtype=float32),\n array(83.07019, dtype=float32),\n array(49.372536, dtype=float32),\n array(50.47129, dtype=float32),\n array(48.101337, dtype=float32),\n array(42.66597, dtype=float32),\n array(111.0479, dtype=float32),\n array(46.817814, dtype=float32),\n array(39.619587, dtype=float32),\n array(46.759155, dtype=float32),\n array(47.97696, dtype=float32),\n array(42.71805, dtype=float32),\n array(40.498524, dtype=float32),\n array(46.79665, dtype=float32),\n array(46.17945, dtype=float32),\n array(46.585766, dtype=float32),\n array(47.36301, dtype=float32),\n array(46.624775, dtype=float32),\n array(34.869637, dtype=float32),\n array(48.906338, dtype=float32),\n array(60.98006, dtype=float32),\n array(42.38485, dtype=float32),\n array(39.34352, dtype=float32),\n array(48.96642, dtype=float32),\n array(59.140247, dtype=float32),\n array(50.348118, dtype=float32),\n array(38.975277, dtype=float32),\n array(46.41655, dtype=float32),\n array(42.940193, dtype=float32),\n array(45.563633, dtype=float32),\n array(56.732433, dtype=float32),\n array(2.2168584, dtype=float32)]"},"metadata":{}}]},{"cell_type":"code","source":"def test_loop(test_set, num_nodes, state_channels, model, loss_fn, device):\n    size = len(train_set)\n\n    hidden_encoder_global = create_hidden_global(num_nodes=num_nodes,out_channels=state_channels)\n    hidden_decoder_global = create_hidden_global(num_nodes=num_nodes,out_channels=state_channels)\n    # cell_global = create_cell_global(num_nodes=len(global_nodes),out_channels=256)\n    \n    test_losses = []\n    with torch.no_grad():\n        for i,snapshot in enumerate(test_set,start=1):\n            if snapshot.node_attr.shape[0] == 0:\n                # print(\"snapshot_{} has no data...\".format(i))\n                continue\n\n            node_attr = snapshot.node_attr\n            node_index = snapshot.node_index\n            edge_attr = snapshot.edge_attr\n            edge_index = snapshot.edge_index\n\n\n            _node_attr = node_attr.to(device)\n            _edge_attr = edge_attr.to(device)\n            _edge_index = edge_index.to(device)\n\n            pre_h_encoder = torch.tensor(select_hidden_local(hidden_encoder_global, node_index),dtype=torch.float32).to(device)\n            pre_h_decoder = torch.tensor(select_hidden_local(hidden_decoder_global, node_index),dtype=torch.float32).to(device)\n\n            # Compute prediction and loss\n            outs, h_encoder, h_decoder = model(_node_attr,_edge_index,_edge_attr)\n\n            update_hidden_gobal(hidden_encoder_global, h_encoder, node_index)\n            update_hidden_gobal(hidden_decoder_global, h_decoder, node_index)\n\n            test_loss = 0\n            for i in range(node_attr.shape[0]):\n    #             train_loss += torch.sqrt(loss_f(torch.log(_node_attr[i]+1), torch.log(outs[i]+1)))\n                test_loss += torch.sqrt(loss_f(_node_attr[i], outs[i]))\n        \n        if(use_gpu):\n            test_losses.append(test_loss.cpu().detach().numpy())\n        else:\n            test_losses.append(test_loss.detach().numpy())\n    return test_losses\n        ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-12T23:16:15.023811Z","iopub.execute_input":"2023-07-12T23:16:15.024273Z","iopub.status.idle":"2023-07-12T23:16:15.034894Z","shell.execute_reply.started":"2023-07-12T23:16:15.024225Z","shell.execute_reply":"2023-07-12T23:16:15.033904Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}