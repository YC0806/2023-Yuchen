{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_gpu = torch.cuda.is_available()\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:30.330561Z","iopub.execute_input":"2023-07-26T10:51:30.330814Z","iopub.status.idle":"2023-07-26T10:51:34.729255Z","shell.execute_reply.started":"2023-07-26T10:51:30.330791Z","shell.execute_reply":"2023-07-26T10:51:34.726343Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize for CPU & GPU","metadata":{}},{"cell_type":"code","source":"if use_gpu:\n    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric --no-index --find-links=file:///kaggle/input/torch-geometric\n#     !pip install torch_geometric_temporal  \nelse:\n    !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n    !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n#     !pip install torch_geometric_temporal\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-26T10:51:34.731607Z","iopub.execute_input":"2023-07-26T10:51:34.732322Z","iopub.status.idle":"2023-07-26T10:51:48.804071Z","shell.execute_reply.started":"2023-07-26T10:51:34.732281Z","shell.execute_reply":"2023-07-26T10:51:48.802904Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/torch-geometric\nProcessing /kaggle/input/torch-geometric/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_cluster-1.6.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_geometric-2.3.1-py3-none-any.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\nSuccessfully installed torch-cluster-1.6.1 torch-geometric-2.3.1 torch-scatter-2.1.1 torch-sparse-0.6.17 torch-spline-conv-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"from torch.profiler import profile, record_function, ProfilerActivity\nimport torch.autograd.profiler as profiler","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:48.807275Z","iopub.execute_input":"2023-07-26T10:51:48.807636Z","iopub.status.idle":"2023-07-26T10:51:48.812559Z","shell.execute_reply.started":"2023-07-26T10:51:48.807605Z","shell.execute_reply":"2023-07-26T10:51:48.811350Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport json\nfrom scipy.special import perm\nfrom itertools import combinations,chain\nfrom typing import List, Union\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nfrom torch import nn\nfrom torch.nn import Linear as Lin\nfrom torch.nn import ReLU\nfrom torch.nn import Sequential as Seq\nimport torch.nn.functional as F\nfrom torch_geometric.nn import NNConv\nfrom torch import autograd\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:48.815668Z","iopub.execute_input":"2023-07-26T10:51:48.816028Z","iopub.status.idle":"2023-07-26T10:51:50.272932Z","shell.execute_reply.started":"2023-07-26T10:51:48.815979Z","shell.execute_reply":"2023-07-26T10:51:50.271880Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"Edge_Flag = List[Union[np.ndarray, None]]\nEdge_Index = List[Union[np.ndarray, None]]\nEdge_Attr = List[Union[np.ndarray, None]]\n\nNode_Flag = List[Union[np.ndarray, None]]\nNode_Index = List[Union[np.ndarray, None]]\nNode_Attr = List[Union[np.ndarray, None]]\nAdditional_Attr = List[np.ndarray]\n\n\n\nclass GraphSignal(object):\n    # dynamic node static node attr dynamic edge and edge attr\n    def __init__(\n        self,\n        edge_flag: Edge_Flag,\n        edge_index: Edge_Index,\n        edge_attr: Edge_Attr,\n        node_flag: Node_Flag,\n        node_index: Node_Index,\n        node_attr: Node_Attr,\n        ts_list: List,\n    ):\n        \n        self.edge_flag = torch.LongTensor(edge_flag)\n        self.edge_index = torch.LongTensor(edge_index).T.to(device)\n        self.edge_attr = edge_attr\n        self.node_flag = torch.LongTensor(node_flag)\n        self.node_index = torch.LongTensor(node_index).to(device)\n        self.node_attr = node_attr\n        self.ts_list = ts_list\n        self.edge_attr_encoded = None\n        self.node_attr_encoded = None\n        \n        self._set_snapshot_count()\n\n    def _set_snapshot_count(self):\n        self.snapshot_count = len(self.edge_flag)\n    \n    def encode_edge_attr(self, enocder):\n        self.edge_attr_encoded = torch.FloatTensor(enocder.transform(self.edge_attr)).to(device)\n    \n    def encode_node_attr(self, enocder):\n        self.node_attr_encoded = torch.FloatTensor(enocder.transform(self.node_attr)).to(device)\n        \n    def _get_edge_index(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_index = self.edge_index[:,_start:_end]\n        return _edge_index\n\n    # def _get_edge_weight(self, time_index: int):\n    #     if self.edge_weights[time_index] is None:\n    #         return self.edge_weights[time_index]\n    #     else:\n    #         return torch.FloatTensor(self.edge_weights[time_index])\n\n    def _get_edge_attr(self, time_index: int):\n        if self.edge_attr_encoded is None:\n            print(\"Edge Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_attr = self.edge_attr_encoded[_start:_end]\n        return _edge_attr\n    \n    def _get_node_index_attr(self, time_index: int):\n        if self.node_attr_encoded is None:\n            print(\"Node Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.node_flag[time_index-1]\n        _end = self.node_flag[time_index]\n        _node_index = self.node_index[_start:_end]\n        _node_attr = self.node_attr_encoded[_node_index]\n        return _node_index,_node_attr\n    \n    def _get_timestamp(self, time_index: int):\n        _timestamp = self.ts_list[time_index]\n        return _timestamp\n\n\n    def __getitem__(self, time_index: int):\n        edge_index = self._get_edge_index(time_index)\n        edge_attr = self._get_edge_attr(time_index)\n        node_index,node_attr = self._get_node_index_attr(time_index)\n        _timestamp = self._get_timestamp(time_index)\n\n        snapshot = Data(\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            node_index=node_index,\n            node_attr=node_attr,\n            timestamp = _timestamp\n        )\n        return snapshot\n\n    def __next__(self):\n        if self.t < self.snapshot_count:\n            snapshot = self[self.t]\n            self.t = self.t + 1\n            return snapshot\n        else:\n            self.t = 0\n            raise StopIteration\n\n    def __iter__(self):\n        self.t = 0\n        return self\n    \n    def __len__(self):\n        return self.snapshot_count\n\n\nclass GraphDatasetLoader(object):\n    def __init__(self,input_path=\"\"):\n        self.input_path = input_path\n        self._read_data()\n    \n    def _read_data(self):\n        self._dataset = np.load(self.input_path)\n\n    def get_dataset(self): # -> DynamicGraphTemporalSignal:\n        dataset = GraphSignal(\n            edge_flag = self._dataset['edge_flag'],\n            edge_index = self._dataset['edge_index'],\n            edge_attr = self._dataset['edge_attr'],\n            node_flag = self._dataset['node_flag'],\n            node_index = self._dataset['node_index'],\n            node_attr = self._dataset['node_attr'],\n            ts_list = self._dataset['timestamp']\n        )\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:50.274692Z","iopub.execute_input":"2023-07-26T10:51:50.275116Z","iopub.status.idle":"2023-07-26T10:51:50.301376Z","shell.execute_reply.started":"2023-07-26T10:51:50.275078Z","shell.execute_reply":"2023-07-26T10:51:50.299906Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## MultiNNConv","metadata":{}},{"cell_type":"code","source":"class MultiNNConv(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, edge_channels, gcn_layers, edge_layers, lin_layers):\n        super().__init__()\n        \n        def _create_edge_net(edge_out_channels):\n            edge_nn = Seq()\n            pre_size = edge_channels\n            for size in edge_layers:\n                edge_nn.append(Lin(pre_size,size))\n                edge_nn.append(ReLU())\n                pre_size = size\n            edge_nn.append(Lin(pre_size,edge_out_channels)\n            return edge_nn\n        \n        self.convs = nn.ModuleList()\n        pre_size = in_channels\n        for size in gcn_layers:\n            edge_net = _create_edge_net(pre_size*size)\n            self.convs.append(NNConv(pre_size, size, edge_net, aggr='mean'))\n            pre_size = size\n        \n        self.lin_net = Seq()\n        for size in lin_layers[:-1]:\n            self.lin_net.append(Lin(pre_size,size))\n            pre_size = size\n        self.lin_net.append(ReLU())\n        self.lin_net.append(Lin(pre_size,out_channels))\n\n    def forward(self, x, edge_index, edge_attr):\n        out = x\n        with profiler.record_function(\"Graph Conv\"):\n            for conv in self.convs:\n                out = conv(\n                    x=out,\n                    edge_index=edge_index,\n                    edge_attr=edge_attr,\n                )\n        out = self.lin_net(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:50.303420Z","iopub.execute_input":"2023-07-26T10:51:50.303752Z","iopub.status.idle":"2023-07-26T10:51:50.320312Z","shell.execute_reply.started":"2023-07-26T10:51:50.303721Z","shell.execute_reply":"2023-07-26T10:51:50.319318Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Graph GRU Unit","metadata":{}},{"cell_type":"code","source":"class NNConvGRU(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        edge_channels: int,\n        gcn_layers: List,\n        edge_layers: List,\n        lin_layers: List,\n        normalization: str = \"sym\",\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.edge_channels = edge_channels\n        self.gcn_layers = gcn_layers\n        self.edge_layers = edge_layers\n        self.lin_layers = lin_layers\n            \n        self.normalization = normalization\n        self.bias = bias\n        self._create_parameters_and_layers()\n\n    def _create_update_gate_parameters_and_layers(self):\n\n        self.conv_x_z = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_z = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n        \n    def _create_reset_gate_parameters_and_layers(self):\n\n        self.conv_x_r = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_r = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n    def _create_candidate_state_parameters_and_layers(self):\n\n        self.conv_x_h = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_h = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            gcn_layers = self.gcn_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n    def _create_parameters_and_layers(self):\n        self._create_update_gate_parameters_and_layers()\n        self._create_reset_gate_parameters_and_layers()\n        self._create_candidate_state_parameters_and_layers()\n\n    def _set_hidden_state(self, X, H):\n        if H is None:\n            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n        return H\n\n    def _calculate_update_gate(self, X, edge_index, edge_attr, H):\n        Z = self.conv_x_z(X, edge_index, edge_attr)\n        Z = Z + self.conv_h_z(H, edge_index, edge_attr)\n        Z = torch.sigmoid(Z)\n        return Z\n\n    def _calculate_reset_gate(self, X, edge_index, edge_attr, H):\n        R = self.conv_x_r(X, edge_index, edge_attr)\n        R = R + self.conv_h_r(H, edge_index, edge_attr)\n        R = torch.sigmoid(R)\n        return R\n\n    def _calculate_candidate_state(self, X, edge_index, edge_attr, H, R):\n        H_tilde = self.conv_x_h(X, edge_index, edge_attr)\n        H_tilde = H_tilde + self.conv_h_h(H * R, edge_index, edge_attr)\n        H_tilde = torch.tanh(H_tilde)\n        return H_tilde\n\n    def _calculate_hidden_state(self, Z, H, H_tilde):\n        H = Z * H + (1 - Z) * H_tilde\n        return H\n\n    def forward(\n            self,\n            X: torch.FloatTensor,\n            edge_index: torch.LongTensor,\n            edge_attr: torch.FloatTensor,\n            H: torch.FloatTensor = None,\n        ) -> torch.FloatTensor:\n            H = self._set_hidden_state(X, H)\n            Z = self._calculate_update_gate(X, edge_index, edge_attr, H)\n            R = self._calculate_reset_gate(X, edge_index, edge_attr, H)\n            H_tilde = self._calculate_candidate_state(X, edge_index, edge_attr, H, R)\n            H = self._calculate_hidden_state(Z, H, H_tilde)\n            return H","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:50.321723Z","iopub.execute_input":"2023-07-26T10:51:50.322476Z","iopub.status.idle":"2023-07-26T10:51:50.343137Z","shell.execute_reply.started":"2023-07-26T10:51:50.322443Z","shell.execute_reply":"2023-07-26T10:51:50.342204Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Graph GRU Layer and Global Hidden Function","metadata":{}},{"cell_type":"code","source":"def create_hidden(num_node, out_channels):\n#     hidden_global = torch.FloatTensor(np.zeros([num_node,out_channels])).to(device)\n    hidden =torch.zeros([num_node,out_channels], dtype=torch.float).to(device)\n    return hidden\n\ndef select_hidden(hidden, index):\n#     h = hidden_global[index] #REGULAR INDEXING\n    h = hidden.index_select(dim=0, index=index) #INDEX SELECT\n    return h\n\n# TODO: Aggregation of hidden and cell\ndef update_hidden(hidden, h, index):\n    # hidden_global[index] = h.detach() #REGULAR INDEXING\n    # for key,value in mapping.items():\n    #     hidden_global[value] = h[key] \n    hidden.index_copy_(dim=0, index=index, source=h.detach())        \n\nclass NNConvGRULayer(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        edge_channels: int,\n        gcn_layers: List,\n        edge_layers: List,\n        lin_layers: List,\n        normalization: str = \"sym\",\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.gru = NNConvGRU(\n            in_channels = in_channels,\n            out_channels = out_channels,\n            edge_channels = edge_channels,\n            gcn_layers = gcn_layers,\n            edge_layers = edge_layers,\n            lin_layers = lin_layers,\n            normalization = normalization,\n            bias = bias\n        )\n        \n        self.out_channels = out_channels\n        \n    def forward(\n        self,\n        X: torch.FloatTensor,\n        node_index: torch.LongTensor,\n        node_flag: torch.LongTensor,\n        edge_index: torch.LongTensor,\n        edge_attr: torch.FloatTensor,\n        edge_flag: torch.LongTensor,\n        num_node: int\n    ) -> torch.FloatTensor:\n        \n        X_split = torch.tensor_split(X, node_flag)\n        node_index_split = torch.tensor_split(node_index, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        edge_attr_split = torch.tensor_split(edge_attr, edge_flag)\n        \n        hidden = create_hidden(num_node, self.out_channels)\n        \n        outs = []\n        for _X,_node_index,_edge_index,_edge_attr in zip(X_split,node_index_split,edge_index_split,edge_attr_split):\n            _hidden = select_hidden(hidden, _node_index)\n            _new_hidden = self.gru(_X, _edge_index, _edge_attr, _hidden)\n            update_hidden(hidden, _new_hidden, _node_index)\n            outs.append(_new_hidden)\n        \n        H = torch.cat(outs)\n        return H","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:50.344408Z","iopub.execute_input":"2023-07-26T10:51:50.344911Z","iopub.status.idle":"2023-07-26T10:51:50.359177Z","shell.execute_reply.started":"2023-07-26T10:51:50.344880Z","shell.execute_reply":"2023-07-26T10:51:50.358001Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## TGAE","metadata":{}},{"cell_type":"code","source":"class TGAE(torch.nn.Module): # Not Heterogeneous\n    def __init__(\n        self, in_channels, out_channels, edge_channels, \n        embedding_hidden_nums, gnn_out_channels, deciding_hidden_nums,\n        gru_gcn_hidden_nums, gru_edge_hidden_nums, gru_lin_hidden_nums):\n        super(TGAE, self).__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.edge_channels = edge_channels\n        self.embedding_hidden_nums = embedding_hidden_nums\n        self.gnn_out_channels = gnn_out_channels\n        self.deciding_hidden_nums = deciding_hidden_nums\n        self.gru_gcn_hidden_nums = gru_gcn_hidden_nums\n        self.gru_edge_hidden_nums = gru_edge_hidden_nums\n        self.gru_lin_hidden_nums = gru_lin_hidden_nums\n        \n        # Encoder\n        layers = []\n        pre_h_num = in_channels\n        for h_num in embedding_hidden_nums[:-1]:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,embedding_hidden_nums[-1]))\n        self.encoder_embedding_net = Seq(*layers)\n        \n        self.encoder_gru = NNConvGRULayer(\n            in_channels=embedding_hidden_nums[-1],\n            out_channels=gnn_out_channels,\n            edge_channels=edge_channels,\n            gcn_layers=gru_gcn_hidden_nums,\n            edge_layers=gru_edge_hidden_nums,\n            lin_layers=gru_lin_hidden_nums,\n        )\n\n        layers = []\n        pre_h_num = gnn_out_channels\n        for h_num in deciding_hidden_nums:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,out_channels))\n        self.encoder_deciding_net = Seq(*layers)\n        \n        # Decoder\n        layers = []\n        pre_h_num = out_channels\n        for h_num in deciding_hidden_nums[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,gnn_out_channels))\n        self.decoder_deciding_net = Seq(*layers)\n        \n        self.decoder_gru = NNConvGRULayer(\n            in_channels=gnn_out_channels,\n            out_channels=embedding_hidden_nums[-1],\n            edge_channels=edge_channels,\n            gcn_layers=gru_gcn_hidden_nums,\n            edge_layers=gru_edge_hidden_nums,\n            lin_layers=gru_lin_hidden_nums,\n        )\n        \n        layers = []\n        pre_h_num = embedding_hidden_nums[-1]\n        for h_num in embedding_hidden_nums[:-1][::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,in_channels))\n        self.decoder_embedding_net = Seq(*layers)\n        \n\n    def forward(self, x, node_index, node_flag, edge_index, edge_attr, edge_flag, num_node):\n        # Encoder\n        out = self.encoder_embedding_net(x)\n        \n        # GNN layer\n        with profiler.record_function(\"gru encoder\"):\n            h_encoder = self.encoder_gru(out, node_index, node_flag, edge_index, edge_attr, edge_flag, num_node) \n        \n        out = self.encoder_deciding_net(h_encoder)\n\n        out = self.decoder_deciding_net(out)\n        \n        # TODO Reverse Edge Index \n        with profiler.record_function(\"gru decoder\"):\n            h_decoder = self.decoder_gru(out, node_index, node_flag, torch.flip(edge_index,dims=(0,)), edge_attr, edge_flag, num_node)\n        \n        out = self.decoder_embedding_net(h_decoder)\n\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:50.360701Z","iopub.execute_input":"2023-07-26T10:51:50.361047Z","iopub.status.idle":"2023-07-26T10:51:50.380329Z","shell.execute_reply.started":"2023-07-26T10:51:50.361011Z","shell.execute_reply":"2023-07-26T10:51:50.379395Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Read Data","metadata":{}},{"cell_type":"code","source":"data_dir_0 = '/kaggle/input/dissertation-data'\ndata_dir_1 = '2021-09-11-umbrella-experiment-32run-fran'\n\n\nsignals = []\ny = []\nwith open(os.path.join(data_dir_0, data_dir_1, \"annotated.json\")) as f:\n    annotated_dict = json.load(f)\n\nfor data_dir_2 in os.listdir(os.path.join(data_dir_0, data_dir_1)):\n    if data_dir_2 == \"annotated.json\":\n        continue\n    r = re.compile(\".*.npz\")\n    graph_files = list(filter(r.match, os.listdir(os.path.join(data_dir_0, data_dir_1, data_dir_2))))\n\n    if len(graph_files) > 1:\n        print(\"Multiple Graph Files!\")\n        raise\n    if len(graph_files) == 0:\n        print(\"Not Found Graph File!\")\n        raise\n\n    dataloader = GraphDatasetLoader(os.path.join(data_dir_0, data_dir_1, data_dir_2, graph_files[0]))\n    signal = dataloader.get_dataset()\n    signals.append(signal)\n    y.append(annotated_dict[data_dir_2])\n\n# split train and test dataset\nsignals_train, signals_test, y_train, y_test = train_test_split(signals, y, test_size=0.2, random_state=1)\nsignals_train, signals_val, y_train, y_val = train_test_split(signals_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-26T10:51:50.385046Z","iopub.execute_input":"2023-07-26T10:51:50.385446Z","iopub.status.idle":"2023-07-26T10:51:54.741620Z","shell.execute_reply.started":"2023-07-26T10:51:50.385423Z","shell.execute_reply":"2023-07-26T10:51:54.740692Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"node_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nnode_attr_encoder = node_attr_encoder.fit(np.concatenate([sample.node_attr for sample in signals_train]))\n\nedge_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nedge_attr_encoder = edge_attr_encoder.fit(np.concatenate([sample.edge_attr for sample in signals_train]))\n\nfor signal in signals_train:\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n\nX_train = []\nfor signal in signals_train:\n#     X_train.append(list(signal))\n    X_train.append(signal.node_attr_encoded.index_select(dim=0,index=signal.node_index))\n    \nnum_node_train = []\nfor signal in signals_train:\n    num_node_train.append(signal.node_attr.shape[0])\n\n    \nfor signal in signals_val:\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n\nX_val = []\n# for signal in signals_val:\n#     X_val.append(list(signal))\nfor signal in signals_val:\n#     X_train.append(list(signal))\n    X_val.append(signal.node_attr_encoded.index_select(dim=0,index=signal.node_index))\n\nnums_node_val = []\nfor signal in signals_val:\n    nums_node_val.append(signal.node_attr.shape[0])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-26T10:51:54.742957Z","iopub.execute_input":"2023-07-26T10:51:54.743317Z","iopub.status.idle":"2023-07-26T10:51:57.812678Z","shell.execute_reply.started":"2023-07-26T10:51:54.743285Z","shell.execute_reply":"2023-07-26T10:51:57.811705Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for sample in X_train[:1]:\n#     for snapshot in sample:\n#         node_attr = snapshot.node_attr\n#         node_index = snapshot.node_index\n#         edge_attr = snapshot.edge_attr\n#         edge_index = snapshot.edge_index\n        \n#         print(f\"node_attr: {node_attr.shape}\")\n#         print(f\"node_index: {node_index.shape}\")\n#         print(f\"edge_attr: {edge_attr.shape}\")\n#         print(f\"edge_index: {edge_index.shape}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-26T10:51:57.814135Z","iopub.execute_input":"2023-07-26T10:51:57.814591Z","iopub.status.idle":"2023-07-26T10:51:57.820754Z","shell.execute_reply.started":"2023-07-26T10:51:57.814552Z","shell.execute_reply":"2023-07-26T10:51:57.819774Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"IN_CHANNELS = signals_train[0].node_attr_encoded.shape[1]\nEDGE_CHANNELS = signals_train[0].edge_attr_encoded.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.822332Z","iopub.execute_input":"2023-07-26T10:51:57.822976Z","iopub.status.idle":"2023-07-26T10:51:57.839731Z","shell.execute_reply.started":"2023-07-26T10:51:57.822944Z","shell.execute_reply":"2023-07-26T10:51:57.838729Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = TGAE(\n    in_channels=IN_CHANNELS, \n    out_channels=5, \n    edge_channels=EDGE_CHANNELS, \n    embedding_hidden_nums=[16,32],\n    gnn_out_channels=8,\n    deciding_hidden_nums=[16,16],\n    gru_gcn_hidden_nums=[16,16],\n    gru_edge_hidden_nums=[32],\n    gru_lin_hidden_nums=[],\n)\n\nloss_f = torch.nn.MSELoss(reduction = 'none')\noptimizer = torch.optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5)\n\nmodel = model.to(device)\nloss_f = loss_f.to(device)\nprint(model)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-26T10:51:57.841314Z","iopub.execute_input":"2023-07-26T10:51:57.841709Z","iopub.status.idle":"2023-07-26T10:51:57.902302Z","shell.execute_reply.started":"2023-07-26T10:51:57.841677Z","shell.execute_reply":"2023-07-26T10:51:57.901282Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"TGAE(\n  (encoder_embedding_net): Sequential(\n    (0): Linear(in_features=5, out_features=16, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=16, out_features=32, bias=True)\n  )\n  (encoder_gru): NNConvGRULayer(\n    (gru): NNConvGRU(\n      (conv_x_z): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n    )\n  )\n  (encoder_deciding_net): Sequential(\n    (0): Linear(in_features=8, out_features=16, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=16, out_features=16, bias=True)\n    (3): LeakyReLU(negative_slope=0.01)\n    (4): Linear(in_features=16, out_features=5, bias=True)\n  )\n  (decoder_deciding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=5, out_features=16, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=16, out_features=16, bias=True)\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Linear(in_features=16, out_features=8, bias=True)\n  )\n  (decoder_gru): NNConvGRULayer(\n    (gru): NNConvGRU(\n      (conv_x_z): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (gcn_layers): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n    )\n  )\n  (decoder_embedding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=32, out_features=16, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Linear(in_features=16, out_features=5, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"next(model.decoder_gru.gru.conv_x_z.gcn_layers[0].nn.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.903620Z","iopub.execute_input":"2023-07-26T10:51:57.904173Z","iopub.status.idle":"2023-07-26T10:51:57.912667Z","shell.execute_reply.started":"2023-07-26T10:51:57.904142Z","shell.execute_reply":"2023-07-26T10:51:57.911684Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train Loop","metadata":{}},{"cell_type":"code","source":"# train_loop\ndef train_loop(signal, model, loss_fn, optimizer, device):\n    model.train()\n    \n    X = signal.node_attr_encoded.index_select(dim=0,index=signal.node_index)\n    node_index = signal.node_index\n    node_flag = signal.node_flag\n    edge_index = signal.edge_index\n    edge_attr = signal.edge_attr_encoded\n    edge_flag = signal.edge_flag\n    \n    outs = model(X, node_index, node_flag, edge_index, edge_attr, edge_flag, signal.node_attr.shape[0])\n\n    train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n    total_loss = torch.sum(train_losses)\n    snapshot_losses = train_losses.detach().index_select(dim=0,index=signal.node_index).cpu().numpy()\n    \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    return snapshot_losses","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.914032Z","iopub.execute_input":"2023-07-26T10:51:57.914753Z","iopub.status.idle":"2023-07-26T10:51:57.923845Z","shell.execute_reply.started":"2023-07-26T10:51:57.914723Z","shell.execute_reply":"2023-07-26T10:51:57.922905Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Test Loop","metadata":{}},{"cell_type":"code","source":"# test_loop\ndef test_loop(signal, model, loss_fn, optimizer, device):\n    with torch.no_grad():\n        X = signal.node_attr_encoded.index_select(dim=0,index=signal.node_index)\n        node_index = signal.node_index\n        node_flag = signal.node_flag\n        edge_index = signal.edge_index\n        edge_attr = signal.edge_attr_encoded\n        edge_flag = signal.edge_flag\n\n        outs = model(X, node_index, node_flag, edge_index, edge_attr, edge_flag, signal.node_attr.shape[0])\n\n        train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n        total_loss = torch.sum(train_losses)\n        snapshot_losses = [torch.sum(loss).cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n    \n    return snapshot_losses","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.925457Z","iopub.execute_input":"2023-07-26T10:51:57.925781Z","iopub.status.idle":"2023-07-26T10:51:57.934711Z","shell.execute_reply.started":"2023-07-26T10:51:57.925752Z","shell.execute_reply":"2023-07-26T10:51:57.933720Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"GLOBAL_EPOCH = 0\ndef train_function(num_epoch):\n    history_train = []\n    history_val = []\n\n    for i_epoch in range(1,num_epoch+1):\n        train_losses = []\n        _start = time.time()\n        for signal in signals_train[:2]:\n            snapshot_losses = train_loop(signal, model, loss_f, optimizer, device)\n#             print(np.mean(train_loss))\n            train_losses.append(np.mean(snapshot_losses))\n            \n        \n        if i_epoch % 5 == 0:\n            val_losses = []\n            for signal in signals_val:\n                snapshot_losses = test_loop(signal, model, loss_f, optimizer, device)\n#                 print(np.mean(val_loss))\n                val_losses.append(np.mean(snapshot_losses))\n            _end = time.time()\n            \n            history_train.append(np.mean(train_losses))\n            history_val.append(np.mean(val_losses))\n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train RMSE {np.mean(train_losses):.4f} val RMSE{np.mean(val_losses):.4f}\")\n        else:\n            _end = time.time()\n            history_train.append(np.mean(train_losses))\n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train RMSE {np.mean(train_losses):.4f}\")\n    return (history_train,history_val)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.936068Z","iopub.execute_input":"2023-07-26T10:51:57.936434Z","iopub.status.idle":"2023-07-26T10:51:57.949301Z","shell.execute_reply.started":"2023-07-26T10:51:57.936403Z","shell.execute_reply":"2023-07-26T10:51:57.948331Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"history_train_list = []\nhistory_val_list = []","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.950602Z","iopub.execute_input":"2023-07-26T10:51:57.951009Z","iopub.status.idle":"2023-07-26T10:51:57.962913Z","shell.execute_reply.started":"2023-07-26T10:51:57.950979Z","shell.execute_reply":"2023-07-26T10:51:57.961960Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# history_train, history_val = train_function(10)\n# history_train_list += history_train\n# history_val_list += history_val\n# plt.plot(history_train_list[1::5],label=\"Train\")\n# plt.plot(history_val_list,label=\"Val\")\n# plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.964137Z","iopub.execute_input":"2023-07-26T10:51:57.965285Z","iopub.status.idle":"2023-07-26T10:51:57.973919Z","shell.execute_reply.started":"2023-07-26T10:51:57.965253Z","shell.execute_reply":"2023-07-26T10:51:57.972898Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], use_cuda=True, record_shapes=True) as prof:\n        history_train, history_val = train_function(1)\n        history_train_list += history_train\n        history_val_list += history_val\n        plt.plot(history_train_list[1::5],label=\"Train\")\n        plt.plot(history_val_list,label=\"Val\")\n        plt.legend()\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:51:57.975409Z","iopub.execute_input":"2023-07-26T10:51:57.976094Z","iopub.status.idle":"2023-07-26T10:53:08.158689Z","shell.execute_reply.started":"2023-07-26T10:51:57.976062Z","shell.execute_reply":"2023-07-26T10:53:08.157012Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/profiler/profiler.py:433: UserWarning: use_cuda is deprecated, use activities argument instead\n  warn(\"use_cuda is deprecated, use activities argument instead\")\nSTAGE:2023-07-26 10:51:58 28:28 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n","output_type":"stream"},{"name":"stdout","text":"1/1: cost 5.3519s train RMSE 1.0059\n","output_type":"stream"},{"name":"stderr","text":"STAGE:2023-07-26 10:52:04 28:28 ActivityProfilerController.cpp:317] Completed Stage: Collection\nSTAGE:2023-07-26 10:52:04 28:28 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n    autograd::engine::evaluate_function: AddmmBackward0         1.50%      75.686ms         9.47%     477.474ms     116.457us       0.000us         0.00%     265.833ms      64.837us          4100  \n                                               aten::mm         3.44%     173.251ms         5.05%     254.385ms      22.733us     214.001ms        32.70%     223.578ms      19.980us         11190  \n                                         AddmmBackward0         0.79%      39.962ms         4.51%     227.408ms      55.465us       0.000us         0.00%     191.374ms      46.677us          4100  \n                                             Graph Conv         9.35%     471.372ms        30.62%        1.544s       1.892ms       0.000us         0.00%     184.301ms     225.859us           816  \n                                              aten::bmm         2.02%     101.772ms         2.81%     141.430ms      30.586us     152.359ms        23.28%     156.146ms      33.769us          4624  \n      autograd::engine::evaluate_function: BmmBackward0         0.38%      19.152ms         2.53%     127.332ms      78.022us       0.000us         0.00%     136.334ms      83.538us          1632  \n                                           BmmBackward0         0.27%      13.755ms         2.14%     107.877ms      66.101us       0.000us         0.00%     136.037ms      83.356us          1632  \n                                            gru decoder         0.65%      32.804ms        17.14%     863.971ms     431.986ms       0.000us         0.00%     110.515ms      55.258ms             2  \n                                maxwell_sgemm_128x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us     105.878ms        16.18%     105.878ms      65.845us          1608  \n                                            gru encoder         0.71%      35.859ms        17.85%     899.894ms     449.947ms       0.000us         0.00%      96.196ms      48.098ms             2  \n                                           aten::linear         0.58%      29.361ms        42.75%        2.155s     375.957us       0.000us         0.00%      90.027ms      15.706us          5732  \n                                  sgemm_32x32x32_TN_vec         0.00%       0.000us         0.00%       0.000us       0.000us      83.814ms        12.81%      83.814ms      21.101us          3972  \n                                            aten::addmm         4.25%     214.100ms        40.05%        2.019s     492.506us      51.856ms         7.92%      80.739ms      19.692us          4100  \n                                              aten::sum         2.09%     105.264ms         3.03%     152.849ms      26.647us      71.809ms        10.97%      77.734ms      13.552us          5736  \nvoid at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      70.804ms        10.82%      70.804ms      12.735us          5560  \n                                       cudaLaunchKernel         9.55%     481.616ms         9.55%     481.616ms       7.082us      55.820ms         8.53%      55.820ms       0.821us         68008  \n                                             aten::add_         1.86%      93.975ms         3.50%     176.552ms      13.487us      37.765ms         5.77%      47.297ms       3.613us         13091  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      45.107ms         6.89%      45.107ms       2.998us         15048  \n                                 maxwell_sgemm_64x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us      42.354ms         6.47%      42.354ms      60.853us           696  \n                                      sgemm_32x32x32_TN         0.00%       0.000us         0.00%       0.000us       0.000us      41.516ms         6.34%      41.516ms      47.610us           872  \n                                maxwell_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us      34.840ms         5.32%      34.840ms      11.613us          3000  \n                                  sgemm_32x32x32_NN_vec         0.00%       0.000us         0.00%       0.000us       0.000us      34.426ms         5.26%      34.426ms       9.725us          3540  \n                                           aten::matmul         0.54%      27.011ms         3.61%     182.148ms      55.805us       0.000us         0.00%      30.577ms       9.368us          3264  \nvoid gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      28.082ms         4.29%      28.082ms      21.274us          1320  \n                                     aten::scatter_add_         1.34%      67.487ms         1.80%      90.500ms      27.727us      25.073ms         3.83%      27.585ms       8.451us          3264  \nvoid at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us      25.073ms         3.83%      25.073ms       7.914us          3168  \nautograd::engine::evaluate_function: IndexSelectBack...         0.38%      19.083ms         2.23%     112.662ms      82.840us       0.000us         0.00%      24.721ms      18.177us          1360  \n          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.16%       8.075ms         0.16%       8.075ms       0.304us      24.295ms         3.71%      24.295ms       0.913us         26600  \n      autograd::engine::evaluate_function: AddBackward0         0.74%      37.214ms         1.92%      97.015ms      25.477us       0.000us         0.00%      24.164ms       6.346us          3808  \n       autograd::engine::evaluate_function: MmBackward0         0.29%      14.518ms         2.22%     111.733ms      68.464us       0.000us         0.00%      22.420ms      13.738us          1632  \n                                            MmBackward0         0.34%      16.986ms         1.86%      93.723ms      57.428us       0.000us         0.00%      21.381ms      13.101us          1632  \n        autograd::engine::evaluate_function: TBackward0         1.00%      50.163ms         3.30%     166.199ms      28.995us       0.000us         0.00%      20.899ms       3.646us          5732  \n                                            aten::fill_         0.88%      44.311ms         1.90%      96.003ms      13.503us      14.130ms         2.16%      19.613ms       2.759us          7110  \n                            aten::index_select_backward         0.13%       6.358ms         1.42%      71.597ms      52.645us       0.000us         0.00%      19.416ms      14.276us          1360  \n                                   IndexSelectBackward0         0.12%       5.835ms         1.52%      76.691ms      56.390us       0.000us         0.00%      19.328ms      14.212us          1360  \nvoid gemv2N_kernel<int, int, float, float, float, fl...         0.00%       0.000us         0.00%       0.000us       0.000us      18.326ms         2.80%      18.326ms      12.120us          1512  \n                                              aten::add         1.18%      59.274ms         1.69%      85.210ms      20.562us      13.613ms         2.08%      17.480ms       4.218us          4144  \n                                              aten::div         1.11%      56.197ms         1.55%      78.059ms      23.900us      13.651ms         2.09%      16.090ms       4.927us          3266  \n                                       aten::index_add_         0.34%      17.162ms         0.51%      25.797ms      18.968us      15.021ms         2.30%      15.947ms      11.726us          1360  \n                                            aten::zero_         0.34%      17.253ms         1.91%      96.255ms      17.571us       0.000us         0.00%      15.057ms       2.749us          5478  \nvoid at::native::indexFuncLargeIndex<float, long, un...         0.00%       0.000us         0.00%       0.000us       0.000us      14.501ms         2.22%      14.501ms      11.509us          1260  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      14.130ms         2.16%      14.130ms       2.041us          6924  \nvoid at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      13.651ms         2.09%      13.651ms       4.306us          3170  \n                                 maxwell_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      12.745ms         1.95%      12.745ms      19.252us           662  \n                                        aten::new_zeros         0.40%      20.035ms         2.84%     143.081ms      30.943us       0.000us         0.00%      12.347ms       2.670us          4624  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.889ms         1.82%      11.889ms       3.002us          3960  \nautograd::engine::evaluate_function: ScatterAddBackw...         0.19%       9.598ms         1.22%      61.319ms      37.573us       0.000us         0.00%      10.364ms       6.350us          1632  \n                                           aten::gather         0.60%      30.452ms         0.87%      43.954ms      26.933us       9.465ms         1.45%      10.364ms       6.350us          1632  \n                                     aten::index_select         0.84%      42.287ms         1.54%      77.529ms      43.752us       8.478ms         1.30%      10.275ms       5.799us          1772  \n                                    ScatterAddBackward0         0.15%       7.767ms         0.97%      49.083ms      30.075us       0.000us         0.00%       9.748ms       5.973us          1632  \n                                             aten::relu         0.50%      25.181ms         1.62%      81.787ms      33.410us       0.000us         0.00%       9.540ms       3.897us          2448  \n                                        aten::clamp_min         0.76%      38.546ms         1.12%      56.606ms      23.123us       7.137ms         1.09%       9.540ms       3.897us          2448  \nvoid at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us       9.465ms         1.45%       9.465ms       5.975us          1584  \n     autograd::engine::evaluate_function: ReluBackward0         0.36%      17.981ms         1.57%      79.248ms      32.373us       0.000us         0.00%       9.208ms       3.761us          2448  \n                               aten::threshold_backward         0.68%      34.095ms         0.98%      49.595ms      20.259us       7.185ms         1.10%       9.208ms       3.761us          2448  \n                                          ReluBackward0         0.23%      11.672ms         1.21%      60.880ms      24.869us       0.000us         0.00%       9.144ms       3.735us          2448  \n                                  sgemm_32x32x32_NT_vec         0.00%       0.000us         0.00%       0.000us       0.000us       9.102ms         1.39%       9.102ms       6.020us          1512  \n      autograd::engine::evaluate_function: DivBackward0         0.32%      16.149ms         1.13%      56.817ms      34.814us       0.000us         0.00%       8.515ms       5.218us          1632  \n                                           DivBackward0         0.12%       5.883ms         0.80%      40.532ms      24.836us       0.000us         0.00%       8.469ms       5.189us          1632  \nvoid at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       7.409ms         1.13%       7.409ms       4.900us          1512  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.185ms         1.10%       7.185ms       3.024us          2376  \nvoid at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.271ms         0.96%       6.271ms       3.959us          1584  \n                                            aten::clamp         0.47%      23.698ms         0.66%      33.462ms      20.504us       4.752ms         0.73%       6.202ms       3.800us          1632  \n                                         aten::new_ones         0.12%       5.980ms         0.74%      37.374ms      22.901us       0.000us         0.00%       4.446ms       2.724us          1632  \n                                              aten::mul         0.29%      14.491ms         0.40%      20.218ms      21.193us       2.777ms         0.42%       3.482ms       3.650us           954  \n                                maxwell_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       3.386ms         0.52%       3.386ms      24.536us           138  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.769ms         0.42%       2.769ms       2.997us           924  \n      autograd::engine::evaluate_function: MulBackward0         0.07%       3.382ms         0.32%      16.203ms      39.713us       0.000us         0.00%       2.380ms       5.833us           408  \n                                           MulBackward0         0.03%       1.729ms         0.21%      10.527ms      25.801us       0.000us         0.00%       1.886ms       4.623us           408  \n                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.632ms         0.25%       1.632ms       1.000us          1632  \n                               Optimizer.step#Adam.step         0.34%      17.314ms         1.24%      62.291ms      31.145ms       0.000us         0.00%       1.290ms     645.000us             2  \n    autograd::engine::evaluate_function: SliceBackward0         0.03%       1.266ms         0.18%       9.264ms      68.118us       0.000us         0.00%       1.269ms       9.331us           136  \n                                          aten::sigmoid         0.11%       5.646ms         0.15%       7.385ms      27.151us     835.000us         0.13%       1.104ms       4.059us           272  \nautograd::engine::evaluate_function: SigmoidBackward...         0.04%       2.073ms         0.19%       9.327ms      34.290us       0.000us         0.00%       1.080ms       3.971us           272  \n                                 aten::sigmoid_backward         0.08%       3.954ms         0.11%       5.687ms      20.908us     797.000us         0.12%       1.080ms       3.971us           272  \n                                       SigmoidBackward0         0.03%       1.567ms         0.14%       7.237ms      26.607us       0.000us         0.00%       1.077ms       3.960us           272  \nvoid gemmSN_NN_kernel<float, 128, 2, 4, 8, 3, 4, fal...         0.00%       0.000us         0.00%       0.000us       0.000us     963.000us         0.15%     963.000us      13.375us            72  \n                                      aten::index_copy_         0.08%       4.034ms         0.10%       5.180ms      38.088us     845.000us         0.13%     934.000us       6.868us           136  \nvoid at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     845.000us         0.13%     845.000us       6.402us           132  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     835.000us         0.13%     835.000us       3.163us           264  \n                                        cudaMemsetAsync         0.23%      11.375ms         0.23%      11.375ms       6.970us     835.000us         0.13%     835.000us       0.512us          1632  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     797.000us         0.12%     797.000us       3.019us           264  \n                                   aten::slice_backward         0.01%     698.000us         0.12%       5.978ms      43.956us       0.000us         0.00%     785.000us       5.772us           136  \n                                            aten::empty         1.65%      83.198ms         2.36%     118.928ms       9.634us       0.000us         0.00%     757.000us       0.061us         12344  \n                                         SliceBackward0         0.01%     509.000us         0.12%       6.281ms      46.184us       0.000us         0.00%     755.000us       5.551us           136  \nvoid gemmSN_TN_kernel<float, 128, 16, 2, 4, 4, 4, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     727.000us         0.11%     727.000us       5.049us           144  \n                                       aten::zeros_like         0.02%       1.160ms         0.17%       8.332ms      22.042us       0.000us         0.00%     702.000us       1.857us           378  \ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.01%     670.000us         0.01%     670.000us       0.908us     680.000us         0.10%     680.000us       0.921us           738  \n     autograd::engine::evaluate_function: RsubBackward1         0.01%     656.000us         0.10%       4.886ms      35.926us       0.000us         0.00%     624.000us       4.588us           136  \n                                              aten::neg         0.06%       2.997ms         0.08%       3.882ms      28.544us     397.000us         0.06%     624.000us       4.588us           136  \n                                          RsubBackward1         0.01%     348.000us         0.08%       4.155ms      30.551us       0.000us         0.00%     612.000us       4.500us           136  \n                                             aten::tanh         0.04%       1.993ms         0.06%       2.839ms      20.875us     428.000us         0.07%     584.000us       4.294us           136  \n                                  cudaStreamIsCapturing         0.01%     283.000us         0.01%     283.000us       1.141us     530.000us         0.08%     530.000us       2.137us           248  \nvoid at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     521.000us         0.08%     521.000us       4.070us           128  \n                                            aten::copy_         0.01%     684.000us         0.04%       2.256ms       4.355us     437.000us         0.07%     505.000us       0.975us           518  \n     autograd::engine::evaluate_function: TanhBackward0         0.02%     877.000us         0.10%       4.814ms      35.397us       0.000us         0.00%     504.000us       3.706us           136  \n                                          TanhBackward0         0.01%     639.000us         0.08%       3.937ms      28.949us       0.000us         0.00%     504.000us       3.706us           136  \n                                    aten::tanh_backward         0.05%       2.391ms         0.07%       3.298ms      24.250us     411.000us         0.06%     504.000us       3.706us           136  \nvoid at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     500.000us         0.08%     500.000us       6.944us            72  \nvoid gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, fal...         0.00%       0.000us         0.00%       0.000us       0.000us     496.000us         0.08%     496.000us       5.167us            96  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 5.041s\nSelf CUDA time total: 654.462ms\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# print(prof.key_averages().table(sort_by=\"cpu_time_total\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:53:08.160636Z","iopub.execute_input":"2023-07-26T10:53:08.161689Z","iopub.status.idle":"2023-07-26T10:53:08.165997Z","shell.execute_reply.started":"2023-07-26T10:53:08.161654Z","shell.execute_reply":"2023-07-26T10:53:08.165040Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# fig = plt.figure(figsize=(20,20))    \n# for i_signal,(signal,y) in enumerate(zip(signals_val,y_val)):\n#     snapshot_losses = test_loop(signal, model, loss_f, optimizer, device)\n# #                 print(np.mean(val_loss))\n# #     val_losses.append(np.mean(snapshot_losses))\n#     ts_list = signal.ts_list\n#     plt.subplot(len(signals_val)+1,1,i_signal+1)\n# #     print(len(snapshot_losses))\n#     plt.plot(ts_list,snapshot_losses[:-1])\n#     if y[0] == 'dos':\n#         plt.axvline(x = float(y[1]), color = 'red', label = y[0])\n#     if y[0] == 'privesc':\n#         plt.axvline(x = float(y[1]), color = 'blue', label = y[0])\n# # plt.show()\n# plt.savefig('val_res.png')","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:53:08.167308Z","iopub.execute_input":"2023-07-26T10:53:08.167982Z","iopub.status.idle":"2023-07-26T10:53:08.178269Z","shell.execute_reply.started":"2023-07-26T10:53:08.167946Z","shell.execute_reply":"2023-07-26T10:53:08.177151Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# torch.save(model,f\"{GLOBAL_EPOCH}_{np.mean(history_train_list[-1]):.4f}_{np.mean(history_val_list[-1]):.4f}.model\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:53:08.179732Z","iopub.execute_input":"2023-07-26T10:53:08.180122Z","iopub.status.idle":"2023-07-26T10:53:08.189426Z","shell.execute_reply.started":"2023-07-26T10:53:08.180092Z","shell.execute_reply":"2023-07-26T10:53:08.188568Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# history_train, history_val = train_function(100)\n# history_train_list += history_train\n# history_val_list += history_val\n# plt.plot(history_train_list[1::5],label=\"Train\")\n# plt.plot(history_val_list,label=\"Val\")\n# plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:53:08.190876Z","iopub.execute_input":"2023-07-26T10:53:08.191399Z","iopub.status.idle":"2023-07-26T10:53:08.201074Z","shell.execute_reply.started":"2023-07-26T10:53:08.191368Z","shell.execute_reply":"2023-07-26T10:53:08.199905Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# states_encoder_val = []\n# states_decoder_val = []\n# for num_node in nums_node_val:\n#     states_encoder_val.append(create_hidden_global(num_node=num_node,out_channels=model.gnn_out_channels))\n#     states_decoder_val.append(create_hidden_global(num_node=num_node,out_channels=model.embedding_hidden_nums[-1]))    \n\n# fig = plt.figure(figsize=(20,20))    \n# for i_sample, (sample, y, hidden_encoder_global, hidden_decoder_global) in enumerate(zip(X_val, y_val, states_encoder_val, states_decoder_val)):\n#     val_loss = test_loop(sample, hidden_encoder_global, hidden_decoder_global, model, loss_f, optimizer, device)\n#     # print(np.mean(val_loss))\n#     ts_list = [snapshot.timestamp for snapshot in sample]\n#     plt.subplot(len(X_val)+1,1,i_sample+1)\n#     plt.plot(ts_list,val_loss)\n#     if y[0] == 'dos':\n#         plt.axvline(x = float(y[1]), color = 'red', label = y[0])\n#     if y[0] == 'privesc':\n#         plt.axvline(x = float(y[1]), color = 'blue', label = y[0])\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T10:53:08.202529Z","iopub.execute_input":"2023-07-26T10:53:08.202903Z","iopub.status.idle":"2023-07-26T10:53:08.211365Z","shell.execute_reply.started":"2023-07-26T10:53:08.202836Z","shell.execute_reply":"2023-07-26T10:53:08.210424Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}