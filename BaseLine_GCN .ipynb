{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_gpu = torch.cuda.is_available()\nprint(torch.__version__)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:40:28.070890Z","iopub.execute_input":"2023-08-28T15:40:28.071526Z","iopub.status.idle":"2023-08-28T15:40:32.011610Z","shell.execute_reply.started":"2023-08-28T15:40:28.071491Z","shell.execute_reply":"2023-08-28T15:40:32.010020Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.0.0\ncuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize for CPU & GPU","metadata":{}},{"cell_type":"code","source":"if use_gpu:\n    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric --no-index --find-links=file:///kaggle/input/torch-geometric\n#     !pip install torch_geometric_temporal  \nelse:\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_scatter-2.1.1pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_sparse-0.6.17pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_cluster-1.6.1pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_spline_conv-1.2.2pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_geometric-2.3.1-py3-none-any.whl\n#     !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n#     !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch_spline_conv -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n#     !pip install torch_geometric_temporal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-28T15:40:32.019459Z","iopub.execute_input":"2023-08-28T15:40:32.020857Z","iopub.status.idle":"2023-08-28T15:40:46.326568Z","shell.execute_reply.started":"2023-08-28T15:40:32.020797Z","shell.execute_reply":"2023-08-28T15:40:46.325358Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/torch-geometric\nProcessing /kaggle/input/torch-geometric/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_cluster-1.6.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_geometric-2.3.1-py3-none-any.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\nSuccessfully installed torch-cluster-1.6.1 torch-geometric-2.3.1 torch-scatter-2.1.1 torch-sparse-0.6.17 torch-spline-conv-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport json\nimport random\nfrom scipy.special import perm\nfrom itertools import combinations,chain\nfrom typing import List, Union\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nfrom torch import nn\nfrom torch.nn import Linear as Lin\nfrom torch.nn import ReLU, LeakyReLU\nfrom torch.nn import Sequential as Seq\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GINConv\nfrom torch_geometric.data import Batch\nfrom torch import autograd\nfrom torch_geometric.nn.models import InnerProductDecoder\nfrom torch_geometric.utils import to_dense_adj\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score\n\nfrom texttable import Texttable\nfrom itertools import product","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:40:46.334089Z","iopub.execute_input":"2023-08-28T15:40:46.334459Z","iopub.status.idle":"2023-08-28T15:40:47.789912Z","shell.execute_reply.started":"2023-08-28T15:40:46.334419Z","shell.execute_reply":"2023-08-28T15:40:47.788873Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"Edge_Flag = List[Union[np.ndarray, None]]\nEdge_Index = List[Union[np.ndarray, None]]\nEdge_Attr = List[Union[np.ndarray, None]]\n\nNode_Flag = List[Union[np.ndarray, None]]\nNode_Index = List[Union[np.ndarray, None]]\nNode_Attr = List[Union[np.ndarray, None]]\n\n\n\nclass GraphSignal(object):\n    # dynamic node static node attr dynamic edge and edge attr\n    def __init__(\n        self,\n        edge_flag: Edge_Flag,\n        edge_index: Edge_Index,\n        edge_attr: Edge_Attr,\n        node_flag: Node_Flag,\n        node_index: Node_Index,\n        node_attr: Node_Attr,\n        ts_list: List,\n        path: str,\n    ):\n        \n        self.raw_edge_flag = torch.LongTensor(edge_flag[:-1])\n        self.raw_edge_index = torch.LongTensor(edge_index).T\n        self.raw_edge_attr = edge_attr\n        self.raw_node_attr = node_attr \n        self.node_flag = torch.LongTensor(node_flag[:-1])\n        self.node_index = torch.LongTensor(node_index)\n        \n        self.ts_list = ts_list\n        \n        self.path = path\n        \n        self.node_attr = None\n        self.edge_flag = None\n        self.edge_index = None\n        \n        self.y = None\n        \n        self._set_snapshot_count()\n        self._set_node_count()\n\n    def _set_snapshot_count(self):\n        self.snapshot_count = len(self.ts_list)\n    \n    def _set_node_count(self):\n        self.node_count = self.raw_node_attr.shape[0]\n    \n    def encode_edge_attr(self, enocder):\n        self.edge_attr_encoded = torch.FloatTensor(enocder.transform(self.raw_edge_attr))\n        \n    def encode_node_attr(self, enocder):\n        self.node_attr_encoded = torch.FloatTensor(enocder.transform(self.raw_node_attr))\n        \n    def extend_node_attr(self):\n        if self.edge_attr_encoded is None:\n            print(\"Edge Attr Need to be Encoded!\")\n            raise\n        node_index = self.node_index\n        node_attr = self.node_attr_encoded.index_select(dim=0,index=self.node_index)\n        node_flag = self.node_flag\n        \n        edge_index = self.raw_edge_index\n        edge_attr = self.edge_attr_encoded\n        edge_flag = self.raw_edge_flag\n        \n        node_index_split = torch.tensor_split(node_index, node_flag)\n        node_attr_split = torch.tensor_split(node_attr, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        edge_attr_split = torch.tensor_split(edge_attr, edge_flag)\n\n        base = 0\n        new_node_attr = []\n        new_edge_flag = []\n        new_edge_index = []\n        \n        for i_snapshot in range(self.snapshot_count):\n            _node_index = node_index_split[i_snapshot]\n            _node_attr = node_attr_split[i_snapshot]\n            _edge_index = edge_index_split[i_snapshot]\n            _edge_attr = edge_attr_split[i_snapshot]\n\n            if _edge_index.shape[1] != _edge_attr.shape[0]:\n                print(i_snapshot, edge_index.shape, _edge_attr.shape)\n                raise\n            if _edge_index.shape[1] > 0:\n                index_dict = {}\n                for i_edge in range(_edge_index.shape[1]):\n                    index_tuple = tuple(_edge_index[:,i_edge].tolist())\n                    if index_tuple in index_dict:\n                        index_dict[index_tuple] += [i_edge]\n                    else:\n                        index_dict[index_tuple] = [i_edge]\n\n                _new_edge_index = []\n                _new_edge_attr = []\n                for key in index_dict.keys():\n                    _new_edge_index.append(key)\n                    _new_edge_attr.append(torch.sum(_edge_attr.index_select(0, torch.LongTensor(index_dict[key])),dim=0).unsqueeze(0))\n\n                _new_edge_index = torch.LongTensor(_new_edge_index).T\n                _new_edge_attr = torch.cat(_new_edge_attr,dim=0)\n                \n                base += _new_edge_index.shape[1]\n                new_edge_index.append(_new_edge_index)\n\n#                 _source_attr = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1])).to(device)\n#                 _target_attr = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1])).to(device)\n#                 _source_attr.index_add_(0, _new_edge_index[0], _new_edge_attr)\n#                 _target_attr.index_add_(0, _new_edge_index[1], _new_edge_attr)\n#                 new_node_attr.append(torch.cat([_node_attr,_source_attr,_target_attr], dim=1))\n\n                _node_attr_extend = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1]))\n                _node_attr_extend.index_add_(0, _new_edge_index[0], _new_edge_attr)\n                _node_attr_extend.index_add_(0, _new_edge_index[1], _new_edge_attr)\n                new_node_attr.append(torch.cat([_node_attr,_node_attr_extend], dim=1)) \n                \n            new_edge_flag.append(base)\n        \n        self.node_attr = torch.cat(new_node_attr, dim=0)\n        self.edge_flag = torch.LongTensor(new_edge_flag)\n        self.edge_index = torch.cat(new_edge_index,dim=1)\n    \n    def remove_init_stop(self, threshold, period,):\n        node_index = self.node_index\n        node_attr = self.node_attr\n        node_flag = self.node_flag\n\n        edge_index = self.edge_index\n        edge_flag = self.edge_flag\n        \n        raw_edge_index = self.raw_edge_index\n        raw_edge_attr = self.raw_edge_attr\n        raw_edge_flag = self.raw_edge_flag\n\n        node_index_split = torch.tensor_split(node_index, node_flag)\n        node_attr_split = torch.tensor_split(node_attr, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        \n        raw_edge_index_split = torch.tensor_split(raw_edge_index, raw_edge_flag, dim=1)\n        raw_edge_attr_split = np.split(raw_edge_attr, raw_edge_flag)\n\n        i_init = None\n        i_stop = None\n        for i_snapshot, node_num in enumerate(torch.diff(self.node_flag)):\n            if node_num > threshold:\n                i_init = i_snapshot+1\n                break\n\n        for i_snapshot, node_num in enumerate(torch.flip(torch.diff(self.node_flag),dims=[0])):\n            if node_num > threshold:\n                i_stop = self.node_flag.shape[0]-1-i_snapshot\n                break\n        \n        new_node_attr = torch.cat(node_attr_split[i_init+1+period:i_stop-period],dim=0)\n        new_node_index = torch.cat(node_index_split[i_init+1+period:i_stop-period],dim=0)\n        new_edge_index = torch.cat(edge_index_split[i_init+1+period:i_stop-period],dim=1)\n        new_raw_edge_index = torch.cat(raw_edge_index_split[i_init+1+period:i_stop-period], dim=1)\n        new_raw_edge_attr = np.concatenate(raw_edge_attr_split[i_init+1+period:i_stop-period])\n\n        new_node_flag = node_flag[i_init+period+1:i_stop-period-1]-node_flag[i_init+period]\n        new_edge_flag = edge_flag[i_init+period+1:i_stop-period-1]-edge_flag[i_init+period]\n        new_raw_edge_flag = raw_edge_flag[i_init+period+1:i_stop-period-1]-raw_edge_flag[i_init+period]\n        \n        self.node_attr = new_node_attr\n        self.node_index = new_node_index\n        self.edge_index = new_edge_index\n        self.raw_edge_attr = new_raw_edge_attr\n        self.raw_edge_index = new_raw_edge_index\n        \n        self.node_flag = new_node_flag\n        self.edge_flag = new_edge_flag\n        self.raw_edge_flag = new_raw_edge_flag\n        \n        self.ts_list = self.ts_list[i_init+1+period:i_stop-period]\n        \n        self._set_snapshot_count()\n    \n    def annotation2y(self, annotation, interval, overlap, offset= -30):\n        ts_list = self.ts_list\n        y = torch.zeros(self.snapshot_count, dtype=torch.long)\n        for i_ts, ts in enumerate(ts_list):\n            if ts < float(annotation[1])+offset and float(annotation[1])+offset <= ts+interval-overlap: \n                y[i_ts] = 1\n        self.y = y\n        self.type = annotation[0]\n    \n    def to(self,device):\n        self.node_attr = self.node_attr.to(device)\n        self.node_index = self.node_index.to(device)\n        self.edge_index = self.edge_index.to(device)\n    \n    def get_adj_list(self):\n        edge_index = self.edge_index\n        edge_flag = self.edge_flag\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        adj_list = [torch.clamp(to_dense_adj(_edge_index)[0], min=0, max=1) for _edge_index in edge_index_split]\n        return adj_list\n\n    def __getitem__(self, time_index: int):\n        raise\n#         edge_index = self._get_edge_index(time_index)\n#         edge_attr = self._get_edge_attr(time_index)\n#         node_index,node_attr = self._get_node_index_attr(time_index)\n#         _timestamp = self._get_timestamp(time_index)\n\n#         snapshot = Data(\n#             edge_index=edge_index,\n#             edge_attr=edge_attr,\n#             node_index=node_index,\n#             node_attr=node_attr,\n#             timestamp=_timestamp\n#         )\n#         return snapshot\n\n    def __next__(self):\n        if self.t < self.snapshot_count:\n            snapshot = self[self.t]\n            self.t = self.t + 1\n            return snapshot\n        else:\n            self.t = 0\n            raise StopIteration\n\n    def __iter__(self):\n        self.t = 0\n        return self\n    \n    def __len__(self):\n        return self.snapshot_count\n\n\nclass GraphDatasetLoader(object):\n    def __init__(self,input_path=\"\"):\n        self.input_path = input_path\n        self._read_data()\n    \n    def _read_data(self):\n        self._dataset = np.load(self.input_path)\n\n    def get_dataset(self): # -> DynamicGraphTemporalSignal:\n        dataset = GraphSignal(\n            edge_flag = self._dataset['edge_flag'],\n            edge_index = self._dataset['edge_index'],\n            edge_attr = self._dataset['edge_attr'],\n            node_flag = self._dataset['node_flag'],\n            node_index = self._dataset['node_index'],\n            node_attr = self._dataset['node_attr'],\n            ts_list = self._dataset['timestamp'],\n            path = self.input_path\n        )\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:40:47.797844Z","iopub.execute_input":"2023-08-28T15:40:47.798568Z","iopub.status.idle":"2023-08-28T15:40:47.847727Z","shell.execute_reply.started":"2023-08-28T15:40:47.798533Z","shell.execute_reply":"2023-08-28T15:40:47.846205Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/dissertation-data/signals_train.pkl\", \"rb\") as f:\n    signals_train = pickle.load(f)\nwith open(\"/kaggle/input/dissertation-data/signals_val.pkl\", \"rb\") as f:\n    signals_val = pickle.load(f)\nwith open(\"/kaggle/input/dissertation-data/signals_test.pkl\", \"rb\") as f:\n    signals_test = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:40:47.848884Z","iopub.execute_input":"2023-08-28T15:40:47.849641Z","iopub.status.idle":"2023-08-28T15:41:06.907938Z","shell.execute_reply.started":"2023-08-28T15:40:47.849609Z","shell.execute_reply":"2023-08-28T15:41:06.906850Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Multi-Layer GraphConv","metadata":{}},{"cell_type":"code","source":"class MultiGraphConv(torch.nn.Module):\n    def __init__(\n        self, \n        channels, \n    ):\n        super().__init__()\n         \n        self.convs = nn.ModuleList()\n        \n        net1 = Seq(\n            Lin(channels, channels),\n            LeakyReLU(),\n            Lin(channels, channels*2),\n            LeakyReLU(),\n            Lin(channels*2, channels*2),\n            LeakyReLU(),\n        )\n        conv1 = GINConv(net1,train_eps=True)\n        self.convs.append(conv1)\n        \n        net2 = Seq(\n            Lin(channels*2, channels*2),\n            LeakyReLU(),\n            Lin(channels*2, channels),\n            LeakyReLU(),\n            Lin(channels, channels),\n            LeakyReLU(),\n        )\n        conv2 = GINConv(net2,train_eps=True)\n        self.convs.append(conv2)\n        \n\n    def forward(self, x, edge_index):\n        out = x\n        for conv in self.convs:\n            out = conv(x=out, edge_index=edge_index)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:06.913236Z","iopub.execute_input":"2023-08-28T15:41:06.913607Z","iopub.status.idle":"2023-08-28T15:41:06.924871Z","shell.execute_reply.started":"2023-08-28T15:41:06.913579Z","shell.execute_reply":"2023-08-28T15:41:06.923975Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# TGAE Model","metadata":{}},{"cell_type":"code","source":"class TGAE(torch.nn.Module): # Not Heterogeneous\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        embed_layers,  \n        decide_layers,\n    ):\n        super(TGAE, self).__init__()\n        \n        # Encoder Embeding\n#         layers = [torch.nn.BatchNorm1d(in_channels)]\n        layers = []\n        pre_h_num = in_channels\n        for h_num in embed_layers[:-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,embed_layers[-1]))\n        self.embed_net = Seq(*layers)\n                \n        self.conv = MultiGraphConv(channels=embed_layers[-1])\n\n        layers = []\n        pre_h_num = embed_layers[-1]\n        for h_num in decide_layers:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,out_channels))\n#         layers.append(torch.nn.Sigmoid())\n        self.decide_net = Seq(*layers)\n        \n        # Decoder\n        layers = []\n        pre_h_num = out_channels\n        for h_num in decide_layers[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,embed_layers[-1]))\n        self.decode_decide_net = Seq(*layers)\n        \n        self.decoder_conv = MultiGraphConv(channels=embed_layers[-1])\n\n        \n        layers = []\n        pre_h_num = embed_layers[-1]\n        for h_num in embed_layers[:-1][::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,in_channels))\n        self.decode_embed_net = Seq(*layers)\n        \n\n    def forward(self, x, node_index, node_flag, edge_index, edge_flag):\n        # Encoder\n        out = self.embed_net(x)\n        \n        # GNN layer\n        h_encoder = self.conv(out, edge_index) \n        \n        out = self.decide_net(h_encoder)\n        out = self.decode_decide_net(out)\n \n        h_decoder = self.decoder_conv(out, torch.flip(edge_index,dims=(0,)))\n        \n        out = self.decode_embed_net(h_decoder)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:06.929412Z","iopub.execute_input":"2023-08-28T15:41:06.930171Z","iopub.status.idle":"2023-08-28T15:41:06.948466Z","shell.execute_reply.started":"2023-08-28T15:41:06.930138Z","shell.execute_reply":"2023-08-28T15:41:06.947199Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Batch Signal","metadata":{}},{"cell_type":"code","source":"# Concat Batchs to from BatchSignal\nclass BatchSignal():\n    def __init__(self, signals, batch_size, seq_len):   \n        data_list_2D = [[] for _ in range(seq_len)]\n        node_index_split_list = [[] for _ in range(seq_len)]\n        base_node_index = 0\n        \n        self.batch_index = [[i_signal, 0] for i_signal in np.random.randint(low=0, high=len(signals), size=batch_size, dtype=int)]\n\n        for i_batch, (i_signal,_) in enumerate(self.batch_index):\n            signal = signals[i_signal]\n    \n            while signal.snapshot_count < seq_len:\n                new_index = np.random.randint(low=0, high=len(signals))\n                signal = signals[new_index]\n                self.batch_index[i_batch] = [new_index, 0]\n            \n            node_attr_split = signal.node_attr.tensor_split(signal.node_flag)\n            node_index_split = signal.node_index.tensor_split(signal.node_flag)\n            edge_index_split = signal.edge_index.tensor_split(signal.edge_flag, dim=1)\n\n            if signal.snapshot_count-seq_len != 0:\n                _start = np.random.randint(low=0, high=signal.snapshot_count-seq_len+1)\n                self.batch_index[i_batch][1] = _start\n            else:\n                _start = 0\n\n            for i_series, i_snapshot in enumerate(range(_start, _start+seq_len)):\n                data = Data(\n                    x = node_attr_split[i_snapshot],\n                    edge_index = edge_index_split[i_snapshot],\n                    y = signal.y[i_snapshot]\n                )\n                data_list_2D[i_series].append(data)\n                node_index_split_list[i_series].append(node_index_split[i_snapshot]+base_node_index)\n            base_node_index += signal.node_count\n\n        batch_list = [Batch.from_data_list(data_list) for data_list in data_list_2D]\n        node_index_list = [torch.cat(_node_index, dim=0) for _node_index in node_index_split_list]\n\n        node_flag_list = []\n        edge_flag_list = []\n        node_attr = []\n        edge_index = []\n\n        node_base = 0\n        edge_base = 0\n\n        for batch in batch_list:\n            node_base += batch.x.shape[0]\n            edge_base += batch.edge_index.shape[1]\n            node_flag_list.append(node_base)\n            edge_flag_list.append(edge_base)\n\n        self.node_index = torch.cat([_node_index for _node_index in node_index_list], dim=0) \n        self.node_attr = torch.cat([_batch.x for _batch in batch_list], dim=0)\n        self.edge_index = torch.cat([_batch.edge_index for _batch in batch_list], dim=1)\n        self.y = torch.cat([_batch.y for _batch in batch_list], dim=0)\n        self.batch_index_dict = [batch._slice_dict for batch in batch_list]\n        \n        self.node_flag = torch.LongTensor(node_flag_list)[:-1]\n        self.edge_flag = torch.LongTensor(edge_flag_list)[:-1]\n    \n    def to(self, device):\n        self.node_attr = self.node_attr.to(device)\n        self.node_index = self.node_index.to(device)\n        self.edge_index = self.edge_index.to(device)\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:06.950127Z","iopub.execute_input":"2023-08-28T15:41:06.950545Z","iopub.status.idle":"2023-08-28T15:41:06.970023Z","shell.execute_reply.started":"2023-08-28T15:41:06.950515Z","shell.execute_reply":"2023-08-28T15:41:06.968907Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"# train_loop\ndef train_loop(signal, model, loss_fn, optimizer, device, split_loss: bool = False):\n    model.train()\n    \n    X = signal.node_attr\n    node_index = signal.node_index\n    node_flag = signal.node_flag\n    edge_index = signal.edge_index\n    edge_flag = signal.edge_flag\n    \n    outs = model(X, node_index, node_flag, edge_index, edge_flag)\n\n    train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n    total_loss = torch.mean(train_losses)\n        \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    if split_loss:\n        snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n        return snapshot_losses\n    else:\n        return total_loss.cpu().detach().numpy()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:06.971321Z","iopub.execute_input":"2023-08-28T15:41:06.971970Z","iopub.status.idle":"2023-08-28T15:41:06.985489Z","shell.execute_reply.started":"2023-08-28T15:41:06.971939Z","shell.execute_reply":"2023-08-28T15:41:06.984707Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Test Loop","metadata":{}},{"cell_type":"code","source":"# test_loop\ndef test_loop(signal, model, loss_fn, optimizer, device, split_loss: bool = False):\n    model.eval()\n    with torch.no_grad():\n        X = signal.node_attr\n        node_index = signal.node_index\n        node_flag = signal.node_flag\n        edge_index = signal.edge_index\n        edge_flag = signal.edge_flag\n\n        outs = model(X,node_index, node_flag, edge_index, edge_flag)\n\n        train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n        total_loss = torch.mean(train_losses)\n        snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n    \n        if split_loss:\n            snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n            return snapshot_losses\n        else:\n            return total_loss.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:47:55.885753Z","iopub.execute_input":"2023-08-28T15:47:55.886174Z","iopub.status.idle":"2023-08-28T15:47:55.897723Z","shell.execute_reply.started":"2023-08-28T15:47:55.886143Z","shell.execute_reply":"2023-08-28T15:47:55.896380Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{}},{"cell_type":"code","source":"def get_graph_anomaly_threshold(batch_signal, batch_node_losses,q=95):\n    batch_index_dict = batch_signal.batch_index_dict\n    graph_losses = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        graph_losses += [np.sum(node_losses) for node_losses in loss_split]\n        \n    return np.percentile(graph_losses,q)\n\ndef batch_graph_eval_func(batch_signal, batch_node_losses, threshold):\n    batch_index_dict = batch_signal.batch_index_dict\n    y_pred = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        for losses in loss_split:\n            y_pred.append(int(np.sum(losses.reshape(-1, 1)) > threshold))\n    tn, fp, fn, tp = confusion_matrix(batch_signal.y, y_pred, labels=[0,1]).ravel()\n    return tn, fp, fn, tp\n\ndef batch_graph_eval_func_2(batch_signal, batch_node_losses):\n    batch_index_dict = batch_signal.batch_index_dict\n    y_loss = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        for losses in loss_split:\n            y_loss.append(np.mean(losses.reshape(-1, 1)))\n            \n    precision_list, recall_list, thresholds = precision_recall_curve(batch_signal.y,y_loss)\n    f1_list = []\n    for precision, recall in zip(precision_list, recall_list):\n        if precision == 0 and recall == 0:\n            f1_list.append(0)\n        else:\n            f1_list.append(2 * (precision * recall) / (precision + recall))\n        \n    precision = precision_list[np.nanargmax(f1_list)]\n    recall = recall_list[np.nanargmax(f1_list)]\n    f1 = f1_list[np.nanargmax(f1_list)]\n    roc_auc = roc_auc_score(batch_signal.y,y_loss)\n    \n    return precision, recall, f1, roc_auc\n\ndef single_graph_eval_func(signal, node_losses, threshold):\n    y_pred = []\n    for losses in node_losses:\n        y_pred.append(int(np.sum(losses.reshape(-1, 1)) > threshold))\n    tn, fp, fn, tp = confusion_matrix(signal.y, y_pred, labels=[0,1]).ravel()\n    return tn, fp, fn, tp\n\ndef single_graph_eval_func_2(signals, node_losses_list):\n    y_true = np.concatenate([signal.y for signal in signals])\n    y_loss = []\n    for losses in node_losses_list:\n        y_loss.append(np.mean(losses.reshape(-1, 1)))\n            \n    precision_list, recall_list, thresholds = precision_recall_curve(y_true,y_loss)\n    f1_list = []\n    for precision, recall in zip(precision_list, recall_list):\n        if precision == 0 and recall == 0:\n            f1_list.append(0)\n        else:\n            f1_list.append(2 * (precision * recall) / (precision + recall))\n        \n    precision = precision_list[np.nanargmax(f1_list)]\n    recall = recall_list[np.nanargmax(f1_list)]\n    f1 = f1_list[np.nanargmax(f1_list)]\n    roc_auc = roc_auc_score(y_true,y_loss)\n    \n    return precision, recall, f1, roc_auc","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:06.998968Z","iopub.execute_input":"2023-08-28T15:41:06.999581Z","iopub.status.idle":"2023-08-28T15:41:07.019603Z","shell.execute_reply.started":"2023-08-28T15:41:06.999546Z","shell.execute_reply":"2023-08-28T15:41:07.018649Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Train Function","metadata":{}},{"cell_type":"code","source":"def train_function(num_epoch, val_interval=100, start_val=0, save=False, save_dir=None):\n    global GLOBAL_EPOCH\n    \n    history = {}\n    history['train'] = {\n        \"loss\":[],\n        \"precision\":[],\n        \"recall\":[],\n        \"f1\":[],\n        \"roc_auc\":[],\n        \"time\":[]\n    }\n    history['val'] = {\n        \"loss\":[],\n        \"precision\":[],\n        \"recall\":[],\n        \"f1\":[],\n        \"roc_auc\":[],\n    }\n    history['save'] = []\n    \n    batch_signal = BatchSignal(signals=signals_train, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)\n    for i_epoch in range(1,num_epoch+1):\n        _start = time.time()    \n#         if i_epoch % 10: \n        del batch_signal\n        batch_signal = BatchSignal(signals=signals_train, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)\n        train_losses = train_loop(batch_signal, model, loss_f, optimizer, device, split_loss=False)\n        _end = time.time()\n        if i_epoch >= start_val and i_epoch % val_interval == 0:\n            \n#             batch_signal = BatchSignal(signals=signals_train, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)        \n#             node_losses_list_train = test_loop(batch_signal, model, loss_f, optimizer, device, split_loss=True)\n#             precision_train, recall_train, f1_train, roc_auc_train = batch_graph_eval_func_2(batch_signal, node_losses_list_train)\n            \n            f1_list = []\n            roc_auc_list = []\n            for _ in range(5):\n                batch_signal_val = BatchSignal(signals=signals_val, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)\n                node_losses_list_val = test_loop(batch_signal_val, model, loss_f, optimizer, device, split_loss=True)\n                precision_val, recall_val, f1_val, roc_auc_val = batch_graph_eval_func_2(batch_signal_val, node_losses_list_val)\n                f1_list.append(f1_val)\n                roc_auc_list.append(roc_auc_val)\n#             train_losses = np.concatenate(node_losses_list_train)\n            val_losses = np.concatenate(node_losses_list_val)\n            \n#             history['train']['loss'].append(np.mean(train_losses))\n#             history['train']['precision'].append(precision_train)\n#             history['train']['recall'].append(recall_train)\n#             history['train']['f1'].append(f1_train)\n#             history['train']['roc_auc'].append(roc_auc_train)\n            history['train']['time'].append(_end-_start)\n            \n            history['val']['loss'].append(np.mean(val_losses))\n#             history['val']['precision'].append(precision_val)\n#             history['val']['recall'].append(recall_val)\n            history['val']['f1'].append(np.mean(f1_val))\n            history['val']['roc_auc'].append(np.mean(roc_auc_val))\n                \n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train loss {np.mean(train_losses):.4f} val loss {np.mean(val_losses):.4f}\")\n            t = Texttable()\n            t.add_rows([\n                ['', 'F1', 'AUC_ROC'], \n#                 ['Train', precision_train, recall_train, f1_train, roc_auc_train], \n                ['Val', f1_val, roc_auc_val]])\n            print(t.draw())\n            \n            if save:\n                if save_dir is not None:\n                    os.makedirs(save_dir,exist_ok=True)\n                    path = os.path.join(save_dir, f\"{i_epoch+GLOBAL_EPOCH}.pt\")\n                else:\n                    path =  f\"{i_epoch+GLOBAL_EPOCH}.pt\"\n                torch.save(model.state_dict(), path)\n                history['save'].append(path)\n        else:\n            if i_epoch % 50 == 0:\n                history['train']['time'].append(_end-_start)\n                print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train loss {np.mean(train_losses):.4f}\")\n    GLOBAL_EPOCH += num_epoch\n    return history","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:41:07.022720Z","iopub.execute_input":"2023-08-28T15:41:07.023102Z","iopub.status.idle":"2023-08-28T15:41:07.045840Z","shell.execute_reply.started":"2023-08-28T15:41:07.023056Z","shell.execute_reply":"2023-08-28T15:41:07.044695Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"IN_CHANNELS = signals_train[0].node_attr.shape[1]\nMIN_LEN=10\nBATCH_SIZE = 50\nGLOBAL_EPOCH = 0\n# print(lr, dropout)\nmodel = TGAE(\n    in_channels=IN_CHANNELS, \n    out_channels=32, \n    embed_layers=[128,256,512,256],\n    decide_layers=[512,256,128],\n)\n\nloss_f = torch.nn.MSELoss(reduction = 'none')\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\nmodel = model.to(device)\nloss_f = loss_f.to(device)\n\nhistory = train_function(5000,100,0, save=True)\nraise","metadata":{"execution":{"iopub.status.busy":"2023-08-28T15:56:03.124809Z","iopub.execute_input":"2023-08-28T15:56:03.125773Z","iopub.status.idle":"2023-08-28T16:09:44.953094Z","shell.execute_reply.started":"2023-08-28T15:56:03.125735Z","shell.execute_reply":"2023-08-28T16:09:44.951739Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"50/5000: cost 0.1549s train loss 1.3772\n100/5000: cost 0.1555s train loss 1.3652 val loss 1.3535\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.126 | 0.459   |\n+-----+-------+---------+\n150/5000: cost 0.1563s train loss 1.3847\n200/5000: cost 0.1628s train loss 1.2631 val loss 1.2107\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.087 | 0.510   |\n+-----+-------+---------+\n250/5000: cost 0.1542s train loss 1.2741\n300/5000: cost 0.1549s train loss 1.2653 val loss 1.2631\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.084 | 0.405   |\n+-----+-------+---------+\n350/5000: cost 0.1580s train loss 1.2590\n400/5000: cost 0.1586s train loss 1.2582 val loss 1.1982\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.134 | 0.579   |\n+-----+-------+---------+\n450/5000: cost 0.1555s train loss 1.2411\n500/5000: cost 0.1428s train loss 1.2101 val loss 1.0143\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.067 | 0.505   |\n+-----+-------+---------+\n550/5000: cost 0.1629s train loss 1.1054\n600/5000: cost 0.1410s train loss 1.1564 val loss 1.0014\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.054 | 0.535   |\n+-----+-------+---------+\n650/5000: cost 0.1480s train loss 1.0970\n700/5000: cost 0.1644s train loss 1.0187 val loss 0.9510\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.212 | 0.687   |\n+-----+-------+---------+\n750/5000: cost 0.1606s train loss 0.9765\n800/5000: cost 0.1523s train loss 0.9545 val loss 0.8712\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.170 | 0.859   |\n+-----+-------+---------+\n850/5000: cost 0.1640s train loss 0.9353\n900/5000: cost 0.1380s train loss 0.9141 val loss 0.8250\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.330 | 0.879   |\n+-----+-------+---------+\n950/5000: cost 0.1856s train loss 0.8874\n1000/5000: cost 0.1584s train loss 0.8771 val loss 0.7918\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.313 | 0.911   |\n+-----+-------+---------+\n1050/5000: cost 0.1684s train loss 0.8808\n1100/5000: cost 0.1778s train loss 0.8434 val loss 0.7755\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.308 | 0.911   |\n+-----+-------+---------+\n1150/5000: cost 0.1562s train loss 0.8688\n1200/5000: cost 0.1297s train loss 0.8282 val loss 0.7469\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.253 | 0.881   |\n+-----+-------+---------+\n1250/5000: cost 0.1390s train loss 0.8215\n1300/5000: cost 0.1464s train loss 0.8256 val loss 0.7462\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.301 | 0.876   |\n+-----+-------+---------+\n1350/5000: cost 0.1495s train loss 0.8157\n1400/5000: cost 0.1445s train loss 0.8086 val loss 0.7320\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.295 | 0.870   |\n+-----+-------+---------+\n1450/5000: cost 0.1676s train loss 0.7819\n1500/5000: cost 0.1569s train loss 0.8692 val loss 0.7741\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.250 | 0.893   |\n+-----+-------+---------+\n1550/5000: cost 0.1415s train loss 0.7870\n1600/5000: cost 0.1576s train loss 0.7689 val loss 0.7151\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.301 | 0.889   |\n+-----+-------+---------+\n1650/5000: cost 0.1468s train loss 0.7670\n1700/5000: cost 0.1477s train loss 0.7411 val loss 0.7319\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.365 | 0.881   |\n+-----+-------+---------+\n1750/5000: cost 0.1527s train loss 0.7455\n1800/5000: cost 0.1556s train loss 0.7401 val loss 0.7427\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.323 | 0.898   |\n+-----+-------+---------+\n1850/5000: cost 0.1751s train loss 0.7686\n1900/5000: cost 0.1611s train loss 0.6972 val loss 0.7478\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.412 | 0.908   |\n+-----+-------+---------+\n1950/5000: cost 0.1402s train loss 0.7106\n2000/5000: cost 0.1648s train loss 0.7066 val loss 0.6917\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.366 | 0.919   |\n+-----+-------+---------+\n2050/5000: cost 0.1579s train loss 0.6886\n2100/5000: cost 0.1432s train loss 0.7374 val loss 0.7492\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.330 | 0.911   |\n+-----+-------+---------+\n2150/5000: cost 0.1625s train loss 0.6844\n2200/5000: cost 0.1536s train loss 0.6835 val loss 0.7434\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.340 | 0.907   |\n+-----+-------+---------+\n2250/5000: cost 0.1607s train loss 0.6716\n2300/5000: cost 0.1654s train loss 0.6808 val loss 0.7397\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.292 | 0.888   |\n+-----+-------+---------+\n2350/5000: cost 0.1521s train loss 0.6601\n2400/5000: cost 0.1712s train loss 0.6802 val loss 0.6342\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.258 | 0.883   |\n+-----+-------+---------+\n2450/5000: cost 0.1752s train loss 0.6787\n2500/5000: cost 0.1513s train loss 0.6520 val loss 0.7030\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.358 | 0.902   |\n+-----+-------+---------+\n2550/5000: cost 0.1498s train loss 0.6650\n2600/5000: cost 0.1626s train loss 0.6592 val loss 0.7386\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.310 | 0.912   |\n+-----+-------+---------+\n2650/5000: cost 0.1650s train loss 0.6436\n2700/5000: cost 0.1495s train loss 0.6510 val loss 0.8036\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.337 | 0.890   |\n+-----+-------+---------+\n2750/5000: cost 0.1559s train loss 0.6377\n2800/5000: cost 0.1497s train loss 0.6380 val loss 0.7618\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.357 | 0.903   |\n+-----+-------+---------+\n2850/5000: cost 0.1503s train loss 0.6525\n2900/5000: cost 0.1793s train loss 0.6726 val loss 0.6649\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.418 | 0.915   |\n+-----+-------+---------+\n2950/5000: cost 0.1504s train loss 0.6233\n3000/5000: cost 0.1663s train loss 0.6252 val loss 0.6665\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.340 | 0.885   |\n+-----+-------+---------+\n3050/5000: cost 0.1630s train loss 0.6113\n3100/5000: cost 0.1573s train loss 0.6084 val loss 0.7394\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.300 | 0.912   |\n+-----+-------+---------+\n3150/5000: cost 0.1690s train loss 0.6040\n3200/5000: cost 0.1444s train loss 0.6042 val loss 0.6940\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.323 | 0.886   |\n+-----+-------+---------+\n3250/5000: cost 0.1569s train loss 0.6217\n3300/5000: cost 0.1551s train loss 0.5962 val loss 0.7364\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.282 | 0.913   |\n+-----+-------+---------+\n3350/5000: cost 0.1554s train loss 0.5977\n3400/5000: cost 0.1468s train loss 0.6069 val loss 0.7341\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.296 | 0.904   |\n+-----+-------+---------+\n3450/5000: cost 0.1581s train loss 0.6179\n3500/5000: cost 0.1640s train loss 0.5837 val loss 0.7407\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.286 | 0.888   |\n+-----+-------+---------+\n3550/5000: cost 0.1344s train loss 0.6002\n3600/5000: cost 0.1609s train loss 0.5688 val loss 0.7062\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.341 | 0.901   |\n+-----+-------+---------+\n3650/5000: cost 0.1427s train loss 0.5944\n3700/5000: cost 0.1545s train loss 0.6008 val loss 0.5862\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.306 | 0.910   |\n+-----+-------+---------+\n3750/5000: cost 0.1572s train loss 0.5820\n3800/5000: cost 0.1682s train loss 0.5774 val loss 0.6023\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.424 | 0.905   |\n+-----+-------+---------+\n3850/5000: cost 0.1540s train loss 0.5716\n3900/5000: cost 0.1527s train loss 0.5729 val loss 0.7066\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.274 | 0.908   |\n+-----+-------+---------+\n3950/5000: cost 0.1504s train loss 0.5787\n4000/5000: cost 0.1933s train loss 0.5585 val loss 0.7239\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.387 | 0.903   |\n+-----+-------+---------+\n4050/5000: cost 0.1599s train loss 0.5694\n4100/5000: cost 0.1694s train loss 0.5632 val loss 0.6950\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.375 | 0.932   |\n+-----+-------+---------+\n4150/5000: cost 0.1445s train loss 0.5653\n4200/5000: cost 0.1753s train loss 0.5771 val loss 0.5894\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.286 | 0.904   |\n+-----+-------+---------+\n4250/5000: cost 0.1473s train loss 0.5623\n4300/5000: cost 0.1665s train loss 0.5326 val loss 0.6647\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.247 | 0.922   |\n+-----+-------+---------+\n4350/5000: cost 0.1477s train loss 0.5668\n4400/5000: cost 0.1478s train loss 0.5607 val loss 0.6754\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.444 | 0.919   |\n+-----+-------+---------+\n4450/5000: cost 0.1524s train loss 0.5333\n4500/5000: cost 0.1330s train loss 0.5511 val loss 0.6726\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.286 | 0.912   |\n+-----+-------+---------+\n4550/5000: cost 0.1608s train loss 0.5343\n4600/5000: cost 0.1643s train loss 0.7066 val loss 0.8418\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.098 | 0.611   |\n+-----+-------+---------+\n4650/5000: cost 0.1527s train loss 0.5809\n4700/5000: cost 0.1551s train loss 0.5534 val loss 0.6688\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.352 | 0.916   |\n+-----+-------+---------+\n4750/5000: cost 0.1485s train loss 0.5317\n4800/5000: cost 0.1988s train loss 0.5349 val loss 0.5887\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.384 | 0.909   |\n+-----+-------+---------+\n4850/5000: cost 0.1377s train loss 0.5372\n4900/5000: cost 0.1464s train loss 0.5454 val loss 0.6558\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.366 | 0.902   |\n+-----+-------+---------+\n4950/5000: cost 0.1410s train loss 0.5517\n5000/5000: cost 0.1420s train loss 0.5476 val loss 0.5315\n+-----+-------+---------+\n|     |  F1   | AUC_ROC |\n+=====+=======+=========+\n| Val | 0.321 | 0.916   |\n+-----+-------+---------+\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m loss_f \u001b[38;5;241m=\u001b[39m loss_f\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m history \u001b[38;5;241m=\u001b[39m train_function(\u001b[38;5;241m5000\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m0\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"],"ename":"RuntimeError","evalue":"No active exception to reraise","output_type":"error"}]},{"cell_type":"code","source":"for path in history['save']:\n    #     path = history['save'][-1]\n    model = TGAE(\n        in_channels=IN_CHANNELS, \n        out_channels=32, \n        embed_layers=[128,256,512,256],\n        decide_layers=[512,256,128],\n    )\n\n    loss_f = torch.nn.MSELoss(reduction = 'none')\n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n    model = model.to(device)\n    loss_f = loss_f.to(device)\n\n    model.load_state_dict(torch.load(path))\n    model.eval()\n\n    print(\"Evaluate on whole dataset:\")\n\n    #     node_losses_list_train = []\n    #     for i_signal, signal in enumerate(signals_train):\n    #         signal.to(device)\n    #         node_losses_list = test_loop(signal, model, loss_f, optimizer, device, split_loss=True)\n    #         node_losses_list_train += node_losses_list\n\n    #     precision_train, recall_train, f1_train, roc_auc_train = single_graph_eval_func_2(signals_train, node_losses_list_train)\n\n    privesc_signals_val = []\n    privesc_losses_list_val = []\n    dos_signals_val = []\n    dos_losses_list_val = []\n\n    node_losses_list_val = []\n    for i_signal, signal in enumerate(signals_val):\n        signal.to(device)\n        node_losses_list = test_loop(signal, model, loss_f, optimizer, device, split_loss=True)\n        node_losses_list_val += node_losses_list\n#         if signal.type == 'privesc':\n#             privesc_signals_val.append(signal)\n#             privesc_losses_list_val += node_losses_list\n#         if signal.type == 'dos':\n#             dos_signals_val.append(signal)\n#             dos_losses_list_val += node_losses_list\n\n    precision_val, recall_val, f1_val, roc_auc_val = single_graph_eval_func_2(signals_val, node_losses_list_val)\n#     precision_privesc, recall_privesc, f1_privesc, roc_auc_privesc = single_graph_eval_func_2(privesc_signals_val, privesc_losses_list_val)\n#     precision_dos, recall_dos, f1_dos, roc_auc_dos = single_graph_eval_func_2(dos_signals_val, dos_losses_list_val)\n\n    t = Texttable()\n    t.add_rows([\n        ['', 'Precision', 'Recall', 'F1', 'AUC_ROC'], \n        ['ALL', precision_val, recall_val, f1_val, roc_auc_val], \n#         ['privesc', precision_privesc, recall_privesc, f1_privesc, roc_auc_privesc],\n#         ['dos',precision_dos, recall_dos, f1_dos, roc_auc_dos],\n\n    ])\n    print(t.draw())","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:15:10.575715Z","iopub.execute_input":"2023-08-28T16:15:10.576142Z","iopub.status.idle":"2023-08-28T16:15:31.263747Z","shell.execute_reply.started":"2023-08-28T16:15:10.576112Z","shell.execute_reply":"2023-08-28T16:15:31.262649Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Evaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.052     | 1      | 0.098 | 0.513   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.054     | 1      | 0.102 | 0.534   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.053     | 1      | 0.101 | 0.517   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.053     | 1      | 0.101 | 0.526   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.052     | 1      | 0.099 | 0.518   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.051     | 1      | 0.098 | 0.508   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.077     | 0.293  | 0.122 | 0.673   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.139     | 1      | 0.243 | 0.857   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.143     | 1      | 0.251 | 0.861   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.151     | 1      | 0.263 | 0.864   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.163     | 1      | 0.280 | 0.878   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.152     | 0.976  | 0.262 | 0.864   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.155     | 1      | 0.269 | 0.868   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.153     | 1      | 0.265 | 0.863   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.148     | 0.976  | 0.257 | 0.860   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.147     | 1      | 0.256 | 0.858   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.158     | 1      | 0.273 | 0.868   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.162     | 1      | 0.279 | 0.873   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.170     | 1      | 0.291 | 0.877   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.167     | 1      | 0.286 | 0.874   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.161     | 0.976  | 0.277 | 0.881   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.294 | 0.884   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.175     | 1      | 0.298 | 0.886   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.153     | 1      | 0.265 | 0.864   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.294 | 0.885   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.177     | 1      | 0.300 | 0.887   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.174     | 1      | 0.296 | 0.888   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.182     | 1      | 0.308 | 0.884   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.169     | 1      | 0.289 | 0.881   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.167     | 1      | 0.287 | 0.879   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.169     | 1      | 0.290 | 0.884   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.294 | 0.887   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.174     | 1      | 0.296 | 0.889   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.293 | 0.889   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.176     | 1      | 0.299 | 0.893   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.180     | 1      | 0.305 | 0.894   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.164     | 1      | 0.282 | 0.883   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.165     | 1      | 0.284 | 0.884   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.170     | 1      | 0.291 | 0.894   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.171     | 1      | 0.292 | 0.894   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.293 | 0.889   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.165     | 1      | 0.284 | 0.881   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.293 | 0.889   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.174     | 1      | 0.297 | 0.894   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.174     | 1      | 0.296 | 0.892   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.059     | 0.537  | 0.107 | 0.631   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.172     | 1      | 0.293 | 0.895   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.165     | 1      | 0.283 | 0.880   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.171     | 1      | 0.292 | 0.892   |\n+-----+-----------+--------+-------+---------+\nEvaluate on whole dataset:\n+-----+-----------+--------+-------+---------+\n|     | Precision | Recall |  F1   | AUC_ROC |\n+=====+===========+========+=======+=========+\n| ALL | 0.163     | 1      | 0.281 | 0.883   |\n+-----+-----------+--------+-------+---------+\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}