{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_gpu = torch.cuda.is_available()\nprint(torch.__version__)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:09:24.932479Z","iopub.execute_input":"2023-08-23T02:09:24.933132Z","iopub.status.idle":"2023-08-23T02:09:28.342998Z","shell.execute_reply.started":"2023-08-23T02:09:24.933097Z","shell.execute_reply":"2023-08-23T02:09:28.341824Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.0.0\ncuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize for CPU & GPU","metadata":{}},{"cell_type":"code","source":"if use_gpu:\n    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric --no-index --find-links=file:///kaggle/input/torch-geometric\n#     !pip install torch_geometric_temporal  \nelse:\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_scatter-2.1.1pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_sparse-0.6.17pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_cluster-1.6.1pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_spline_conv-1.2.2pt20cpu-cp310-cp310-linux_x86_64.whl\n    !pip install /kaggle/input/pytorch-geometric-cpu/torch_geometric-2.3.1-py3-none-any.whl\n#     !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n#     !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install torch_spline_conv -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n#     !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n#     !pip install torch_geometric_temporal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-23T02:09:28.345233Z","iopub.execute_input":"2023-08-23T02:09:28.346204Z","iopub.status.idle":"2023-08-23T02:09:44.402292Z","shell.execute_reply.started":"2023-08-23T02:09:28.346163Z","shell.execute_reply":"2023-08-23T02:09:44.400938Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/torch-geometric\nProcessing /kaggle/input/torch-geometric/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_cluster-1.6.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_geometric-2.3.1-py3-none-any.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\nSuccessfully installed torch-cluster-1.6.1 torch-geometric-2.3.1 torch-scatter-2.1.1 torch-sparse-0.6.17 torch-spline-conv-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport json\nimport random\nfrom scipy.special import perm\nfrom itertools import combinations,chain\nfrom typing import List, Union\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nfrom torch import nn\nfrom torch.nn import Linear as Lin\nfrom torch.nn import ReLU, LeakyReLU\nfrom torch.nn import Sequential as Seq\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GINConv, GATConv\nfrom torch_geometric.data import Batch\nfrom torch import autograd\nfrom torch_geometric.nn.models import InnerProductDecoder\nfrom torch_geometric.utils import to_dense_adj\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\n\nfrom texttable import Texttable","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:09:44.404682Z","iopub.execute_input":"2023-08-23T02:09:44.405140Z","iopub.status.idle":"2023-08-23T02:09:46.425263Z","shell.execute_reply.started":"2023-08-23T02:09:44.405093Z","shell.execute_reply":"2023-08-23T02:09:46.423914Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"Edge_Flag = List[Union[np.ndarray, None]]\nEdge_Index = List[Union[np.ndarray, None]]\nEdge_Attr = List[Union[np.ndarray, None]]\n\nNode_Flag = List[Union[np.ndarray, None]]\nNode_Index = List[Union[np.ndarray, None]]\nNode_Attr = List[Union[np.ndarray, None]]\n\n\n\nclass GraphSignal(object):\n    # dynamic node static node attr dynamic edge and edge attr\n    def __init__(\n        self,\n        edge_flag: Edge_Flag,\n        edge_index: Edge_Index,\n        edge_attr: Edge_Attr,\n        node_flag: Node_Flag,\n        node_index: Node_Index,\n        node_attr: Node_Attr,\n        ts_list: List,\n        path: str,\n    ):\n        \n        self.raw_edge_flag = torch.LongTensor(edge_flag[:-1])\n        self.raw_edge_index = torch.LongTensor(edge_index).T\n        self.raw_edge_attr = edge_attr\n        self.raw_node_attr = node_attr \n        self.node_flag = torch.LongTensor(node_flag[:-1])\n        self.node_index = torch.LongTensor(node_index)\n        \n        self.ts_list = ts_list\n        \n        self.path = path\n        \n        self.node_attr = None\n        self.edge_flag = None\n        self.edge_index = None\n        \n        self.y = None\n        \n        self._set_snapshot_count()\n        self._set_node_count()\n\n    def _set_snapshot_count(self):\n        self.snapshot_count = len(self.ts_list)\n    \n    def _set_node_count(self):\n        self.node_count = self.raw_node_attr.shape[0]\n    \n    def encode_edge_attr(self, enocder):\n        self.edge_attr_encoded = torch.FloatTensor(enocder.transform(self.raw_edge_attr))\n        \n    def encode_node_attr(self, enocder):\n        self.node_attr_encoded = torch.FloatTensor(enocder.transform(self.raw_node_attr))\n        \n    def extend_node_attr(self):\n        if self.edge_attr_encoded is None:\n            print(\"Edge Attr Need to be Encoded!\")\n            raise\n        node_index = self.node_index\n        node_attr = self.node_attr_encoded.index_select(dim=0,index=self.node_index)\n        node_flag = self.node_flag\n        \n        edge_index = self.raw_edge_index\n        edge_attr = self.edge_attr_encoded\n        edge_flag = self.raw_edge_flag\n        \n        node_index_split = torch.tensor_split(node_index, node_flag)\n        node_attr_split = torch.tensor_split(node_attr, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        edge_attr_split = torch.tensor_split(edge_attr, edge_flag)\n\n        base = 0\n        new_node_attr = []\n        new_edge_flag = []\n        new_edge_index = []\n        \n        for i_snapshot in range(self.snapshot_count):\n            _node_index = node_index_split[i_snapshot]\n            _node_attr = node_attr_split[i_snapshot]\n            _edge_index = edge_index_split[i_snapshot]\n            _edge_attr = edge_attr_split[i_snapshot]\n\n            if _edge_index.shape[1] != _edge_attr.shape[0]:\n                print(i_snapshot, edge_index.shape, _edge_attr.shape)\n                raise\n            if _edge_index.shape[1] > 0:\n                index_dict = {}\n                for i_edge in range(_edge_index.shape[1]):\n                    index_tuple = tuple(_edge_index[:,i_edge].tolist())\n                    if index_tuple in index_dict:\n                        index_dict[index_tuple] += [i_edge]\n                    else:\n                        index_dict[index_tuple] = [i_edge]\n\n                _new_edge_index = []\n                _new_edge_attr = []\n                for key in index_dict.keys():\n                    _new_edge_index.append(key)\n                    _new_edge_attr.append(torch.sum(_edge_attr.index_select(0, torch.LongTensor(index_dict[key])),dim=0).unsqueeze(0))\n\n                _new_edge_index = torch.LongTensor(_new_edge_index).T\n                _new_edge_attr = torch.cat(_new_edge_attr,dim=0)\n                \n                base += _new_edge_index.shape[1]\n                new_edge_index.append(_new_edge_index)\n\n#                 _source_attr = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1])).to(device)\n#                 _target_attr = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1])).to(device)\n#                 _source_attr.index_add_(0, _new_edge_index[0], _new_edge_attr)\n#                 _target_attr.index_add_(0, _new_edge_index[1], _new_edge_attr)\n#                 new_node_attr.append(torch.cat([_node_attr,_source_attr,_target_attr], dim=1))\n\n                _node_attr_extend = torch.zeros((_node_attr.shape[0], _new_edge_attr.shape[1]))\n                _node_attr_extend.index_add_(0, _new_edge_index[0], _new_edge_attr)\n                _node_attr_extend.index_add_(0, _new_edge_index[1], _new_edge_attr)\n                new_node_attr.append(torch.cat([_node_attr,_node_attr_extend], dim=1)) \n                \n            new_edge_flag.append(base)\n        \n        self.node_attr = torch.cat(new_node_attr, dim=0)\n        self.edge_flag = torch.LongTensor(new_edge_flag)\n        self.edge_index = torch.cat(new_edge_index,dim=1)\n    \n    def remove_init_stop(self, threshold, period):\n        node_index = self.node_index\n        node_attr = self.node_attr\n        node_flag = self.node_flag\n\n        edge_index = self.edge_index\n        edge_flag = self.edge_flag\n        \n        raw_edge_index = self.raw_edge_index\n        raw_edge_attr = self.raw_edge_attr\n        raw_edge_flag = self.raw_edge_flag\n\n        node_index_split = torch.tensor_split(node_index, node_flag)\n        node_attr_split = torch.tensor_split(node_attr, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        \n        raw_edge_index_split = torch.tensor_split(raw_edge_index, raw_edge_flag, dim=1)\n        raw_edge_attr_split = np.split(raw_edge_attr, raw_edge_flag)\n\n        i_init = None\n        i_stop = None\n        for i_snapshot, node_num in enumerate(torch.diff(self.node_flag)):\n            if node_num > threshold:\n                i_init = i_snapshot+1\n                break\n\n        for i_snapshot, node_num in enumerate(torch.flip(torch.diff(self.node_flag),dims=[0])):\n            if node_num > threshold:\n                i_stop = self.node_flag.shape[0]-1-i_snapshot\n                break\n        \n        new_node_attr = torch.cat(node_attr_split[i_init+1+period:i_stop-period],dim=0)\n        new_node_index = torch.cat(node_index_split[i_init+1+period:i_stop-period],dim=0)\n        new_edge_index = torch.cat(edge_index_split[i_init+1+period:i_stop-period],dim=1)\n        new_raw_edge_index = torch.cat(raw_edge_index_split[i_init+1+period:i_stop-period], dim=1)\n        new_raw_edge_attr = np.concatenate(raw_edge_attr_split[i_init+1+period:i_stop-period])\n\n        new_node_flag = node_flag[i_init+period+1:i_stop-period-1]-node_flag[i_init+period]\n        new_edge_flag = edge_flag[i_init+period+1:i_stop-period-1]-edge_flag[i_init+period]\n        new_raw_edge_flag = raw_edge_flag[i_init+period+1:i_stop-period-1]-raw_edge_flag[i_init+period]\n        \n        self.node_attr = new_node_attr\n        self.node_index = new_node_index\n        self.edge_index = new_edge_index\n        self.raw_edge_attr = new_raw_edge_attr\n        self.raw_edge_index = new_raw_edge_index\n        \n        self.node_flag = new_node_flag\n        self.edge_flag = new_edge_flag\n        self.raw_edge_flag = new_raw_edge_flag\n        \n        self.ts_list = self.ts_list[i_init+1+period:i_stop-period]\n        \n        self._set_snapshot_count()\n    \n    def annotation2y(self, annotation, interval, overlap, offset= -30):\n        ts_list = self.ts_list\n        y = torch.zeros(self.snapshot_count, dtype=torch.long)\n        for i_ts, ts in enumerate(ts_list):\n            if ts < float(annotation[1])+offset and float(annotation[1])+offset <= ts+interval-overlap: \n                y[i_ts] = 1\n        self.y = y\n    \n    def to(self,device):\n        self.node_attr = self.node_attr.to(device)\n        self.node_index = self.node_index.to(device)\n        self.edge_index = self.edge_index.to(device)\n    \n    def get_adj_list(self):\n        edge_index = self.edge_index\n        edge_flag = self.edge_flag\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        adj_list = [torch.clamp(to_dense_adj(_edge_index)[0], min=0, max=1) for _edge_index in edge_index_split]\n        return adj_list\n\n    def _get_edge_index(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_index = self.edge_index[:,_start:_end]\n        return _edge_index\n\n    def _get_edge_attr(self, time_index: int):\n        if self.edge_attr_encoded is None:\n            print(\"Edge Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_attr = self.edge_attr_encoded[_start:_end]\n        return _edge_attr\n    \n    def _get_node_index_attr(self, time_index: int):\n        if self.node_attr_encoded is None:\n            print(\"Node Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.node_flag[time_index-1]\n        _end = self.node_flag[time_index]\n        _node_index = self.node_index[_start:_end]\n        _node_attr = self.node_attr_encoded[_node_index]\n        return _node_index,_node_attr\n    \n    def _get_timestamp(self, time_index: int):\n        _timestamp = self.ts_list[time_index]\n        return _timestamp\n\n\n    def __getitem__(self, time_index: int):\n        edge_index = self._get_edge_index(time_index)\n        edge_attr = self._get_edge_attr(time_index)\n        node_index,node_attr = self._get_node_index_attr(time_index)\n        _timestamp = self._get_timestamp(time_index)\n\n        snapshot = Data(\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            node_index=node_index,\n            node_attr=node_attr,\n            timestamp=_timestamp\n        )\n        return snapshot\n\n    def __next__(self):\n        if self.t < self.snapshot_count:\n            snapshot = self[self.t]\n            self.t = self.t + 1\n            return snapshot\n        else:\n            self.t = 0\n            raise StopIteration\n\n    def __iter__(self):\n        self.t = 0\n        return self\n    \n    def __len__(self):\n        return self.snapshot_count\n\n\nclass GraphDatasetLoader(object):\n    def __init__(self,input_path=\"\"):\n        self.input_path = input_path\n        self._read_data()\n    \n    def _read_data(self):\n        self._dataset = np.load(self.input_path)\n\n    def get_dataset(self): # -> DynamicGraphTemporalSignal:\n        dataset = GraphSignal(\n            edge_flag = self._dataset['edge_flag'],\n            edge_index = self._dataset['edge_index'],\n            edge_attr = self._dataset['edge_attr'],\n            node_flag = self._dataset['node_flag'],\n            node_index = self._dataset['node_index'],\n            node_attr = self._dataset['node_attr'],\n            ts_list = self._dataset['timestamp'],\n            path = self.input_path\n        )\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:09:46.428794Z","iopub.execute_input":"2023-08-23T02:09:46.429253Z","iopub.status.idle":"2023-08-23T02:09:46.486201Z","shell.execute_reply.started":"2023-08-23T02:09:46.429214Z","shell.execute_reply":"2023-08-23T02:09:46.484780Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"data_dir_0 = '/kaggle/input/dissertation-data'\ndata_dir_1_list = [\n    '2021-09-24-umbrella-experiment-64run-fran',\n    '2021-09-27-RaspVM-experiment-64run-env1',\n    '2021-09-29-RaspVM-experiment-64run-env1'\n]\n\n\nsignals = []\nannotation = []\n\nfor data_dir_1 in data_dir_1_list:\n    with open(os.path.join(data_dir_0, data_dir_1, \"annotated.json\")) as f:\n        annotated_dict = json.load(f)\n\n    for data_dir_2 in os.listdir(os.path.join(data_dir_0, data_dir_1)):\n        if data_dir_2 == \"annotated.json\":\n            continue\n        r = re.compile(\".*.npz\")\n        graph_files = list(filter(r.match, os.listdir(os.path.join(data_dir_0, data_dir_1, data_dir_2))))\n\n        if len(graph_files) > 1:\n            print(\"Multiple Graph Files!\")\n            raise\n        if len(graph_files) == 0:\n            print(\"Not Found Graph File!\")\n            continue\n\n        dataloader = GraphDatasetLoader(os.path.join(data_dir_0, data_dir_1, data_dir_2, graph_files[0]))\n        signal = dataloader.get_dataset()\n        signals.append(signal)\n        annotation.append(annotated_dict[data_dir_2])\n\n# split train and test dataset\n# split train and test dataset\nsignals_train, signals_test, annotation_train, annotation_test = train_test_split(signals, annotation, test_size=0.2, random_state=1365)\nsignals_train, signals_val, annotation_train, annotation_val = train_test_split(signals_train, annotation_train, test_size=0.5, random_state=1365)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:09:46.489888Z","iopub.execute_input":"2023-08-23T02:09:46.491030Z","iopub.status.idle":"2023-08-23T02:09:50.131216Z","shell.execute_reply.started":"2023-08-23T02:09:46.490988Z","shell.execute_reply":"2023-08-23T02:09:50.130209Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"_interval = 60\n_overlap = 30\n\nnode_num_list = []\nfor signal in signals_train:\n    node_num_list += torch.diff(signal.node_flag).tolist()\n    \nthreshold = np.median(node_num_list)\nperiod = 3\n\nprint(f\"Threshold = {threshold} Period = {period}\")\n\nnode_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nnode_attr_encoder = node_attr_encoder.fit(np.concatenate([sample.raw_node_attr for sample in signals_train]))\n\nedge_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nedge_attr_encoder = edge_attr_encoder.fit(np.concatenate([sample.raw_edge_attr for sample in signals_train]))\n\nfor i_signal, (signal, annotation) in enumerate(zip(signals_train, annotation_train)):\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n    signal.extend_node_attr()\n#     signal.remove_init_stop(threshold, period)\n    signal.node_attr = F.tanh(signal.node_attr)\n    signal.annotation2y(annotation, _interval, _overlap)\n    \nfor i_signal, (signal, annotation) in enumerate(zip(signals_val, annotation_val)):\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n    signal.extend_node_attr()\n#     signal.remove_init_stop(threshold, period)\n    signal.node_attr = F.tanh(signal.node_attr)\n    signal.annotation2y(annotation, _interval, _overlap)\n\n# for signal in signals_train:\n#     signal.to(device)\n\n# for signal in signals_val:\n#     signal.to(device)\n    \n    \nIN_CHANNELS = signals_train[0].node_attr.shape[1]\n# EDGE_CHANNELS = signals_train[0].edge_attr_encoded.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:09:50.135203Z","iopub.execute_input":"2023-08-23T02:09:50.135532Z","iopub.status.idle":"2023-08-23T02:10:59.857825Z","shell.execute_reply.started":"2023-08-23T02:09:50.135504Z","shell.execute_reply":"2023-08-23T02:10:59.856650Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Threshold = 25.0 Period = 3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multi-Layer GraphConv","metadata":{}},{"cell_type":"code","source":"class MultiGraphConv(torch.nn.Module):\n    def __init__(\n        self, \n        channels, \n    ):\n        super().__init__() \n        \n        self.conv1 = GCNConv(channels, channels * 2)\n#         self.conv2 = GCNConv(channels * 2, channels)\n        self.conv3 = GCNConv(channels * 2, channels)\n        \n\n    def forward(self, x, edge_index):\n        out = x\n        out = F.dropout(out, p=0.6)\n        out = F.elu(self.conv1(out, edge_index))\n#         out = F.dropout(out, p=0.6)\n#         out = F.elu(self.conv2(out, edge_index))\n        out = F.dropout(out, p=0.6)\n        out = self.conv3(out, edge_index)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.859926Z","iopub.execute_input":"2023-08-23T02:10:59.860383Z","iopub.status.idle":"2023-08-23T02:10:59.869760Z","shell.execute_reply.started":"2023-08-23T02:10:59.860348Z","shell.execute_reply":"2023-08-23T02:10:59.868534Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Graph RNN Operator","metadata":{}},{"cell_type":"code","source":"class GraphGRU(torch.nn.Module):\n    def __init__(\n        self,\n        channels: int,\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        self.channels = channels\n        self.bias = bias\n        self._create_parameters_and_layers()\n\n    def _create_update_gate_parameters_and_layers(self):\n\n        self.conv_x_z = MultiGraphConv(channels = self.channels)\n\n        self.conv_h_z = MultiGraphConv(channels = self.channels)\n        \n    def _create_reset_gate_parameters_and_layers(self):\n\n        self.conv_x_r = MultiGraphConv(channels = self.channels)\n\n        self.conv_h_r = MultiGraphConv(channels = self.channels)\n\n    def _create_candidate_state_parameters_and_layers(self):\n\n        self.conv_x_h = MultiGraphConv(channels = self.channels)\n\n        self.conv_h_h = MultiGraphConv(channels = self.channels)\n\n    def _create_parameters_and_layers(self):\n        self._create_update_gate_parameters_and_layers()\n        self._create_reset_gate_parameters_and_layers()\n        self._create_candidate_state_parameters_and_layers()\n\n    def _set_hidden_state(self, X, H):\n        if H is None:\n            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n        return H\n\n    def _calculate_update_gate(self, X, edge_index, H):\n        Z = self.conv_x_z(X, edge_index)\n        Z = Z + self.conv_h_z(H, edge_index)\n        Z = torch.sigmoid(Z)\n        return Z\n\n    def _calculate_reset_gate(self, X, edge_index, H):\n        R = self.conv_x_r(X, edge_index)\n        R = R + self.conv_h_r(H, edge_index)\n        R = torch.sigmoid(R)\n        return R\n\n    def _calculate_candidate_state(self, X, edge_index, H, R):\n        H_tilde = self.conv_x_h(X, edge_index)\n        H_tilde = H_tilde + self.conv_h_h(H * R, edge_index)\n        H_tilde = torch.tanh(H_tilde)\n        return H_tilde\n\n    def _calculate_hidden_state(self, Z, H, H_tilde):\n        H = Z * H + (1 - Z) * H_tilde\n        return H\n\n    def forward(\n            self,\n            X: torch.FloatTensor,\n            edge_index: torch.LongTensor,\n            H: torch.FloatTensor = None,\n        ) -> torch.FloatTensor:\n        H = self._set_hidden_state(X, H)\n        Z = self._calculate_update_gate(X, edge_index, H)\n        R = self._calculate_reset_gate(X, edge_index, H)\n        H_tilde = self._calculate_candidate_state(X, edge_index, H, R)\n        H = self._calculate_hidden_state(Z, H, H_tilde)\n        return H","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.871861Z","iopub.execute_input":"2023-08-23T02:10:59.872754Z","iopub.status.idle":"2023-08-23T02:10:59.891648Z","shell.execute_reply.started":"2023-08-23T02:10:59.872715Z","shell.execute_reply":"2023-08-23T02:10:59.890474Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Graph RNN Layer","metadata":{}},{"cell_type":"code","source":"class GraphGRULayer(torch.nn.Module):\n    def __init__(\n        self,\n        channels: int,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.gru = GraphGRU(\n            channels = channels,\n            bias = bias\n        )\n        \n        self.channels = channels\n        \n    def forward(\n        self,\n        X: torch.FloatTensor,\n        node_index: torch.LongTensor,\n        node_flag: torch.LongTensor,\n        edge_index: torch.LongTensor,\n        edge_flag: torch.LongTensor,\n        direction: bool # True for Forward; False for Backward\n    ) -> torch.FloatTensor:\n        \n        X_split = torch.tensor_split(X, node_flag)\n        node_index_split = torch.tensor_split(node_index, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        \n#         print(X_split[0].device)\n#         print(node_index_split[0].device)\n#         print(edge_index_split[0].device)\n             \n        pre_hidden = None\n        \n        outs = []\n        if direction:\n            snapshot_index = range(len(X_split))\n        else:\n            snapshot_index = range(len(X_split)-1,-1,-1)\n            \n        for i_snapshot in snapshot_index:\n            _X = X_split[i_snapshot]\n            _node_index = node_index_split[i_snapshot]\n            _edge_index = edge_index_split[i_snapshot]\n            \n            curr_hidden = torch.zeros(_X.shape[0],self.channels).to(_X.device)\n            if i_snapshot != 0:\n                pre_node_index = node_index_split[i_snapshot-1]\n                # Solution from https://discuss.pytorch.org/t/find-indexes-of-elements-from-one-tensor-that-matches-in-another-tensor/147482/2\n                _index = (node_index_split[i_snapshot-1].unsqueeze(1) == node_index_split[i_snapshot].unsqueeze(0)).nonzero() \n                curr_hidden.index_add_(0, _index[:,1], torch.index_select(pre_hidden,0, _index[:,0]))\n\n            new_hidden = self.gru(_X, _edge_index, curr_hidden)\n            pre_hidden = new_hidden\n            outs.append(new_hidden)\n        if direction:\n            H = torch.cat(outs)\n        else:\n            H = torch.cat(outs[::-1])\n        return H","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.893214Z","iopub.execute_input":"2023-08-23T02:10:59.893903Z","iopub.status.idle":"2023-08-23T02:10:59.909641Z","shell.execute_reply.started":"2023-08-23T02:10:59.893866Z","shell.execute_reply":"2023-08-23T02:10:59.908564Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# STAE Model","metadata":{}},{"cell_type":"code","source":"class STAE(torch.nn.Module): # Not Heterogeneous\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        gcn_channels,\n        embed_layers,  \n        decide_layers,\n    ):\n        super().__init__()\n        \n        # Encoder Embeding\n#         layers = [torch.nn.BatchNorm1d(in_channels)]\n        layers = []\n        pre_h_num = in_channels\n        for h_num in embed_layers:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,gcn_channels))\n        self.embed_net = Seq(*layers)\n                \n        self.encoder_gru = GraphGRULayer(channels=gcn_channels)\n\n        layers = []\n        pre_h_num = gcn_channels\n        for h_num in decide_layers:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,out_channels))\n#         layers.append(torch.nn.Sigmoid())\n        self.decide_net = Seq(*layers)\n        \n        # Decoder\n#         self.decoder = InnerProductDecoder()\n        layers = []\n        pre_h_num = out_channels\n        for h_num in decide_layers[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,gcn_channels))\n        self.decode_decide_net = Seq(*layers)\n        \n        self.decoder_gru = GraphGRULayer(channels=gcn_channels)\n        \n        layers = []\n        pre_h_num = gcn_channels\n        for h_num in embed_layers[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,in_channels))\n        self.decode_embed_net = Seq(*layers)\n        \n\n    def forward(self, x, node_index, node_flag, edge_index, edge_flag):\n        # Encoder\n        out = self.embed_net(x)\n        \n        # GNN layer\n        h_encoder = self.encoder_gru(out, node_index, node_flag, edge_index, edge_flag, True) \n        \n        out = self.decide_net(h_encoder)\n        out = self.decode_decide_net(out)\n \n        h_decoder = self.decoder_gru(out, node_index, node_flag, torch.flip(edge_index,dims=(0,)), edge_flag, True)\n        \n        out = self.decode_embed_net(h_decoder)\n\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.914612Z","iopub.execute_input":"2023-08-23T02:10:59.914996Z","iopub.status.idle":"2023-08-23T02:10:59.933179Z","shell.execute_reply.started":"2023-08-23T02:10:59.914962Z","shell.execute_reply":"2023-08-23T02:10:59.931892Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Batch Signal","metadata":{}},{"cell_type":"code","source":"# Concat Batchs to from BatchSignal\nclass BatchSignal():\n    def __init__(self, signals, batch_size, seq_len):   \n        data_list_2D = [[] for _ in range(seq_len)]\n        node_index_split_list = [[] for _ in range(seq_len)]\n        base_node_index = 0\n        \n        self.batch_index = [[i_signal, 0] for i_signal in np.random.randint(low=0, high=len(signals), size=batch_size, dtype=int)]\n\n        for i_batch, (i_signal,_) in enumerate(self.batch_index):\n            signal = signals[i_signal]\n    \n            while signal.snapshot_count < seq_len:\n                new_index = np.random.randint(low=0, high=len(signals))\n                signal = signals[new_index]\n                self.batch_index[i_batch] = [new_index, 0]\n            \n            node_attr_split = signal.node_attr.tensor_split(signal.node_flag)\n            node_index_split = signal.node_index.tensor_split(signal.node_flag)\n            edge_index_split = signal.edge_index.tensor_split(signal.edge_flag, dim=1)\n\n            if signal.snapshot_count-seq_len != 0:\n                _start = np.random.randint(low=0, high=signal.snapshot_count-seq_len+1)\n                self.batch_index[i_batch][1] = _start\n            else:\n                _start = 0\n\n            for i_series, i_snapshot in enumerate(range(_start, _start+seq_len)):\n                data = Data(\n                    x = node_attr_split[i_snapshot],\n                    edge_index = edge_index_split[i_snapshot],\n                    y = signal.y[i_snapshot]\n                )\n                data_list_2D[i_series].append(data)\n                node_index_split_list[i_series].append(node_index_split[i_snapshot]+base_node_index)\n            base_node_index += signal.node_count\n\n        batch_list = [Batch.from_data_list(data_list) for data_list in data_list_2D]\n        node_index_list = [torch.cat(_node_index, dim=0) for _node_index in node_index_split_list]\n\n        node_flag_list = []\n        edge_flag_list = []\n        node_attr = []\n        edge_index = []\n\n        node_base = 0\n        edge_base = 0\n\n        for batch in batch_list:\n            node_base += batch.x.shape[0]\n            edge_base += batch.edge_index.shape[1]\n            node_flag_list.append(node_base)\n            edge_flag_list.append(edge_base)\n\n        self.node_index = torch.cat([_node_index for _node_index in node_index_list], dim=0) \n        self.node_attr = torch.cat([_batch.x for _batch in batch_list], dim=0)\n        self.edge_index = torch.cat([_batch.edge_index for _batch in batch_list], dim=1)\n        self.y = torch.cat([_batch.y for _batch in batch_list], dim=0)\n        self.batch_index_dict = [batch._slice_dict for batch in batch_list]\n        \n        self.node_flag = torch.LongTensor(node_flag_list)[:-1]\n        self.edge_flag = torch.LongTensor(edge_flag_list)[:-1]\n    \n    def to(self, device):\n        self.node_attr = self.node_attr.to(device)\n        self.node_index = self.node_index.to(device)\n        self.edge_index = self.edge_index.to(device)\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.934963Z","iopub.execute_input":"2023-08-23T02:10:59.935372Z","iopub.status.idle":"2023-08-23T02:10:59.957922Z","shell.execute_reply.started":"2023-08-23T02:10:59.935335Z","shell.execute_reply":"2023-08-23T02:10:59.956557Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"# train_loop\ndef train_loop(signal, model, loss_fn, optimizer, device, split_loss: bool = False):\n    model.train()\n    \n    X = signal.node_attr\n    node_index = signal.node_index\n    node_flag = signal.node_flag\n    edge_index = signal.edge_index\n    edge_flag = signal.edge_flag\n    \n    outs = model(X, node_index, node_flag, edge_index, edge_flag)\n\n    train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n    total_loss = torch.mean(train_losses)\n        \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    if split_loss:\n        snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n        return snapshot_losses\n    else:\n        return total_loss.cpu().detach().numpy()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.959815Z","iopub.execute_input":"2023-08-23T02:10:59.960952Z","iopub.status.idle":"2023-08-23T02:10:59.973523Z","shell.execute_reply.started":"2023-08-23T02:10:59.960914Z","shell.execute_reply":"2023-08-23T02:10:59.972674Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Test Loop","metadata":{}},{"cell_type":"code","source":"# test_loop\ndef test_loop(signal, model, loss_fn, optimizer, device, split_loss: bool = False):\n    with torch.no_grad():\n        X = signal.node_attr\n        node_index = signal.node_index\n        node_flag = signal.node_flag\n        edge_index = signal.edge_index\n        edge_flag = signal.edge_flag\n\n        outs = model(X, node_index, node_flag, edge_index, edge_flag)\n\n        train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n        total_loss = torch.mean(train_losses)\n        snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n    \n        if split_loss:\n            snapshot_losses = [loss.cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n            return snapshot_losses\n        else:\n            return total_loss.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.974906Z","iopub.execute_input":"2023-08-23T02:10:59.976039Z","iopub.status.idle":"2023-08-23T02:10:59.985674Z","shell.execute_reply.started":"2023-08-23T02:10:59.976002Z","shell.execute_reply":"2023-08-23T02:10:59.984983Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 30\nMIN_LEN = min([signal.snapshot_count for signal in signals_train])\n# MIN_LEN = 15\n\nprint(MIN_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:10:59.986943Z","iopub.execute_input":"2023-08-23T02:10:59.987750Z","iopub.status.idle":"2023-08-23T02:11:00.002525Z","shell.execute_reply.started":"2023-08-23T02:10:59.987709Z","shell.execute_reply":"2023-08-23T02:11:00.001336Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"30\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.hist([signal.snapshot_count for signal in signals_train],bins=30)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:11:00.003921Z","iopub.execute_input":"2023-08-23T02:11:00.005228Z","iopub.status.idle":"2023-08-23T02:11:00.417908Z","shell.execute_reply.started":"2023-08-23T02:11:00.005162Z","shell.execute_reply":"2023-08-23T02:11:00.416818Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(array([ 6., 21.,  0.,  7.,  0., 19., 11.,  0.,  6.,  0.,  2.,  1.,  0.,\n         2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  1.]),\n array([30. , 30.6, 31.2, 31.8, 32.4, 33. , 33.6, 34.2, 34.8, 35.4, 36. ,\n        36.6, 37.2, 37.8, 38.4, 39. , 39.6, 40.2, 40.8, 41.4, 42. , 42.6,\n        43.2, 43.8, 44.4, 45. , 45.6, 46.2, 46.8, 47.4, 48. ]),\n <BarContainer object of 30 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ndef batch_node_eval_func(batch_signal, batch_node_losses, threshold):\n    batch_index_dict = batch_signal.batch_index_dict\n    y_pred = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        for losses in loss_split:\n            y_pred.append(int(any(losses.reshape(-1, 1) > threshold)))\n    tn, fp, fn, tp = confusion_matrix(batch_signal.y, y_pred, labels=[0,1]).ravel()\n    accuracy = (tn+tp)/(tn+fp+fn+tp)\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return accuracy, precision, recall\n\ndef signal_node_eval_func(signal, node_losses, threshold):\n    y_pred = []\n    for losses in node_losses:\n        y_pred.append(int(any(losses.reshape(-1, 1) > threshold)))\n    tn, fp, fn, tp = confusion_matrix(signal.y, y_pred, labels=[0,1]).ravel()\n    return tn, fp, fn, tp\n\ndef get_graph_anomaly_threshold(batch_signal, batch_node_losses,q=95):\n    batch_index_dict = batch_signal.batch_index_dict\n    graph_losses = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        graph_losses += [np.sum(node_losses) for node_losses in loss_split]\n        \n    return np.percentile(graph_losses,q)\n\ndef batch_graph_eval_func(batch_signal, batch_node_losses, threshold):\n    batch_index_dict = batch_signal.batch_index_dict\n    y_pred = []\n    for i_series in range(MIN_LEN):\n        loss_split = np.split(batch_node_losses[i_series], batch_index_dict[i_series]['x'][1:-1])\n        for losses in loss_split:\n            y_pred.append(int(np.sum(losses.reshape(-1, 1)) > threshold))\n    tn, fp, fn, tp = confusion_matrix(batch_signal.y, y_pred, labels=[0,1]).ravel()\n    accuracy = (tn+tp)/(tn+fp+fn+tp)\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return accuracy, precision, recall\n\ndef signal_graph_eval_func(signal, node_losses, threshold):\n    y_pred = []\n    for losses in node_losses:\n        y_pred.append(int(np.sum(losses.reshape(-1, 1)) > threshold))\n    tn, fp, fn, tp = confusion_matrix(signal.y, y_pred, labels=[0,1]).ravel()\n    return tn, fp, fn, tp","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:11:00.419761Z","iopub.execute_input":"2023-08-23T02:11:00.420505Z","iopub.status.idle":"2023-08-23T02:11:00.439175Z","shell.execute_reply.started":"2023-08-23T02:11:00.420465Z","shell.execute_reply":"2023-08-23T02:11:00.437902Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Train Function","metadata":{}},{"cell_type":"code","source":"def train_function(num_epoch, val_interval=100):\n    global GLOBAL_EPOCH\n    \n    history = {}\n    history['train'] = {\n        \"loss\":[],\n        \"accuracy\":[],\n        \"precision\":[],\n        \"recall\":[],\n        \"f1\":[],\n        \"time\":[]\n    }\n    history['val'] = {\n        \"loss\":[],\n        \"accuracy\":[],\n        \"precision\":[],\n        \"recall\":[],\n        \"f1\":[]\n    }\n    \n    batch_signal = BatchSignal(signals=signals_train, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)\n    for i_epoch in range(1,num_epoch+1):\n        _start = time.time()    \n#         if i_epoch % 10: \n        del batch_signal\n        batch_signal = BatchSignal(signals=signals_train, batch_size=BATCH_SIZE, seq_len=MIN_LEN).to(device)\n        train_losses = train_loop(batch_signal, model, loss_f, optimizer, device, split_loss=False)\n        _end = time.time()\n        if i_epoch % val_interval == 0:\n            node_losses_list_train = []\n            for i_signal, signal in enumerate(signals_train):\n                signal.to(device)\n                node_losses = test_loop(signal, model, loss_f, optimizer, device, split_loss=True)\n                node_losses_list_train.append(node_losses)\n            node_losses_list_val = []\n            for i_signal, signal in enumerate(signals_val):\n                signal.to(device)\n                node_losses = test_loop(signal, model, loss_f, optimizer, device, split_loss=True)\n                node_losses_list_val.append(node_losses)\n            \n            \n            train_losses = np.concatenate([np.concatenate(node_losses) for node_losses in node_losses_list_train])\n            val_losses = np.concatenate([np.concatenate(node_losses) for node_losses in node_losses_list_val])\n            \n            f1_list = []\n            for q in np.linspace(99.8,99.99,20):\n                anomaly_threshold = np.percentile(train_losses, q)\n                tn, fp, fn, tp = 0, 0, 0, 0\n                for signal, node_losses in zip(signals_val,node_losses_list_val):\n                    _tn, _fp, _fn, _tp = signal_node_eval_func(signal, node_losses, anomaly_threshold)\n                    tn += _tn; fp += _fp; fn += _fn; tp += _tp\n                val_f1 = 2*tp/(2*tp+fp+fn)\n                f1_list.append(val_f1)\n            \n            print(np.linspace(99.8,99.99,20)[np.argmax(f1_list)])    \n            anomaly_threshold = np.percentile(train_losses, np.linspace(99.8,99.99,20)[np.argmax(f1_list)]) \n            print(anomaly_threshold)\n\n            tn, fp, fn, tp = 0, 0, 0, 0\n            for signal, node_losses in zip(signals_train,node_losses_list_train):\n                _tn, _fp, _fn, _tp = signal_node_eval_func(signal, node_losses, anomaly_threshold)\n                tn += _tn; fp += _fp; fn += _fn; tp += _tp\n            train_accuracy = (tn+tp)/(tn+fp+fn+tp)\n            train_precision = tp/(tp+fp)\n            train_recall = tp/(tp+fn)\n            train_f1 = 2*tp/(2*tp+fp+fn)\n\n            tn, fp, fn, tp = 0, 0, 0, 0\n            for signal, node_losses in zip(signals_val,node_losses_list_val):\n                _tn, _fp, _fn, _tp = signal_node_eval_func(signal, node_losses, anomaly_threshold)\n                tn += _tn; fp += _fp; fn += _fn; tp += _tp\n            val_accuracy = (tn+tp)/(tn+fp+fn+tp)\n            val_precision = tp/(tp+fp)\n            val_recall = tp/(tp+fn)\n            val_f1 = 2*tp/(2*tp+fp+fn)\n\n            history['train']['loss'].append(np.mean(train_losses))\n            history['train']['accuracy'].append(train_accuracy)\n            history['train']['precision'].append(train_precision)\n            history['train']['recall'].append(train_recall)\n            history['train']['f1'].append(train_f1)\n            history['train']['time'].append(_end-_start)\n            \n            history['val']['loss'].append(np.mean(val_losses))\n            history['val']['accuracy'].append(val_accuracy)\n            history['val']['precision'].append(val_precision)\n            history['val']['recall'].append(val_recall)\n            history['val']['f1'].append(val_f1)\n            \n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train loss {np.mean(train_losses):.4f} val loss {np.mean(val_losses):.4f}\")\n            t = Texttable()\n            t.add_rows([\n                ['', 'Accuracy', 'Precision', 'Recall', 'F1'], \n                ['Train', train_accuracy, train_precision, train_recall, train_f1], \n                ['Val', val_accuracy, val_precision, val_recall, val_f1]])\n            print(t.draw())\n        else:\n#             pass\n#             history['train']['loss'].append(np.mean(train_losses))\n            if i_epoch % 10 == 0:\n                print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train loss {np.mean(train_losses):.4f}\")\n    GLOBAL_EPOCH += num_epoch\n    return history","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:11:00.443053Z","iopub.execute_input":"2023-08-23T02:11:00.443387Z","iopub.status.idle":"2023-08-23T02:11:00.472900Z","shell.execute_reply.started":"2023-08-23T02:11:00.443359Z","shell.execute_reply":"2023-08-23T02:11:00.471695Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# GLOBAL_EPOCH = 0\n# model = STAE(\n#     in_channels=IN_CHANNELS, \n#     out_channels=16, \n#     gcn_channels=256,\n#     embed_layers=[128,512,256],\n#     decide_layers=[256,128,128,64],\n# )\n\n# # model = torch.load(\n# #     \"/kaggle/input/tgae-model-saved/TGAE2_GRUConv_50_5.7319_5.8686.model\",\n# #     map_location=torch.device(device)\n# # )\n\n# # loss_f = torch.nn.CrossEntropyLoss()\n# loss_f = torch.nn.MSELoss(reduction = 'none')\n# optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n\n# model = model.to(device)\n# loss_f = loss_f.to(device)\n\n# history = train_function(1000,200)\n# history_list.append(history)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:11:00.476849Z","iopub.execute_input":"2023-08-23T02:11:00.477277Z","iopub.status.idle":"2023-08-23T02:11:00.490035Z","shell.execute_reply.started":"2023-08-23T02:11:00.477248Z","shell.execute_reply":"2023-08-23T02:11:00.488890Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from itertools import product\nout_channels_range = [8,16,32]\ngcn_channels_range = [64,128,256]\n\ncombinations = list(product(out_channels_range, gcn_channels_range))\nprint(len(combinations))\nhistory_list = [] \nfor out_channels, gcn_channels in combinations:\n    GLOBAL_EPOCH = 0\n    print(out_channels, gcn_channels)\n    model = STAE(\n        in_channels=IN_CHANNELS, \n        out_channels=out_channels, \n        gcn_channels=gcn_channels,\n        embed_layers=[128,512,256],\n        decide_layers=[256,128,128,64],\n    )\n\n    # model = torch.load(\n    #     \"/kaggle/input/tgae-model-saved/TGAE2_GRUConv_50_5.7319_5.8686.model\",\n    #     map_location=torch.device(device)\n    # )\n\n    # loss_f = torch.nn.CrossEntropyLoss()\n    loss_f = torch.nn.MSELoss(reduction = 'none')\n    optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n\n    model = model.to(device)\n    loss_f = loss_f.to(device)\n    \n    history = train_function(200,200)\n    history_list.append(history)\n    \n    del model\n    del loss_f\n    del optimizer\n    torch.cuda.empty_cache()\n    print(\"---------------------------------------------------------------------------\")\n#     torch.save(model,f\"TGAE2_GRUConv_{GLOBAL_EPOCH}_{np.mean(history_train_list[-1]):.4f}_{np.mean(history_val_list[-1]):.4f}.model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-23T02:11:00.493102Z","iopub.execute_input":"2023-08-23T02:11:00.493904Z","iopub.status.idle":"2023-08-23T02:55:06.085087Z","shell.execute_reply.started":"2023-08-23T02:11:00.493866Z","shell.execute_reply":"2023-08-23T02:55:06.083917Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"9\n8 64\n10/200: cost 0.8582s train loss 1.4269\n20/200: cost 0.8784s train loss 1.3505\n30/200: cost 0.8765s train loss 1.3150\n40/200: cost 0.8677s train loss 1.2814\n50/200: cost 1.0467s train loss 1.2779\n60/200: cost 0.8661s train loss 1.2722\n70/200: cost 0.9406s train loss 1.2006\n80/200: cost 0.8577s train loss 1.2155\n90/200: cost 1.0950s train loss 1.1224\n100/200: cost 0.8832s train loss 1.1144\n110/200: cost 0.8801s train loss 1.1219\n120/200: cost 1.1662s train loss 1.0599\n130/200: cost 0.8778s train loss 1.0579\n140/200: cost 0.8924s train loss 1.0269\n150/200: cost 0.8656s train loss 1.0155\n160/200: cost 0.8599s train loss 0.9942\n170/200: cost 0.8765s train loss 0.9545\n180/200: cost 0.9347s train loss 0.9448\n190/200: cost 0.9658s train loss 0.9078\n99.94\n4.162714590740183\n200/200: cost 0.8740s train loss 0.8954 val loss 0.8983\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.996    | 0.933     | 0.921  | 0.927 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.996    | 0.934     | 0.947  | 0.940 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n8 128\n10/200: cost 0.9227s train loss 1.4161\n20/200: cost 0.9486s train loss 1.3426\n30/200: cost 0.9539s train loss 1.3096\n40/200: cost 0.9544s train loss 1.2562\n50/200: cost 0.9189s train loss 1.2548\n60/200: cost 1.2110s train loss 1.1788\n70/200: cost 0.9417s train loss 1.1446\n80/200: cost 0.9525s train loss 1.0998\n90/200: cost 0.9081s train loss 1.1404\n100/200: cost 0.9154s train loss 1.0877\n110/200: cost 0.9486s train loss 1.0558\n120/200: cost 0.9513s train loss 0.9991\n130/200: cost 0.9083s train loss 0.9873\n140/200: cost 1.3463s train loss 0.9538\n150/200: cost 0.9408s train loss 0.9213\n160/200: cost 0.9447s train loss 0.8944\n170/200: cost 1.1269s train loss 0.8742\n180/200: cost 0.9521s train loss 0.8606\n190/200: cost 0.9263s train loss 0.8303\n99.92\n3.6320692514418957\n200/200: cost 0.9293s train loss 0.8156 val loss 0.8226\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.992    | 0.841     | 0.908  | 0.873 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.992    | 0.826     | 0.947  | 0.882 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n8 256\n10/200: cost 1.1222s train loss 1.3994\n20/200: cost 1.1050s train loss 1.3154\n30/200: cost 1.1513s train loss 1.2531\n40/200: cost 1.0995s train loss 1.2345\n50/200: cost 1.1730s train loss 1.1782\n60/200: cost 1.1939s train loss 1.1087\n70/200: cost 1.1283s train loss 1.0832\n80/200: cost 1.1079s train loss 1.0720\n90/200: cost 1.0780s train loss 1.0486\n100/200: cost 1.0508s train loss 1.0276\n110/200: cost 1.2900s train loss 0.9721\n120/200: cost 1.1122s train loss 0.9751\n130/200: cost 1.1115s train loss 0.9290\n140/200: cost 1.3312s train loss 0.8919\n150/200: cost 1.1424s train loss 0.8648\n160/200: cost 1.0560s train loss 0.8591\n170/200: cost 1.1440s train loss 0.8169\n180/200: cost 1.0896s train loss 0.7850\n190/200: cost 1.2041s train loss 0.7737\n99.92\n3.5150765605926337\n200/200: cost 1.0358s train loss 0.7611 val loss 0.7692\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.991    | 0.779     | 0.974  | 0.865 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.990    | 0.763     | 0.987  | 0.860 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n16 64\n10/200: cost 1.0089s train loss 1.4428\n20/200: cost 0.9173s train loss 1.3349\n30/200: cost 0.8961s train loss 1.3057\n40/200: cost 0.8980s train loss 1.3152\n50/200: cost 0.8849s train loss 1.2715\n60/200: cost 0.9128s train loss 1.2156\n70/200: cost 0.9070s train loss 1.2083\n80/200: cost 0.8969s train loss 1.1182\n90/200: cost 0.8833s train loss 1.1386\n100/200: cost 0.8987s train loss 1.1036\n110/200: cost 0.9116s train loss 1.0673\n120/200: cost 0.8945s train loss 1.0684\n130/200: cost 0.9022s train loss 1.0308\n140/200: cost 0.9184s train loss 1.0252\n150/200: cost 0.9092s train loss 0.9853\n160/200: cost 0.9740s train loss 0.9789\n170/200: cost 1.1555s train loss 0.9731\n180/200: cost 0.9133s train loss 0.9472\n190/200: cost 0.9214s train loss 0.9263\n99.92999999999999\n4.055868589973279\n200/200: cost 0.8967s train loss 0.9041 val loss 0.9074\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.995    | 0.862     | 0.987  | 0.920 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.992    | 0.821     | 0.920  | 0.868 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n16 128\n10/200: cost 0.9732s train loss 1.4388\n20/200: cost 0.9049s train loss 1.3510\n30/200: cost 0.9027s train loss 1.4302\n40/200: cost 0.9495s train loss 1.2480\n50/200: cost 0.9421s train loss 1.2248\n60/200: cost 0.9222s train loss 1.2253\n70/200: cost 0.9624s train loss 1.1475\n80/200: cost 0.9106s train loss 1.1572\n90/200: cost 0.9669s train loss 1.1042\n100/200: cost 0.9533s train loss 1.0967\n110/200: cost 1.0301s train loss 1.0492\n120/200: cost 0.9435s train loss 1.0466\n130/200: cost 0.9160s train loss 1.0290\n140/200: cost 0.9186s train loss 1.0095\n150/200: cost 0.9672s train loss 0.9535\n160/200: cost 0.8986s train loss 0.9669\n170/200: cost 0.9987s train loss 0.9044\n180/200: cost 0.9404s train loss 0.8827\n190/200: cost 0.9643s train loss 0.8671\n99.92\n3.744678374099728\n200/200: cost 0.9093s train loss 0.8369 val loss 0.8438\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.991    | 0.781     | 0.987  | 0.872 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.991    | 0.771     | 0.987  | 0.865 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n16 256\n10/200: cost 1.2032s train loss 1.4193\n20/200: cost 1.2197s train loss 1.2877\n30/200: cost 1.2182s train loss 1.2471\n40/200: cost 1.2110s train loss 1.1868\n50/200: cost 1.1198s train loss 1.2241\n60/200: cost 1.1720s train loss 1.1133\n70/200: cost 1.0877s train loss 1.1169\n80/200: cost 1.0712s train loss 1.0840\n90/200: cost 1.0415s train loss 1.0702\n100/200: cost 1.1193s train loss 1.0067\n110/200: cost 1.0850s train loss 0.9692\n120/200: cost 1.1780s train loss 0.9303\n130/200: cost 1.2006s train loss 0.9132\n140/200: cost 1.0951s train loss 0.8799\n150/200: cost 1.1147s train loss 0.8547\n160/200: cost 1.0951s train loss 0.8397\n170/200: cost 1.0950s train loss 0.8215\n180/200: cost 1.1683s train loss 0.8100\n190/200: cost 1.1659s train loss 0.7834\n99.92999999999999\n3.836321783232626\n200/200: cost 1.1357s train loss 0.7530 val loss 0.7665\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.993    | 0.839     | 0.961  | 0.896 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.991    | 0.779     | 0.987  | 0.871 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n32 64\n10/200: cost 0.8989s train loss 1.4734\n20/200: cost 0.9034s train loss 1.3424\n30/200: cost 1.0216s train loss 1.3481\n40/200: cost 0.9122s train loss 1.3349\n50/200: cost 0.9145s train loss 1.2335\n60/200: cost 0.8986s train loss 1.2453\n70/200: cost 1.0966s train loss 1.1967\n80/200: cost 1.2584s train loss 1.1010\n90/200: cost 0.9156s train loss 1.1061\n100/200: cost 1.1198s train loss 1.1297\n110/200: cost 0.9012s train loss 1.1069\n120/200: cost 0.9038s train loss 1.0397\n130/200: cost 0.9007s train loss 1.0428\n140/200: cost 0.9101s train loss 0.9866\n150/200: cost 0.8992s train loss 0.9568\n160/200: cost 0.9132s train loss 0.9289\n170/200: cost 0.9039s train loss 0.9126\n180/200: cost 0.9137s train loss 0.8933\n190/200: cost 0.9250s train loss 0.8898\n99.94\n4.224865554332675\n200/200: cost 0.9030s train loss 0.8636 val loss 0.8733\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.993    | 0.893     | 0.882  | 0.887 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.994    | 0.942     | 0.867  | 0.903 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n32 128\n10/200: cost 1.1463s train loss 1.4450\n20/200: cost 1.3366s train loss 1.3832\n30/200: cost 0.9795s train loss 1.3278\n40/200: cost 1.2351s train loss 1.2543\n50/200: cost 1.0686s train loss 1.2204\n60/200: cost 1.0182s train loss 1.2007\n70/200: cost 1.0040s train loss 1.1671\n80/200: cost 0.9237s train loss 1.1801\n90/200: cost 0.9315s train loss 1.1433\n100/200: cost 0.9478s train loss 1.0717\n110/200: cost 0.9230s train loss 1.0362\n120/200: cost 0.9657s train loss 0.9910\n130/200: cost 0.9165s train loss 0.9603\n140/200: cost 0.9100s train loss 0.9318\n150/200: cost 1.5024s train loss 0.9026\n160/200: cost 0.9237s train loss 0.8712\n170/200: cost 0.9244s train loss 0.8513\n180/200: cost 1.1665s train loss 0.8552\n190/200: cost 0.9196s train loss 0.8287\n99.92999999999999\n3.82592723813052\n200/200: cost 0.9629s train loss 0.8052 val loss 0.8163\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.994    | 0.851     | 0.974  | 0.908 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.992    | 0.793     | 0.973  | 0.874 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n32 256\n10/200: cost 1.2185s train loss 1.3609\n20/200: cost 1.0510s train loss 1.3518\n30/200: cost 1.2291s train loss 1.2189\n40/200: cost 1.1700s train loss 1.1860\n50/200: cost 1.0675s train loss 1.1949\n60/200: cost 1.1781s train loss 1.0924\n70/200: cost 1.0666s train loss 1.1231\n80/200: cost 1.1646s train loss 1.0466\n90/200: cost 1.0271s train loss 1.0612\n100/200: cost 1.0523s train loss 1.0079\n110/200: cost 1.1020s train loss 0.9558\n120/200: cost 1.0974s train loss 0.9168\n130/200: cost 1.2204s train loss 0.8936\n140/200: cost 1.1792s train loss 0.8563\n150/200: cost 1.3052s train loss 0.8335\n160/200: cost 1.2336s train loss 0.8209\n170/200: cost 1.1119s train loss 0.8386\n180/200: cost 1.1969s train loss 0.8143\n190/200: cost 1.1061s train loss 0.7703\n99.91\n3.4849749339580547\n200/200: cost 1.0462s train loss 0.7446 val loss 0.7544\n+-------+----------+-----------+--------+-------+\n|       | Accuracy | Precision | Recall |  F1   |\n+=======+==========+===========+========+=======+\n| Train | 0.989    | 0.753     | 0.961  | 0.844 |\n+-------+----------+-----------+--------+-------+\n| Val   | 0.987    | 0.705     | 0.987  | 0.822 |\n+-------+----------+-----------+--------+-------+\n---------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"for (out_channels, gcn_channels),history in zip(combinations,history_list):\n    print(out_channels, gcn_channels,history['train']['f1'][-1], history['val']['f1'][-1])","metadata":{"execution":{"iopub.status.busy":"2023-08-23T03:03:50.017749Z","iopub.execute_input":"2023-08-23T03:03:50.018177Z","iopub.status.idle":"2023-08-23T03:03:50.025598Z","shell.execute_reply.started":"2023-08-23T03:03:50.018145Z","shell.execute_reply":"2023-08-23T03:03:50.024216Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"8 64 0.9271523178807947 0.9403973509933775\n8 128 0.8734177215189873 0.8819875776397516\n8 256 0.8654970760233918 0.8604651162790697\n16 64 0.9202453987730062 0.8679245283018868\n16 128 0.872093023255814 0.8654970760233918\n16 256 0.8957055214723927 0.8705882352941177\n32 64 0.8874172185430463 0.9027777777777778\n32 128 0.9079754601226994 0.874251497005988\n32 256 0.8439306358381503 0.8222222222222222\n","output_type":"stream"}]},{"cell_type":"code","source":"for (out_channels, gcn_channels),history in zip(combinations,history_list):\n    print(f\"{history['train']['f1'][-1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-23T03:06:10.045217Z","iopub.execute_input":"2023-08-23T03:06:10.045703Z","iopub.status.idle":"2023-08-23T03:06:10.057276Z","shell.execute_reply.started":"2023-08-23T03:06:10.045664Z","shell.execute_reply":"2023-08-23T03:06:10.056125Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"0.927\n0.873\n0.865\n0.920\n0.872\n0.896\n0.887\n0.908\n0.844\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}