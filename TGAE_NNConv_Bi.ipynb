{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_gpu = torch.cuda.is_available()\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:12:41.254121Z","iopub.execute_input":"2023-07-28T09:12:41.254608Z","iopub.status.idle":"2023-07-28T09:12:45.745684Z","shell.execute_reply.started":"2023-07-28T09:12:41.254566Z","shell.execute_reply":"2023-07-28T09:12:45.744543Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize for CPU & GPU","metadata":{}},{"cell_type":"code","source":"if use_gpu:\n    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric --no-index --find-links=file:///kaggle/input/torch-geometric\n#     !pip install torch_geometric_temporal  \nelse:\n#     !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n    !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n    !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n#     !pip install torch_geometric_temporal\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-28T09:12:45.748389Z","iopub.execute_input":"2023-07-28T09:12:45.749673Z","iopub.status.idle":"2023-07-28T09:13:02.492709Z","shell.execute_reply.started":"2023-07-28T09:12:45.749637Z","shell.execute_reply":"2023-07-28T09:13:02.491447Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/torch-geometric\nProcessing /kaggle/input/torch-geometric/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_cluster-1.6.1-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/torch-geometric/torch_geometric-2.3.1-py3-none-any.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\nSuccessfully installed torch-cluster-1.6.1 torch-geometric-2.3.1 torch-scatter-2.1.1 torch-sparse-0.6.17 torch-spline-conv-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"from torch.profiler import profile, record_function, ProfilerActivity\nimport torch.autograd.profiler as profiler","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:02.495146Z","iopub.execute_input":"2023-07-28T09:13:02.495574Z","iopub.status.idle":"2023-07-28T09:13:02.502183Z","shell.execute_reply.started":"2023-07-28T09:13:02.495536Z","shell.execute_reply":"2023-07-28T09:13:02.501096Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport json\nfrom scipy.special import perm\nfrom itertools import combinations,chain\nfrom typing import List, Union\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nfrom torch import nn\nfrom torch.nn import Linear as Lin\nfrom torch.nn import ReLU\nfrom torch.nn import Sequential as Seq\nimport torch.nn.functional as F\nfrom torch_geometric.nn import NNConv\nfrom torch import autograd\n\nfrom sklearn.model_selection import train_test_split","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:02.505293Z","iopub.execute_input":"2023-07-28T09:13:02.505594Z","iopub.status.idle":"2023-07-28T09:13:04.134667Z","shell.execute_reply.started":"2023-07-28T09:13:02.505569Z","shell.execute_reply":"2023-07-28T09:13:04.133408Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"Edge_Flag = List[Union[np.ndarray, None]]\nEdge_Index = List[Union[np.ndarray, None]]\nEdge_Attr = List[Union[np.ndarray, None]]\n\nNode_Flag = List[Union[np.ndarray, None]]\nNode_Index = List[Union[np.ndarray, None]]\nNode_Attr = List[Union[np.ndarray, None]]\nAdditional_Attr = List[np.ndarray]\n\n\n\nclass GraphSignal(object):\n    # dynamic node static node attr dynamic edge and edge attr\n    def __init__(\n        self,\n        edge_flag: Edge_Flag,\n        edge_index: Edge_Index,\n        edge_attr: Edge_Attr,\n        node_flag: Node_Flag,\n        node_index: Node_Index,\n        node_attr: Node_Attr,\n        ts_list: List,\n    ):\n        \n        self.edge_flag = torch.LongTensor(edge_flag)\n        self.edge_index = torch.LongTensor(edge_index).T.to(device)\n        self.edge_attr = edge_attr\n        self.node_flag = torch.LongTensor(node_flag)\n        self.node_index = torch.LongTensor(node_index).to(device)\n        self.node_attr = node_attr\n        self.ts_list = ts_list\n        self.edge_attr_encoded = None\n        self.node_attr_encoded = None\n        \n        self._set_snapshot_count()\n\n    def _set_snapshot_count(self):\n        self.snapshot_count = len(self.edge_flag)\n    \n    def encode_edge_attr(self, enocder):\n        self.edge_attr_encoded = torch.FloatTensor(enocder.transform(self.edge_attr)).to(device)\n    \n    def encode_node_attr(self, enocder):\n        self.node_attr_encoded = torch.FloatTensor(enocder.transform(self.node_attr)).to(device)\n        \n    def _get_edge_index(self, time_index: int):\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_index = self.edge_index[:,_start:_end]\n        return _edge_index\n\n    # def _get_edge_weight(self, time_index: int):\n    #     if self.edge_weights[time_index] is None:\n    #         return self.edge_weights[time_index]\n    #     else:\n    #         return torch.FloatTensor(self.edge_weights[time_index])\n\n    def _get_edge_attr(self, time_index: int):\n        if self.edge_attr_encoded is None:\n            print(\"Edge Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.edge_flag[time_index-1]\n        _end = self.edge_flag[time_index]\n        _edge_attr = self.edge_attr_encoded[_start:_end]\n        return _edge_attr\n    \n    def _get_node_index_attr(self, time_index: int):\n        if self.node_attr_encoded is None:\n            print(\"Node Attr Need to be Encoded!\")\n            raise\n        if time_index == 0:\n            _start = 0\n        else:\n            _start = self.node_flag[time_index-1]\n        _end = self.node_flag[time_index]\n        _node_index = self.node_index[_start:_end]\n        _node_attr = self.node_attr_encoded[_node_index]\n        return _node_index,_node_attr\n    \n    def _get_timestamp(self, time_index: int):\n        _timestamp = self.ts_list[time_index]\n        return _timestamp\n\n\n    def __getitem__(self, time_index: int):\n        edge_index = self._get_edge_index(time_index)\n        edge_attr = self._get_edge_attr(time_index)\n        node_index,node_attr = self._get_node_index_attr(time_index)\n        _timestamp = self._get_timestamp(time_index)\n\n        snapshot = Data(\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            node_index=node_index,\n            node_attr=node_attr,\n            timestamp = _timestamp\n        )\n        return snapshot\n\n    def __next__(self):\n        if self.t < self.snapshot_count:\n            snapshot = self[self.t]\n            self.t = self.t + 1\n            return snapshot\n        else:\n            self.t = 0\n            raise StopIteration\n\n    def __iter__(self):\n        self.t = 0\n        return self\n    \n    def __len__(self):\n        return self.snapshot_count\n\n\nclass GraphDatasetLoader(object):\n    def __init__(self,input_path=\"\"):\n        self.input_path = input_path\n        self._read_data()\n    \n    def _read_data(self):\n        self._dataset = np.load(self.input_path)\n\n    def get_dataset(self): # -> DynamicGraphTemporalSignal:\n        dataset = GraphSignal(\n            edge_flag = self._dataset['edge_flag'],\n            edge_index = self._dataset['edge_index'],\n            edge_attr = self._dataset['edge_attr'],\n            node_flag = self._dataset['node_flag'],\n            node_index = self._dataset['node_index'],\n            node_attr = self._dataset['node_attr'],\n            ts_list = self._dataset['timestamp']\n        )\n        return dataset","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:04.136644Z","iopub.execute_input":"2023-07-28T09:13:04.137068Z","iopub.status.idle":"2023-07-28T09:13:04.164072Z","shell.execute_reply.started":"2023-07-28T09:13:04.137029Z","shell.execute_reply":"2023-07-28T09:13:04.163109Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## MultiNNConv","metadata":{}},{"cell_type":"code","source":"class MultiNNConv(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, edge_channels, conv_layers, edge_layers, lin_layers):\n        super().__init__()\n        \n        def _create_edge_net(edge_out_channels):\n            edge_net = Seq()\n            pre_size = edge_channels\n            for size in edge_layers:\n                edge_net.append(Lin(pre_size,size))\n                edge_net.append(ReLU())\n                pre_size = size\n            edge_net.append(Lin(pre_size,edge_out_channels))\n            return edge_net\n        \n        self.convs = nn.ModuleList()\n        pre_size = in_channels\n        for size in conv_layers:\n            edge_net = _create_edge_net(pre_size*size)\n            self.convs.append(NNConv(pre_size, size, edge_net, aggr='mean'))\n            pre_size = size\n        \n        self.lin_net = Seq()\n        for size in lin_layers[:-1]:\n            self.lin_net.append(Lin(pre_size,size))\n            pre_size = size\n        self.lin_net.append(ReLU())\n        self.lin_net.append(Lin(pre_size,out_channels))\n\n    def forward(self, x, edge_index, edge_attr):\n        out = x\n        with profiler.record_function(\"Graph Conv\"):\n            for conv in self.convs:\n                out = conv(\n                    x=out,\n                    edge_index=edge_index,\n                    edge_attr=edge_attr,\n                )\n        out = self.lin_net(out)\n        return out","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:04.166746Z","iopub.execute_input":"2023-07-28T09:13:04.167434Z","iopub.status.idle":"2023-07-28T09:13:04.184635Z","shell.execute_reply.started":"2023-07-28T09:13:04.167398Z","shell.execute_reply":"2023-07-28T09:13:04.183647Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Graph GRU Unit","metadata":{}},{"cell_type":"code","source":"class GraphGRU(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        edge_channels: int,\n        conv_layers: List,\n        edge_layers: List,\n        lin_layers: List,\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.edge_channels = edge_channels\n        self.conv_layers = conv_layers\n        self.edge_layers = edge_layers\n        self.lin_layers = lin_layers\n        self.bias = bias\n        self._create_parameters_and_layers()\n\n    def _create_update_gate_parameters_and_layers(self):\n\n        self.conv_x_z = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_z = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n        \n    def _create_reset_gate_parameters_and_layers(self):\n\n        self.conv_x_r = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_r = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n    def _create_candidate_state_parameters_and_layers(self):\n\n        self.conv_x_h = MultiNNConv(\n            in_channels = self.in_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n        self.conv_h_h = MultiNNConv(\n            in_channels = self.out_channels,\n            out_channels = self.out_channels,\n            edge_channels = self.edge_channels,\n            conv_layers = self.conv_layers,\n            edge_layers = self.edge_layers,\n            lin_layers = self.lin_layers,\n        )\n\n    def _create_parameters_and_layers(self):\n        self._create_update_gate_parameters_and_layers()\n        self._create_reset_gate_parameters_and_layers()\n        self._create_candidate_state_parameters_and_layers()\n\n    def _set_hidden_state(self, X, H):\n        if H is None:\n            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n        return H\n\n    def _calculate_update_gate(self, X, edge_index, edge_attr, H):\n        Z = self.conv_x_z(X, edge_index, edge_attr)\n        Z = Z + self.conv_h_z(H, edge_index, edge_attr)\n        Z = torch.sigmoid(Z)\n        return Z\n\n    def _calculate_reset_gate(self, X, edge_index, edge_attr, H):\n        R = self.conv_x_r(X, edge_index, edge_attr)\n        R = R + self.conv_h_r(H, edge_index, edge_attr)\n        R = torch.sigmoid(R)\n        return R\n\n    def _calculate_candidate_state(self, X, edge_index, edge_attr, H, R):\n        H_tilde = self.conv_x_h(X, edge_index, edge_attr)\n        H_tilde = H_tilde + self.conv_h_h(H * R, edge_index, edge_attr)\n        H_tilde = torch.tanh(H_tilde)\n        return H_tilde\n\n    def _calculate_hidden_state(self, Z, H, H_tilde):\n        H = Z * H + (1 - Z) * H_tilde\n        return H\n\n    def forward(\n            self,\n            X: torch.FloatTensor,\n            edge_index: torch.LongTensor,\n            edge_attr: torch.FloatTensor,\n            H: torch.FloatTensor = None,\n        ) -> torch.FloatTensor:\n            H = self._set_hidden_state(X, H)\n            Z = self._calculate_update_gate(X, edge_index, edge_attr, H)\n            R = self._calculate_reset_gate(X, edge_index, edge_attr, H)\n            H_tilde = self._calculate_candidate_state(X, edge_index, edge_attr, H, R)\n            H = self._calculate_hidden_state(Z, H, H_tilde)\n            return H","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:04.186297Z","iopub.execute_input":"2023-07-28T09:13:04.186696Z","iopub.status.idle":"2023-07-28T09:13:04.209167Z","shell.execute_reply.started":"2023-07-28T09:13:04.186646Z","shell.execute_reply":"2023-07-28T09:13:04.208187Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Graph GRU Layer and Global Hidden Function","metadata":{}},{"cell_type":"code","source":"def create_hidden(num_node, out_channels):\n#     hidden_global = torch.FloatTensor(np.zeros([num_node,out_channels])).to(device)\n    hidden = torch.empty([num_node,out_channels], dtype=torch.float).uniform_(0, 1).to(device)\n    return hidden\n\ndef select_hidden(hidden, index):\n#     h = hidden_global[index] #REGULAR INDEXING\n    h = hidden.index_select(dim=0, index=index) #INDEX SELECT\n    return h\n\n# TODO: Aggregation of hidden and cell\ndef update_hidden(hidden, h, index):\n    # hidden_global[index] = h.detach() #REGULAR INDEXING\n    # for key,value in mapping.items():\n    #     hidden_global[value] = h[key] \n    size = hidden.shape\n    hidden = torch.empty(size, dtype=torch.float).uniform_(0, 1).to(device)\n    hidden.index_add_(0, index, h)        \n\nclass GraphGRULayer(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        edge_channels: int,\n        conv_layers: List,\n        edge_layers: List,\n        lin_layers: List,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.gru = GraphGRU(\n            in_channels = in_channels,\n            out_channels = out_channels,\n            edge_channels = edge_channels,\n            conv_layers = conv_layers,\n            edge_layers = edge_layers,\n            lin_layers = lin_layers,\n            bias = bias\n        )\n        \n        self.out_channels = out_channels\n        \n    def forward(\n        self,\n        X: torch.FloatTensor,\n        node_index: torch.LongTensor,\n        node_flag: torch.LongTensor,\n        edge_index: torch.LongTensor,\n        edge_attr: torch.FloatTensor,\n        edge_flag: torch.LongTensor,\n        num_node: int,\n        direction: bool # True for Forward; False for Backward\n    ) -> torch.FloatTensor:\n        \n        X_split = torch.tensor_split(X, node_flag)\n        node_index_split = torch.tensor_split(node_index, node_flag)\n        edge_index_split = torch.tensor_split(edge_index, edge_flag, dim=1)\n        edge_attr_split = torch.tensor_split(edge_attr, edge_flag)\n        \n        hidden = create_hidden(num_node, self.out_channels)\n        \n        outs = []\n        if direction:\n            snapshot_index = range(len(X_split))\n        else:\n            snapshot_index = range(len(X_split)-1,-1,-1)\n        for i_snapshot in snapshot_index:\n            _X = X_split[i_snapshot]\n            _node_index = node_index_split[i_snapshot]\n            _edge_index = edge_index_split[i_snapshot]\n            _edge_attr = edge_attr_split[i_snapshot]\n\n            _hidden = select_hidden(hidden, _node_index)\n            _new_hidden = self.gru(_X, _edge_index, _edge_attr, _hidden)\n            update_hidden(hidden, _new_hidden, _node_index)\n            outs.append(_new_hidden)\n        \n        H = torch.cat(outs)\n        return H","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:04.210848Z","iopub.execute_input":"2023-07-28T09:13:04.211250Z","iopub.status.idle":"2023-07-28T09:13:04.232269Z","shell.execute_reply.started":"2023-07-28T09:13:04.211217Z","shell.execute_reply":"2023-07-28T09:13:04.231273Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## TGAE","metadata":{}},{"cell_type":"code","source":"class TGAE(torch.nn.Module): # Not Heterogeneous\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        edge_channels, \n        embed_layers, \n        gru_out_channels, \n        decide_layers,\n        gru_conv_layers, \n        gru_edge_layers, \n        gru_lin_layers\n    ):\n        super(TGAE, self).__init__()\n        \n        # Encoder\n        layers = []\n        pre_h_num = in_channels\n        for h_num in embed_layers[:-1]:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,embed_layers[-1]))\n        self.encoder_embedding_net = Seq(*layers)\n        \n        self.encoder_gru_forward = GraphGRULayer(\n            in_channels=embed_layers[-1],\n            out_channels=gru_out_channels,\n            edge_channels=edge_channels,\n            conv_layers=gru_conv_layers,\n            edge_layers=gru_edge_layers,\n            lin_layers=gru_lin_layers,\n        )\n        \n        self.encoder_gru_backward = GraphGRULayer(\n            in_channels=embed_layers[-1],\n            out_channels=gru_out_channels,\n            edge_channels=edge_channels,\n            conv_layers=gru_conv_layers,\n            edge_layers=gru_edge_layers,\n            lin_layers=gru_lin_layers,\n        )\n\n        layers = []\n        pre_h_num = gru_out_channels*2\n        for h_num in decide_layers:\n#             layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,out_channels))\n        self.encoder_deciding_net = Seq(*layers)\n        \n        # Decoder\n        layers = []\n        pre_h_num = out_channels\n        for h_num in decide_layers[::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,gru_out_channels))\n        self.decoder_deciding_net = Seq(*layers)\n        \n        self.decoder_gru_forward = GraphGRULayer(\n            in_channels=gru_out_channels,\n            out_channels=embed_layers[-1],\n            edge_channels=edge_channels,\n            conv_layers=gru_conv_layers,\n            edge_layers=gru_edge_layers,\n            lin_layers=gru_lin_layers,\n        )\n        \n        self.decoder_gru_backward = GraphGRULayer(\n            in_channels=gru_out_channels,\n            out_channels=embed_layers[-1],\n            edge_channels=edge_channels,\n            conv_layers=gru_conv_layers,\n            edge_layers=gru_edge_layers,\n            lin_layers=gru_lin_layers,\n        )\n        \n        layers = []\n        pre_h_num = embed_layers[-1]*2\n        for h_num in embed_layers[:-1][::-1]:\n            layers.append(torch.nn.Dropout(p=0.2))\n            layers.append(Lin(pre_h_num,h_num))\n            layers.append(torch.nn.LeakyReLU())\n            pre_h_num = h_num\n        layers.append(Lin(pre_h_num,in_channels))\n        self.decoder_embedding_net = Seq(*layers)\n        \n\n    def forward(self, x, node_index, node_flag, edge_index, edge_attr, edge_flag, num_node):\n        # Encoder\n        out = self.encoder_embedding_net(x)\n        \n        h_encoder_forward = self.encoder_gru_forward(out, node_index, node_flag, edge_index, edge_attr, edge_flag, num_node, True) \n        h_encoder_backward = self.encoder_gru_backward(out, node_index, node_flag, edge_index, edge_attr, edge_flag, num_node, False) \n        \n        # Concat\n        out = torch.concat([h_encoder_forward,h_encoder_backward],dim=1)\n        \n        out = self.encoder_deciding_net(out)\n        \n        # Decoder\n        out = self.decoder_deciding_net(out)\n        \n        h_decoder_forward = self.decoder_gru_forward(out, node_index, node_flag, torch.flip(edge_index,dims=(0,)), edge_attr, edge_flag, num_node, True)\n        h_decoder_backward = self.decoder_gru_backward(out, node_index, node_flag, torch.flip(edge_index,dims=(0,)), edge_attr, edge_flag, num_node, False)\n        \n        # Concat\n        out = torch.concat([h_decoder_forward,h_decoder_backward],dim=1)\n        \n        out = self.decoder_embedding_net(out)\n\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:04.233982Z","iopub.execute_input":"2023-07-28T09:13:04.234690Z","iopub.status.idle":"2023-07-28T09:13:04.258351Z","shell.execute_reply.started":"2023-07-28T09:13:04.234657Z","shell.execute_reply":"2023-07-28T09:13:04.257397Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Read Data","metadata":{}},{"cell_type":"code","source":"data_dir_0 = '/kaggle/input/dissertation-data'\ndata_dir_1 = '2021-09-11-umbrella-experiment-32run-fran'\n\n\nsignals = []\ny = []\nwith open(os.path.join(data_dir_0, data_dir_1, \"annotated.json\")) as f:\n    annotated_dict = json.load(f)\n\nfor data_dir_2 in os.listdir(os.path.join(data_dir_0, data_dir_1)):\n    if data_dir_2 == \"annotated.json\":\n        continue\n    r = re.compile(\".*.npz\")\n    graph_files = list(filter(r.match, os.listdir(os.path.join(data_dir_0, data_dir_1, data_dir_2))))\n\n    if len(graph_files) > 1:\n        print(\"Multiple Graph Files!\")\n        raise\n    if len(graph_files) == 0:\n        print(\"Not Found Graph File!\")\n        raise\n\n    dataloader = GraphDatasetLoader(os.path.join(data_dir_0, data_dir_1, data_dir_2, graph_files[0]))\n    signal = dataloader.get_dataset()\n    signals.append(signal)\n    y.append(annotated_dict[data_dir_2])\n\n# split train and test dataset\nsignals_train, signals_test, y_train, y_test = train_test_split(signals, y, test_size=0.2, random_state=1)\nsignals_train, signals_val, y_train, y_val = train_test_split(signals_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:04.264972Z","iopub.execute_input":"2023-07-28T09:13:04.265970Z","iopub.status.idle":"2023-07-28T09:13:09.200689Z","shell.execute_reply.started":"2023-07-28T09:13:04.265910Z","shell.execute_reply":"2023-07-28T09:13:09.199346Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"node_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nnode_attr_encoder = node_attr_encoder.fit(np.concatenate([sample.node_attr for sample in signals_train]))\n\nedge_attr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nedge_attr_encoder = edge_attr_encoder.fit(np.concatenate([sample.edge_attr for sample in signals_train]))\n\nfor signal in signals_train:\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n\nX_train = []\nfor signal in signals_train:\n#     X_train.append(list(signal))\n    X_train.append(signal.node_attr_encoded.index_select(dim=0,index=signal.node_index))\n    \nnum_node_train = []\nfor signal in signals_train:\n    num_node_train.append(signal.node_attr.shape[0])\n\n    \nfor signal in signals_val:\n    signal.encode_node_attr(node_attr_encoder)\n    signal.encode_edge_attr(edge_attr_encoder)\n\nX_val = []\n# for signal in signals_val:\n#     X_val.append(list(signal))\nfor signal in signals_val:\n#     X_train.append(list(signal))\n    X_val.append(signal.node_attr_encoded.index_select(dim=0,index=signal.node_index))\n\nnums_node_val = []\nfor signal in signals_val:\n    nums_node_val.append(signal.node_attr.shape[0])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:09.204588Z","iopub.execute_input":"2023-07-28T09:13:09.204987Z","iopub.status.idle":"2023-07-28T09:13:12.783885Z","shell.execute_reply.started":"2023-07-28T09:13:09.204957Z","shell.execute_reply":"2023-07-28T09:13:12.782860Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for sample in X_train[:1]:\n#     for snapshot in sample:\n#         node_attr = snapshot.node_attr\n#         node_index = snapshot.node_index\n#         edge_attr = snapshot.edge_attr\n#         edge_index = snapshot.edge_index\n        \n#         print(f\"node_attr: {node_attr.shape}\")\n#         print(f\"node_index: {node_index.shape}\")\n#         print(f\"edge_attr: {edge_attr.shape}\")\n#         print(f\"edge_index: {edge_index.shape}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:12.785284Z","iopub.execute_input":"2023-07-28T09:13:12.785668Z","iopub.status.idle":"2023-07-28T09:13:12.791880Z","shell.execute_reply.started":"2023-07-28T09:13:12.785636Z","shell.execute_reply":"2023-07-28T09:13:12.790073Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"IN_CHANNELS = signals_train[0].node_attr_encoded.shape[1]\nEDGE_CHANNELS = signals_train[0].edge_attr_encoded.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.793167Z","iopub.execute_input":"2023-07-28T09:13:12.793987Z","iopub.status.idle":"2023-07-28T09:13:12.825143Z","shell.execute_reply.started":"2023-07-28T09:13:12.793951Z","shell.execute_reply":"2023-07-28T09:13:12.823899Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = TGAE(\n    in_channels=IN_CHANNELS, \n    out_channels=5, \n    edge_channels=EDGE_CHANNELS, \n    embed_layers=[16,32],\n    gru_out_channels=8,\n    decide_layers=[16,16],\n    gru_conv_layers=[16,16],\n    gru_edge_layers=[32],\n    gru_lin_layers=[],\n)\n\nloss_f = torch.nn.MSELoss(reduction = 'none')\noptimizer = torch.optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5)\n\nmodel = model.to(device)\nloss_f = loss_f.to(device)\nprint(model)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-28T09:13:12.829073Z","iopub.execute_input":"2023-07-28T09:13:12.829413Z","iopub.status.idle":"2023-07-28T09:13:12.931475Z","shell.execute_reply.started":"2023-07-28T09:13:12.829375Z","shell.execute_reply":"2023-07-28T09:13:12.930349Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"TGAE(\n  (encoder_embedding_net): Sequential(\n    (0): Linear(in_features=5, out_features=16, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=16, out_features=32, bias=True)\n  )\n  (encoder_gru_forward): GraphGRULayer(\n    (gru): GraphGRU(\n      (conv_x_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n    )\n  )\n  (encoder_gru_backward): GraphGRULayer(\n    (gru): GraphGRU(\n      (conv_x_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=8, bias=True)\n        )\n      )\n    )\n  )\n  (encoder_deciding_net): Sequential(\n    (0): Linear(in_features=16, out_features=16, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Linear(in_features=16, out_features=16, bias=True)\n    (3): LeakyReLU(negative_slope=0.01)\n    (4): Linear(in_features=16, out_features=5, bias=True)\n  )\n  (decoder_deciding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=5, out_features=16, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=16, out_features=16, bias=True)\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Linear(in_features=16, out_features=8, bias=True)\n  )\n  (decoder_gru_forward): GraphGRULayer(\n    (gru): GraphGRU(\n      (conv_x_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n    )\n  )\n  (decoder_gru_backward): GraphGRULayer(\n    (gru): GraphGRU(\n      (conv_x_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_z): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_r): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_x_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(8, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=128, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n      (conv_h_h): MultiNNConv(\n        (convs): ModuleList(\n          (0): NNConv(32, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=512, bias=True)\n          ))\n          (1): NNConv(16, 16, aggr=mean, nn=Sequential(\n            (0): Linear(in_features=58, out_features=32, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=32, out_features=256, bias=True)\n          ))\n        )\n        (lin_net): Sequential(\n          (0): ReLU()\n          (1): Linear(in_features=16, out_features=32, bias=True)\n        )\n      )\n    )\n  )\n  (decoder_embedding_net): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=64, out_features=16, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Linear(in_features=16, out_features=5, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train Loop","metadata":{}},{"cell_type":"code","source":"# train_loop\ndef train_loop(signal, model, loss_fn, optimizer, device):\n    model.train()\n    \n    X = signal.node_attr_encoded.index_select(dim=0,index=signal.node_index)\n    node_index = signal.node_index\n    node_flag = signal.node_flag[:-1]\n    edge_index = signal.edge_index\n    edge_attr = signal.edge_attr_encoded\n    edge_flag = signal.edge_flag[:-1]\n    \n    outs = model(X, node_index, node_flag, edge_index, edge_attr, edge_flag, signal.node_attr.shape[0])\n\n    train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n    total_loss = torch.mean(train_losses)\n    snapshot_losses = [torch.mean(loss).cpu().numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n    \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    return snapshot_losses","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.933065Z","iopub.execute_input":"2023-07-28T09:13:12.933663Z","iopub.status.idle":"2023-07-28T09:13:12.943091Z","shell.execute_reply.started":"2023-07-28T09:13:12.933622Z","shell.execute_reply":"2023-07-28T09:13:12.941846Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Test Loop","metadata":{}},{"cell_type":"code","source":"# test_loop\ndef test_loop(signal, model, loss_fn, optimizer, device):\n    with torch.no_grad():\n        X = signal.node_attr_encoded.index_select(dim=0,index=signal.node_index)\n        node_index = signal.node_index\n        node_flag = signal.node_flag[:-1]\n        edge_index = signal.edge_index\n        edge_attr = signal.edge_attr_encoded\n        edge_flag = signal.edge_flag[:-1]\n\n        outs = model(X, node_index, node_flag, edge_index, edge_attr, edge_flag, signal.node_attr.shape[0])\n\n        train_losses = torch.sqrt(torch.sum(loss_f(X, outs),dim=1))\n        total_loss = torch.sum(train_losses)\n        snapshot_losses = [torch.mean(loss.cpu()).numpy() for loss in torch.tensor_split(train_losses.detach(), node_flag)]\n    \n    return snapshot_losses","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.944881Z","iopub.execute_input":"2023-07-28T09:13:12.945240Z","iopub.status.idle":"2023-07-28T09:13:12.956750Z","shell.execute_reply.started":"2023-07-28T09:13:12.945208Z","shell.execute_reply":"2023-07-28T09:13:12.955676Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"GLOBAL_EPOCH = 0\ndef train_function(num_epoch):\n    global GLOBAL_EPOCH\n    \n    history_train = []\n    history_val = []\n\n    for i_epoch in range(1,num_epoch+1):\n        train_losses = []\n        _start = time.time()\n        for signal in signals_train:\n            snapshot_losses = train_loop(signal, model, loss_f, optimizer, device)\n#             print(np.mean(train_loss))\n            train_losses.append(np.mean(snapshot_losses))\n            \n        \n        if i_epoch % 5 == 0:\n            val_losses = []\n            for signal in signals_val:\n                snapshot_losses = test_loop(signal, model, loss_f, optimizer, device)\n#                 print(np.mean(val_loss))\n                val_losses.append(np.mean(snapshot_losses))\n            _end = time.time()\n            \n            history_train.append(np.mean(train_losses))\n            history_val.append(np.mean(val_losses))\n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train RMSE {np.mean(train_losses):.4f} val RMSE{np.mean(val_losses):.4f}\")\n        else:\n            _end = time.time()\n            history_train.append(np.mean(train_losses))\n            print(f\"{i_epoch+GLOBAL_EPOCH}/{num_epoch+GLOBAL_EPOCH}: cost {_end-_start:.4f}s train RMSE {np.mean(train_losses):.4f}\")\n    GLOBAL_EPOCH += num_epoch\n    return (history_train,history_val)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.959812Z","iopub.execute_input":"2023-07-28T09:13:12.960251Z","iopub.status.idle":"2023-07-28T09:13:12.971928Z","shell.execute_reply.started":"2023-07-28T09:13:12.960215Z","shell.execute_reply":"2023-07-28T09:13:12.970906Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"history_train_list = []\nhistory_val_list = []","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.973444Z","iopub.execute_input":"2023-07-28T09:13:12.974248Z","iopub.status.idle":"2023-07-28T09:13:12.986421Z","shell.execute_reply.started":"2023-07-28T09:13:12.974213Z","shell.execute_reply":"2023-07-28T09:13:12.985374Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"history_train, history_val = train_function(50)\nhistory_train_list += history_train\nhistory_val_list += history_val\nplt.plot(history_train_list[4::5],label=\"Train\")\nplt.plot(history_val_list,label=\"Val\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:13:12.987719Z","iopub.execute_input":"2023-07-28T09:13:12.988346Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1/50: cost 81.1749s train RMSE 1.0445\n2/50: cost 75.6948s train RMSE 0.8643\n3/50: cost 76.2329s train RMSE 0.7310\n4/50: cost 75.9420s train RMSE 0.6477\n5/50: cost 88.1867s train RMSE 0.6005 val RMSE0.5670\n6/50: cost 76.0546s train RMSE 0.5440\n7/50: cost 76.3357s train RMSE 0.5011\n","output_type":"stream"}]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))    \nfor i_signal,(signal,y) in enumerate(zip(signals_val,y_val)):\n    snapshot_losses = test_loop(signal, model, loss_f, optimizer, device)\n#                 print(np.mean(val_loss))\n#     val_losses.append(np.mean(snapshot_losses))\n    ts_list = signal.ts_list\n    plt.subplot(len(signals_val)+1,1,i_signal+1)\n#     print(len(snapshot_losses))\n    plt.plot(ts_list,snapshot_losses)\n    if y[0] == 'dos':\n        plt.axvline(x = float(y[1]), color = 'red', label = y[0])\n    if y[0] == 'privesc':\n        plt.axvline(x = float(y[1]), color = 'blue', label = y[0])\n# plt.show()\nplt.savefig(f'val_res_TGAE_NNConv_{GLOBAL_EPOCH}_{np.mean(history_train_list[-1]):.4f}_{np.mean(history_val_list[-1]):.4f}.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,f\"TGAE_NNConv_{GLOBAL_EPOCH}_{np.mean(history_train_list[-1]):.4f}_{np.mean(history_val_list[-1]):.4f}.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], use_cuda=True, record_shapes=True) as prof:\n#         history_train, history_val = train_function(1)\n#         history_train_list += history_train\n#         history_val_list += history_val\n#         plt.plot(history_train_list[1::5],label=\"Train\")\n#         plt.plot(history_val_list,label=\"Val\")\n#         plt.legend()\n# print(prof.key_averages().table(sort_by=\"cuda_time_total\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(prof.key_averages().table(sort_by=\"cpu_time_total\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig = plt.figure(figsize=(20,20))    \n# for i_signal,(signal,y) in enumerate(zip(signals_val,y_val)):\n#     snapshot_losses = test_loop(signal, model, loss_f, optimizer, device)\n# #                 print(np.mean(val_loss))\n# #     val_losses.append(np.mean(snapshot_losses))\n#     ts_list = signal.ts_list\n#     plt.subplot(len(signals_val)+1,1,i_signal+1)\n# #     print(len(snapshot_losses))\n#     plt.plot(ts_list,snapshot_losses[:-1])\n#     if y[0] == 'dos':\n#         plt.axvline(x = float(y[1]), color = 'red', label = y[0])\n#     if y[0] == 'privesc':\n#         plt.axvline(x = float(y[1]), color = 'blue', label = y[0])\n# # plt.show()\n# plt.savefig('val_res.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model,f\"{GLOBAL_EPOCH}_{np.mean(history_train_list[-1]):.4f}_{np.mean(history_val_list[-1]):.4f}.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_train, history_val = train_function(100)\n# history_train_list += history_train\n# history_val_list += history_val\n# plt.plot(history_train_list[1::5],label=\"Train\")\n# plt.plot(history_val_list,label=\"Val\")\n# plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# states_encoder_val = []\n# states_decoder_val = []\n# for num_node in nums_node_val:\n#     states_encoder_val.append(create_hidden_global(num_node=num_node,out_channels=model.gnn_out_channels))\n#     states_decoder_val.append(create_hidden_global(num_node=num_node,out_channels=model.embedding_hidden_nums[-1]))    \n\n# fig = plt.figure(figsize=(20,20))    \n# for i_sample, (sample, y, hidden_encoder_global, hidden_decoder_global) in enumerate(zip(X_val, y_val, states_encoder_val, states_decoder_val)):\n#     val_loss = test_loop(sample, hidden_encoder_global, hidden_decoder_global, model, loss_f, optimizer, device)\n#     # print(np.mean(val_loss))\n#     ts_list = [snapshot.timestamp for snapshot in sample]\n#     plt.subplot(len(X_val)+1,1,i_sample+1)\n#     plt.plot(ts_list,val_loss)\n#     if y[0] == 'dos':\n#         plt.axvline(x = float(y[1]), color = 'red', label = y[0])\n#     if y[0] == 'privesc':\n#         plt.axvline(x = float(y[1]), color = 'blue', label = y[0])\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}